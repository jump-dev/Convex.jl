var documenterSearchIndex = {"docs":
[{"location":"contributing/#Contributing-1","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"We'd welcome contributions to the Convex.jl package. Here are some short instructions on how to get started. If you don't know what you'd like to contribute, you could","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"take a look at the current   issues and pick   one. (Feature requests are probably the easiest to tackle.)\nadd a usage   example.","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Then submit a pull request (PR). (Let us know if it's a work in progress by putting [WIP] in the name of the PR.)","category":"page"},{"location":"contributing/#Adding-examples-1","page":"Contributing","title":"Adding examples","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Take a look at our exising usage   examples   and add another in similar style.\nSubmit a PR. (Let us know if it's a work in progress by putting   [WIP] in the name of the PR.)\nWe'll look it over, fix up anything that doesn't work, and merge   it!","category":"page"},{"location":"contributing/#Adding-atoms-1","page":"Contributing","title":"Adding atoms","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Here are the steps to add a new function or operation (atom) to Convex.jl. Let's say you're adding the new function f.","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Take a look at the nuclear norm   atom   for an example of how to construct atoms, and see the norm   atom   for an example of an atom that depends on a parameter.\nCopy paste (eg) the nuclear norm file, replace anything saying   nuclear norm with the name of the atom f, fill in monotonicity,   curvature, etc. Save it in the appropriate subfolder of   src/atoms/.\nAdd as a comment a description of what the atom does and its   parameters.\nThe most mathematically interesting part is the conic_form!   function. Following the example in the nuclear norm atom, you'll   see that you can just construct the problem whose optimal value is   f(x), introducing any auxiliary variables you need, exactly as   you would normally in Convex.jl, and then call cache_conic_form!   on that problem.\nAdd a test for the atom so we can verify it works in   test/test_<cone>, where <cone> matches the subfolder of   src/atoms.\nSubmit a PR, including a description of what the atom does and its   parameters. (Let us know if it's a work in progress by putting   [WIP] in the name of the PR.)\nWe'll look it over, fix up anything that doesn't work, and merge   it!","category":"page"},{"location":"contributing/#Fixing-the-guts-1","page":"Contributing","title":"Fixing the guts","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"If you want to do a more major bug fix, you may need to understand how Convex.jl thinks about conic form. To do this, start by reading the Convex.jl paper. You may find our JuliaCon 2014 talk helpful as well; you can find the ipython notebook presented in the talk here.","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Then read the conic form code:","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"We define data structures for conic objectives and conic   constraints, and simple ways of combining them, in   conic_form.jl\nWe convert the internal conic form representation into the   standard form for conic   solvers   in the function   conic_problem.\nWe solve problems (that is, pass the standard form of the problem   to a solver, and put the solution back into the values of the   appropriate variables) in   solution.jl.","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"You're now armed and dangerous. Go ahead and open an issue (or comment on a previous one) if you can't figure something out, or submit a PR if you can figure it out. (Let us know if it's a work in progress by putting [WIP] in the name of the PR.)","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"PRs that comment the code more thoroughly will also be welcomed.","category":"page"},{"location":"quick_tutorial/#Quick-Tutorial-1","page":"Quick Tutorial","title":"Quick Tutorial","text":"","category":"section"},{"location":"quick_tutorial/#","page":"Quick Tutorial","title":"Quick Tutorial","text":"Consider a constrained least squares problem","category":"page"},{"location":"quick_tutorial/#","page":"Quick Tutorial","title":"Quick Tutorial","text":"beginaligned\nbeginarrayll\nmboxminimize  Ax - b_2^2 \nmboxsubject to  x geq 0\nendarray\nendaligned","category":"page"},{"location":"quick_tutorial/#","page":"Quick Tutorial","title":"Quick Tutorial","text":"with variable xin mathbfR^n, and problem data A in mathbfR^m times n, b in mathbfR^m.","category":"page"},{"location":"quick_tutorial/#","page":"Quick Tutorial","title":"Quick Tutorial","text":"This problem can be solved in Convex.jl as follows: :","category":"page"},{"location":"quick_tutorial/#","page":"Quick Tutorial","title":"Quick Tutorial","text":"# Make the Convex.jl module available\nusing Convex, SCS\n\n# Generate random problem data\nm = 4;  n = 5\nA = randn(m, n); b = randn(m, 1)\n\n# Create a (column vector) variable of size n x 1.\nx = Variable(n)\n\n# The problem is to minimize ||Ax - b||^2 subject to x >= 0\n# This can be done by: minimize(objective, constraints)\nproblem = minimize(sumsquares(A * x - b), [x >= 0])\n\n# Solve the problem by calling solve!\nsolve!(problem, SCSSolver())\n\n# Check the status of the problem\nproblem.status # :Optimal, :Infeasible, :Unbounded etc.\n\n# Get the optimum value\nproblem.optval","category":"page"},{"location":"installation/#Installation-1","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/#","page":"Installation","title":"Installation","text":"Installing Convex.jl is a one step process. Open up Julia and type :","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"Pkg.update()\nPkg.add(\"Convex\")","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"This does not install any solvers. If you don't have a solver installed already, you will want to install a solver such as SCS by running :","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"Pkg.add(\"SCS\")","category":"page"},{"location":"installation/#","page":"Installation","title":"Installation","text":"To solve certain problems such as mixed integer programming problems you will need to install another solver as well, such as GLPK. If you wish to use other solvers, please read the section on Solvers.","category":"page"},{"location":"complex-domain_optimization/#Optimization-with-Complex-Variables-1","page":"Complex-domain Optimization","title":"Optimization with Complex Variables","text":"","category":"section"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"Convex.jl also supports optimization with complex variables. Below, we present a quick start guide on how to use Convex.jl for optimization with complex variables, and then list the operations supported on complex variables in Convex.jl. In general, any operation available in Convex.jl that is well defined and DCP compliant on complex variables should be available. We list these functions below. organized by the type of cone (linear, second-order, or semidefinite) used to represent that operation.","category":"page"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"Internally, Convex.jl transforms the complex-domain problem to a larger real-domain problem using a bijective mapping. It then solves the real-domain problem and transforms the solution back to the complex domain.","category":"page"},{"location":"complex-domain_optimization/#Complex-Variables-1","page":"Complex-domain Optimization","title":"Complex Variables","text":"","category":"section"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"Complex Variables in Convex.jl are declared in the same way as the variables are declared but using the different keyword ComplexVariable.","category":"page"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"  # Scalar complex variable\n  z = ComplexVariable()\n\n  # Column vector variable\n  z = ComplexVariable(5)\n\n  # Matrix variable\n  z = ComplexVariable(4, 6)\n\n  # Complex Positive Semidefinite variable\n  z = HermitianSemidefinite(4)","category":"page"},{"location":"complex-domain_optimization/#Linear-Program-Representable-Functions-(complex-variables)-1","page":"Complex-domain Optimization","title":"Linear Program Representable Functions (complex variables)","text":"","category":"section"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"All of the linear functions that are listed under Linear Program Representable Functions operate on complex variables as well. In addition, several specialized functions for complex variables are available:","category":"page"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"operation description vexity slope notes\nreal(z) real part of complex of variable affine increasing none\nimag(z) imaginary part of complex variable affine increasing none\nconj(x) element-wise complex conjugate affine increasing none\ninnerproduct(x,y) real(trace(x'*y)) affine increasing PR: one argument is constant","category":"page"},{"location":"complex-domain_optimization/#Second-Order-Cone-Representable-Functions-(complex-variables)-1","page":"Complex-domain Optimization","title":"Second-Order Cone Representable Functions (complex variables)","text":"","category":"section"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"Most of the second order cone function listed under Second-Order Cone Representable Functions operate on complex variables as well. Notable exceptions include:","category":"page"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"inverse\nsquare\nquadoverlin\nsqrt\ngeomean\nhuber","category":"page"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"One new function is available:","category":"page"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"operation description vexity slope notes\nabs2(z) square(abs(z)) convex increasing none","category":"page"},{"location":"complex-domain_optimization/#Semidefinite-Program-Representable-Functions-(complex-variables)-1","page":"Complex-domain Optimization","title":"Semidefinite Program Representable Functions (complex variables)","text":"","category":"section"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"All SDP-representable functions listed under Semidefinite Program Representable Functions work for complex variables.","category":"page"},{"location":"complex-domain_optimization/#Exponential-SDP-representable-Functions-(complex-variables)-1","page":"Complex-domain Optimization","title":"Exponential + SDP representable Functions (complex variables)","text":"","category":"section"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"Complex variables also support logdet function.","category":"page"},{"location":"complex-domain_optimization/#Optimizing-over-quantum-states-1","page":"Complex-domain Optimization","title":"Optimizing over quantum states","text":"","category":"section"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"The complex and Hermitian matrix variables, along with the kron and partialtrace operations, enable the definition of a wide range of problems in quantum information theory. As a simple example, let us consider a state rho over a composite Hilbert space mathcalH_AotimesmathcalH_B, where both component spaces are isomorphic to mathbbC^2. Assume that rho is a product state, with its component in mathcalH_A given as A, a complex-valued matrix. We can optimize over the second component B to meet some requirement. Here we simply fix the second component too, but via the partialtrace operator:","category":"page"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"using Convex, SCS\nA = [ 0.47213595 0.11469794+0.48586827im; 0.11469794-0.48586827im  0.52786405]\nB = ComplexVariable(2, 2)\nρ = kron(A, B)\nconstraints = [partialtrace(ρ, 1, [2; 2]) == [1 0; 0 0]\n               tr(ρ) == 1\n               ρ in :SDP]\np = satisfy(constraints)\nsolve!(p, SCSSolver())\np.status","category":"page"},{"location":"complex-domain_optimization/#","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"Since we fix both components as trace-1 positive semidefinite matrices, the last two constraints are actually redundant in this case.","category":"page"},{"location":"solvers/#Solvers-1","page":"Solvers","title":"Solvers","text":"","category":"section"},{"location":"solvers/#","page":"Solvers","title":"Solvers","text":"Convex.jl transforms each problem into an equivalent cone program in order to pass the problem to a specialized solver. Depending on the types of functions used in the problem, the conic constraints may include linear, second-order, exponential, or semidefinite constraints, as well as any binary or integer constraints placed on the variables.","category":"page"},{"location":"solvers/#","page":"Solvers","title":"Solvers","text":"By default, Convex.jl does not install any solvers. Many users use the solver SCS, which is able to solve problems with linear, second-order cone constraints (SOCPs), exponential constraints and semidefinite constraints (SDPs). Any other solver in JuliaOpt may also be used, so long as it supports the conic constraints used to represent the problem. Most other solvers in the JuliaOpt ecosystem can be used to solve (mixed integer) linear programs (LPs and MILPs). Mosek and Gurobi can be used to solve SOCPs (even with binary or integer constraints), and Mosek can also solve SDPs. For up-to-date information about solver capabilities, please see the table here describing which solvers can solve which kind of problems.","category":"page"},{"location":"solvers/#","page":"Solvers","title":"Solvers","text":"Installing these solvers is very simple. Just follow the instructions in the documentation for that solver.","category":"page"},{"location":"solvers/#","page":"Solvers","title":"Solvers","text":"To use a specific solver, you can use the following syntax","category":"page"},{"location":"solvers/#","page":"Solvers","title":"Solvers","text":"solve!(p, GurobiSolver())\nsolve!(p, MosekSolver())\nsolve!(p, GLPKSolverMIP())\nsolve!(p, GLPKSolverLP())\nsolve!(p, ECOSSolver())\nsolve!(p, SCSSolver())","category":"page"},{"location":"solvers/#","page":"Solvers","title":"Solvers","text":"(Of course, the solver must be installed first.) For example, we can use GLPK to solve a MILP","category":"page"},{"location":"solvers/#","page":"Solvers","title":"Solvers","text":"using GLPKMathProgInterface\nsolve!(p, GLPKSolverMIP())","category":"page"},{"location":"solvers/#","page":"Solvers","title":"Solvers","text":"Many of the solvers also allow options to be passed in. More details can be found in each solver's documentation.","category":"page"},{"location":"solvers/#","page":"Solvers","title":"Solvers","text":"For example, if we wish to turn off printing for the SCS solver (ie, run in quiet mode), we can do so by","category":"page"},{"location":"solvers/#","page":"Solvers","title":"Solvers","text":"using SCS\nsolve!(p, SCSSolver(verbose=false))","category":"page"},{"location":"solvers/#","page":"Solvers","title":"Solvers","text":"If we wish to increase the maximum number of iterations for ECOS or SCS, we can do so by","category":"page"},{"location":"solvers/#","page":"Solvers","title":"Solvers","text":"using ECOS\nsolve!(p, ECOSSolver(maxit=10000))\nusing SCS\nsolve!(p, SCSSolver(max_iters=10000))","category":"page"},{"location":"solvers/#","page":"Solvers","title":"Solvers","text":"To turn off the problem status warning issued by Convex when a solver is not able to solve a problem to optimality, use the keyword argument verbose=false of the solve method, along with any desired solver parameters:","category":"page"},{"location":"solvers/#","page":"Solvers","title":"Solvers","text":"solve!(p, SCSSolver(verbose=false), verbose=false)","category":"page"},{"location":"advanced/#Advanced-Features-1","page":"Advanced","title":"Advanced Features","text":"","category":"section"},{"location":"advanced/#Dual-Variables-1","page":"Advanced","title":"Dual Variables","text":"","category":"section"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"Convex.jl also returns the optimal dual variables for a problem. These are stored in the dual field associated with each constraint.","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"using Convex, SCS\n\nx = Variable()\nconstraint = x >= 0\np = minimize(x, constraint)\nsolve!(p, SCSSolver())\n\n# Get the dual value for the constraint\np.constraints[1].dual\n# or\nconstraint.dual","category":"page"},{"location":"advanced/#Warmstarting-1","page":"Advanced","title":"Warmstarting","text":"","category":"section"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"If you're solving the same problem many times with different values of a parameter, Convex.jl can initialize many solvers with the solution to the previous problem, which sometimes speeds up the solution time. This is called a warm start.","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"To use this feature, pass the optional argument warmstart=true to the solve! method.","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"# initialize data\nn = 1000\ny = rand(n)\nx = Variable(n)\n\n# first solve\nlambda = 100\nproblem = minimize(sumsquares(y - x) + lambda * sumsquares(x - 10))\n@time solve!(problem, SCSSolver())\n\n# now warmstart\n# if the solver takes advantage of warmstarts, \n# this run will be faster\nlambda = 105\n@time solve!(problem, SCSSolver(), warmstart=true)","category":"page"},{"location":"advanced/#Fixing-and-freeing-variables-1","page":"Advanced","title":"Fixing and freeing variables","text":"","category":"section"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"Convex.jl allows you to fix a variable x to a value by calling the fix! method. Fixing the variable essentially turns it into a constant. Fixed variables are sometimes also called parameters.","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"fix!(x, v) fixes the variable x to the value v.","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"fix!(x) fixes x to the value x.value, which might be the value obtained by solving another problem involving the variable x.","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"To allow the variable x to vary again, call free!(x).","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"Fixing and freeing variables can be particularly useful as a tool for performing alternating minimization on nonconvex problems. For example, we can find an approximate solution to a nonnegative matrix factorization problem with alternating minimization as follows. We use warmstarts to speed up the solution.","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"# initialize nonconvex problem\nn, k = 10, 1\nA = rand(n, k) * rand(k, n)\nx = Variable(n, k)\ny = Variable(k, n)\nproblem = minimize(sum_squares(A - x*y), x>=0, y>=0)\n\n# initialize value of y\ny.value = rand(k, n)\n# we'll do 10 iterations of alternating minimization\nfor i=1:10 \n    # first solve for x\n    # with y fixed, the problem is convex\n    fix!(y)\n    solve!(problem, SCSSolver(), warmstart = i > 1 ? true : false)\n    free!(y)\n\n    # now solve for y with x fixed at the previous solution\n    fix!(x)\n    solve!(problem, SCSSolver(), warmstart = true)\n    free!(x)\nend","category":"page"},{"location":"advanced/#Printing-and-the-tree-structure-1","page":"Advanced","title":"Printing and the tree structure","text":"","category":"section"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"A Convex problem is structured as a tree, with the root being the problem object, with branches to the objective and the set of constraints. The objective is an AbstractExpr which itself is a tree, with each atom being a node and having children which are other atoms, variables, or constants. Convex provides children methods from AbstractTrees.jl so that the tree-traversal functions of that package can be used with Convex.jl problems and structures. This is what allows powers the printing of problems, expressions, and constraints. The depth to which the tree corresponding to a problem, expression, or constraint is printed is controlled by the global variable MAXDEPTH, which defaults to 3. This can be changed by e.g. setting","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"Convex.MAXDEPTH[] = 5","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"The AbstractTrees methods can also be used to analyze the structure of a Convex.jl problem. For example,","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"using Convex, AbstractTrees\nx = Variable()\np = maximize( log(x), x >= 1, x <= 3 )\nfor leaf in AbstractTrees.Leaves(p)\n    println(\"Here's a leaf: $(summary(leaf))\")\nend","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"We can also iterate over the problem in various orders. The following descriptions are taken from the AbstractTrees.jl docstrings, which have more information.","category":"page"},{"location":"advanced/#PostOrderDFS-1","page":"Advanced","title":"PostOrderDFS","text":"","category":"section"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"Iterator to visit the nodes of a tree, guaranteeing that children will be visited before their parents.","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"for (i, node) in enumerate(AbstractTrees.PostOrderDFS(p))\n    println(\"Here's node $i via PostOrderDFS: $(summary(node))\")\nend","category":"page"},{"location":"advanced/#PreOrderDFS-1","page":"Advanced","title":"PreOrderDFS","text":"","category":"section"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"Iterator to visit the nodes of a tree, guaranteeing that parents will be visited before their children.","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"for (i, node) in enumerate(AbstractTrees.PreOrderDFS(p))\n    println(\"Here's node $i via PreOrderDFS: $(summary(node))\")\nend","category":"page"},{"location":"advanced/#StatelessBFS-1","page":"Advanced","title":"StatelessBFS","text":"","category":"section"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"Iterator to visit the nodes of a tree, guaranteeing that all nodes of a level will be visited before their children.","category":"page"},{"location":"advanced/#","page":"Advanced","title":"Advanced","text":"for (i, node) in enumerate(AbstractTrees.StatelessBFS(p))\n    println(\"Here's node $i via StatelessBFS: $(summary(node))\")\nend","category":"page"},{"location":"faq/#FAQ-1","page":"FAQ","title":"FAQ","text":"","category":"section"},{"location":"faq/#Where-can-I-get-help?-1","page":"FAQ","title":"Where can I get help?","text":"","category":"section"},{"location":"faq/#","page":"FAQ","title":"FAQ","text":"For usage questions, please contact us via the Julia Discourse. If you're running into bugs or have feature requests, please use the Github Issue Tracker.","category":"page"},{"location":"faq/#How-does-Convex.jl-differ-from-JuMP?-1","page":"FAQ","title":"How does Convex.jl differ from JuMP?","text":"","category":"section"},{"location":"faq/#","page":"FAQ","title":"FAQ","text":"Convex.jl and JuMP are both modelling languages for mathematical programming embedded in Julia, and both interface with solvers via the MathProgBase interface, so many of the same solvers are available in both. Convex.jl converts problems to a standard conic form. This approach requires (and certifies) that the problem is convex and DCP compliant, and guarantees global optimality of the resulting solution. JuMP allows nonlinear programming through an interface that learns about functions via their derivatives. This approach is more flexible (for example, you can optimize non-convex functions), but can't guarantee global optimality if your function is not convex, or warn you if you've entered a non-convex formulation.","category":"page"},{"location":"faq/#","page":"FAQ","title":"FAQ","text":"For linear programming, the difference is more stylistic. JuMP's syntax is scalar-based and similar to AMPL and GAMS making it easy and fast to create constraints by indexing and summation (like sum{x[i], i=1:numLocation}). Convex.jl allows (and prioritizes) linear algebraic and functional constructions (like max(x,y) < A*z); indexing and summation are also supported in Convex.jl, but are somewhat slower than in JuMP. JuMP also lets you efficiently solve a sequence of problems when new constraints are added or when coefficients are modified, whereas Convex.jl parses the problem again whenever the [solve!]{.title-ref} method is called.","category":"page"},{"location":"faq/#Where-can-I-learn-more-about-Convex-Optimization?-1","page":"FAQ","title":"Where can I learn more about Convex Optimization?","text":"","category":"section"},{"location":"faq/#","page":"FAQ","title":"FAQ","text":"See the freely available book Convex Optimization by Boyd and Vandenberghe for general background on convex optimization. For help understanding the rules of Disciplined Convex Programming, we recommend this DCP tutorial website.","category":"page"},{"location":"faq/#Are-there-similar-packages-available-in-other-languages?-1","page":"FAQ","title":"Are there similar packages available in other languages?","text":"","category":"section"},{"location":"faq/#","page":"FAQ","title":"FAQ","text":"Indeed! You might use CVXPY in Python, or CVX in Matlab.","category":"page"},{"location":"faq/#How-does-Convex.jl-work?-1","page":"FAQ","title":"How does Convex.jl work?","text":"","category":"section"},{"location":"faq/#","page":"FAQ","title":"FAQ","text":"For a detailed discussion of how Convex.jl works, see our paper.","category":"page"},{"location":"faq/#How-do-I-cite-this-package?-1","page":"FAQ","title":"How do I cite this package?","text":"","category":"section"},{"location":"faq/#","page":"FAQ","title":"FAQ","text":"If you use Convex.jl for published work, we encourage you to cite the software using the following BibTeX citation: :","category":"page"},{"location":"faq/#","page":"FAQ","title":"FAQ","text":"@article{convexjl,\n title = {Convex Optimization in {J}ulia},\n author ={Udell, Madeleine and Mohan, Karanveer and Zeng, David and Hong, Jenny and Diamond, Steven and Boyd, Stephen},\n year = {2014},\n journal = {SC14 Workshop on High Performance Technical Computing in Dynamic Languages},\n archivePrefix = \"arXiv\",\n eprint = {1410.4821},\n primaryClass = \"math-oc\",\n}","category":"page"},{"location":"credits/#Credits-1","page":"Credits","title":"Credits","text":"","category":"section"},{"location":"credits/#","page":"Credits","title":"Credits","text":"Currently, Convex.jl is developed and maintained by:","category":"page"},{"location":"credits/#","page":"Credits","title":"Credits","text":"Jenny Hong\nKaranveer Mohan\nMadeleine Udell\nDavid Zeng","category":"page"},{"location":"credits/#","page":"Credits","title":"Credits","text":"The Convex.jl developers also thank:","category":"page"},{"location":"credits/#","page":"Credits","title":"Credits","text":"the JuliaOpt team: Iain   Dunning, Joey   Huchette and Miles   Lubin\nStephen Boyd, co-author of the   book Convex   Optimization\nSteven Diamond, developer of   CVXPY and of a DCP tutorial   website to teach disciplined convex   programming.\nMichael Grant, developer of   CVX.\nJohn Duchi and Hongseok   Namkoong for developing the representation of power cones in   terms of SOCP   constraints   used in this package.","category":"page"},{"location":"loop/#Optimizing-in-a-Loop-1","page":"Optimizing in a Loop","title":"Optimizing in a Loop","text":"","category":"section"},{"location":"loop/#Memory-Management-1","page":"Optimizing in a Loop","title":"Memory Management","text":"","category":"section"},{"location":"loop/#","page":"Optimizing in a Loop","title":"Optimizing in a Loop","text":"Convex uses a module-level dictionary to store the conic forms of every variable and expression created in the same Julia session. These variables and expressions persist even after they are out of scope. If you create large numbers of variables inside a loop, this dictionary can eat a considerable amount of memory.","category":"page"},{"location":"loop/#","page":"Optimizing in a Loop","title":"Optimizing in a Loop","text":"To flush the memory, you can call Convex.clearmemory().","category":"page"},{"location":"loop/#","page":"Optimizing in a Loop","title":"Optimizing in a Loop","text":"This will remove every variable and expression you've formed before from the memory cache, so that you're starting as fresh as if you'd just reimported Convex.","category":"page"},{"location":"loop/#Caching-Expressions-1","page":"Optimizing in a Loop","title":"Caching Expressions","text":"","category":"section"},{"location":"loop/#","page":"Optimizing in a Loop","title":"Optimizing in a Loop","text":"Better yet, take advantage of this cache of variables and expressions! Create variables and expressions outside the loop, and reuse them inside the loop as you tweak parameters. Doing this will allow Convex to reuse the conic forms it has already calculated for previously used expressions.","category":"page"},{"location":"loop/#","page":"Optimizing in a Loop","title":"Optimizing in a Loop","text":"For example, the following bad code will create a new instance of a variable and of the expression square(x) for each value of i. Don't do this:","category":"page"},{"location":"loop/#","page":"Optimizing in a Loop","title":"Optimizing in a Loop","text":"for i = 1:10\n    x = Variable()\n    p = minimize(square(x), x >= i)\n    solve!(p, SCSSolver())\nend","category":"page"},{"location":"loop/#","page":"Optimizing in a Loop","title":"Optimizing in a Loop","text":"Contrast this with the following good code, which will reuse the cached conic form for square(x) for each i, reducing the memory footprint and speeding up the computation. Do this instead:","category":"page"},{"location":"loop/#","page":"Optimizing in a Loop","title":"Optimizing in a Loop","text":"x = Variable()\nobj = square(x)\nfor i = 1:10\n    p = minimize(obj, x >= i)\n    solve!(p, SCSSolver())\nend","category":"page"},{"location":"loop/#Warmstarts,-Parameters,-Fixing-and-Freeing-Variables-1","page":"Optimizing in a Loop","title":"Warmstarts, Parameters, Fixing and Freeing Variables","text":"","category":"section"},{"location":"loop/#","page":"Optimizing in a Loop","title":"Optimizing in a Loop","text":"If you're solving many problems of the same form, or many similar problems, you may also want to use warmstarts, or to dynamically fix and free variables. The former is particularly good for a family of problems related by a parameter; the latter allows for easy implementation of alternating minimization for nonconvex problems. See the Advanced Features section of the documentation for more information.","category":"page"},{"location":"operations/#Operations-1","page":"Supported Operations","title":"Operations","text":"","category":"section"},{"location":"operations/#","page":"Supported Operations","title":"Supported Operations","text":"Convex.jl currently supports the following functions. These functions may be composed according to the DCP composition rules to form new convex, concave, or affine expressions. Convex.jl transforms each problem into an equivalent cone program in order to pass the problem to a specialized solver. Depending on the types of functions used in the problem, the conic constraints may include linear, second-order, exponential, or semidefinite constraints, as well as any binary or integer constraints placed on the variables. Below, we list each function available in Convex.jl organized by the (most complex) type of cone used to represent that function, and indicate which solvers may be used to solve problems with those cones. Problems mixing many different conic constraints can be solved by any solver that supports every kind of cone present in the problem.","category":"page"},{"location":"operations/#","page":"Supported Operations","title":"Supported Operations","text":"In the notes column in the tables below, we denote implicit constraints imposed on the arguments to the function by IC, and parameter restrictions that the arguments must obey by PR. (Convex.jl will automatically impose ICs; the user must make sure to satisfy PRs.) Elementwise means that the function operates elementwise on vector arguments, returning a vector of the same size.","category":"page"},{"location":"operations/#Linear-Program-Representable-Functions-1","page":"Supported Operations","title":"Linear Program Representable Functions","text":"","category":"section"},{"location":"operations/#","page":"Supported Operations","title":"Supported Operations","text":"An optimization problem using only these functions can be solved by any LP solver.","category":"page"},{"location":"operations/#","page":"Supported Operations","title":"Supported Operations","text":"operation description vexity slope notes\nx+y or x.+y addition affine increasing none\nx-y or x.-y subtraction affine increasing in \\(x\\) decreasing in \\(y\\) none none\nx*y multiplication affine increasing if constant term \\(\\ge 0\\) decreasing if constant term \\(\\le 0\\) not monotonic otherwise PR: one argument is constant\nx/y division affine increasing PR: \\(y\\) is scalar constant\ndot(*)(x, y) elementwise multiplication affine increasing PR: one argument is constant\ndot(/)(x, y) elementwise division affine increasing PR: one argument is constant\nx[1:4, 2:3] indexing and slicing affine increasing none\ndiag(x, k) \\(k\\)-th diagonal of a matrix affine increasing none\ndiagm(x) construct diagonal matrix affine increasing PR: \\(x\\) is a vector\nx' transpose affine increasing none\nvec(x) vector representation affine increasing none\ndot(x,y) \\(\\sum_i x_i y_i\\) affine increasing PR: one argument is constant\nkron(x,y) Kronecker product affine increasing PR: one argument is constant\nvecdot(x,y) dot(vec(x),vec(y)) affine increasing PR: one argument is constant\nsum(x) \\(\\sum_{ij} x_{ij}\\) affine increasing none\nsum(x, k) sum elements across dimension \\(k\\) affine increasing none\nsumlargest(x, k) sum of \\(k\\) largest elements of \\(x\\) convex increasing none\nsumsmallest(x, k) sum of \\(k\\) smallest elements of \\(x\\) concave increasing none\ndotsort(a, b) dot(sort(a),sort(b)) convex increasing PR: one argument is constant\nreshape(x, m, n) reshape into \\(m \\times n\\) affine increasing none\nminimum(x) \\(\\min(x)\\) concave increasing none\nmaximum(x) \\(\\max(x)\\) convex increasing none\n[x y] or [x; y] hcat(x, y) or vcat(x, y) stacking affine increasing none\ntr(x) \\(\\mathrm{tr} \\left(X \\right)\\) affine increasing none\npartialtrace(x,sys,dims) Partial trace affine increasing none\npartialtranspose(x,sys,dims) Partial transpose affine increasing none\nconv(h,x) \\(h \\in \\mathbb{R}^m\\) \\(x \\in \\mathbb{R}^m\\) \\(h*x \\in \\mathbb{R}^{m+n-1}\\) entry \\(i\\) is given by \\(\\sum_{j=1}^m h_jx_{i-j}\\) affine increasing if \\(h\\ge 0\\) decreasing if \\(h\\le 0\\) not monotonic otherwise PR: \\(h\\) is constant\nmin(x,y) \\(\\min(x,y)\\) concave increasing none\nmax(x,y) \\(\\max(x,y)\\) convex increasing none\npos(x) \\(\\max(x,0)\\) convex increasing none\nneg(x) \\(\\max(-x,0)\\) convex decreasing none\ninvpos(x) \\(1/x\\) convex decreasing IC: \\(x\\>0\\)\nabs(x) \\(\\left|x\\right|\\) convex increasing on \\(x \\ge 0\\) decreasing on \\(x \\le 0\\) none","category":"page"},{"location":"operations/#Second-Order-Cone-Representable-Functions-1","page":"Supported Operations","title":"Second-Order Cone Representable Functions","text":"","category":"section"},{"location":"operations/#","page":"Supported Operations","title":"Supported Operations","text":"An optimization problem using these functions can be solved by any SOCP solver (including ECOS, SCS, Mosek, Gurobi, and CPLEX). Of course, if an optimization problem has both LP and SOCP representable functions, then any solver that can solve both LPs and SOCPs can solve the problem.","category":"page"},{"location":"operations/#","page":"Supported Operations","title":"Supported Operations","text":"operation description vexity slope notes\nnorm(x, p) \\((\\sum x_i^p)^{1/p}\\) convex increasing on \\(x \\ge 0\\) decreasing on \\(x \\le 0\\) PR: p >= 1\nvecnorm(x, p) \\((\\sum x_{ij}^p)^{1/p}\\) convex increasing on \\(x \\ge 0\\) decreasing on \\(x \\le 0\\) PR: p >= 1\nquadform(x, P) \\(x^T P x\\) convex in \\(x\\) affine in \\(P\\) increasing on \\(x \\ge 0\\) decreasing on \\(x \\le 0\\) increasing in \\(P\\) PR: either \\(x\\) or \\(P\\) must be constant; if \\(x\\) is not constant, then \\(P\\) must be symmetric and positive semidefinite\nquadoverlin(x, y) \\(x^T x/y\\) convex increasing on \\(x \\ge 0\\) decreasing on \\(x \\le 0\\) decreasing in \\(y\\) IC: \\(y \\> 0\\)\nsumsquares(x) \\(\\sum x_i^2\\) convex increasing on \\(x \\ge 0\\) decreasing on \\(x \\le 0\\) none\nsqrt(x) \\(\\sqrt{x}\\) concave decreasing IC: \\(x\\>0\\)\nsquare(x), x^2 \\(x^2\\) convex increasing on \\(x \\ge 0\\) decreasing on \\(x \\le 0\\) PR : \\(x\\) is scalar\ndot(^)(x,2) \\(x.^2\\) convex increasing on \\(x \\ge 0\\) decreasing on \\(x \\le 0\\) elementwise\ngeomean(x, y) \\(\\sqrt{xy}\\) concave increasing IC: \\(x\\ge0\\), \\(y\\ge0\\)\nhuber(x, M=1) \\(\\begin{cases} x^2 &|x| \\leq M \\\\ 2M|x| - M^2 &|x| \\> M \\end{cases}\\) convex increasing on \\(x \\ge 0\\) decreasing on \\(x \\le 0\\) PR: \\(M\\>=1\\)","category":"page"},{"location":"operations/#Exponential-Cone-Representable-Functions-1","page":"Supported Operations","title":"Exponential Cone Representable Functions","text":"","category":"section"},{"location":"operations/#","page":"Supported Operations","title":"Supported Operations","text":"An optimization problem using these functions can be solved by any exponential cone solver (SCS).","category":"page"},{"location":"operations/#","page":"Supported Operations","title":"Supported Operations","text":"operation description vexity slope notes\nlogsumexp(x) \\(\\log(\\sum_i \\exp(x_i))\\) convex increasing none\nexp(x) \\(\\exp(x)\\) convex increasing none\nlog(x) \\(\\log(x)\\) concave increasing IC: \\(x\\>0\\)\nentropy(x) \\(\\sum_{ij} -x_{ij} \\log (x_{ij})\\) concave not monotonic IC: \\(x\\>0\\)\nlogisticloss(x) \\(\\log(1 + \\exp(x_i))\\) convex increasing none","category":"page"},{"location":"operations/#Semidefinite-Program-Representable-Functions-1","page":"Supported Operations","title":"Semidefinite Program Representable Functions","text":"","category":"section"},{"location":"operations/#","page":"Supported Operations","title":"Supported Operations","text":"An optimization problem using these functions can be solved by any SDP solver (including SCS and Mosek).","category":"page"},{"location":"operations/#","page":"Supported Operations","title":"Supported Operations","text":"operation description vexity slope notes\nnuclearnorm(x) sum of singular values of \\(x\\) convex not monotonic none\noperatornorm(x) max of singular values of \\(x\\) convex not monotonic none\nlambdamax(x) max eigenvalue of \\(x\\) convex not monotonic none\nlambdamin(x) min eigenvalue of \\(x\\) concave not monotonic none\nmatrixfrac(x, P) \\(x^TP^{-1}x\\) convex not monotonic IC: P is positive semidefinite","category":"page"},{"location":"operations/#Exponential-SDP-representable-Functions-1","page":"Supported Operations","title":"Exponential + SDP representable Functions","text":"","category":"section"},{"location":"operations/#","page":"Supported Operations","title":"Supported Operations","text":"An optimization problem using these functions can be solved by any solver that supports exponential constraints and semidefinite constraints simultaneously (SCS).","category":"page"},{"location":"operations/#","page":"Supported Operations","title":"Supported Operations","text":"operation description vexity slope notes\nlogdet(x) log of determinant of \\(x\\) concave increasing IC: x is positive semidefinite","category":"page"},{"location":"operations/#Promotions-1","page":"Supported Operations","title":"Promotions","text":"","category":"section"},{"location":"operations/#","page":"Supported Operations","title":"Supported Operations","text":"When an atom or constraint is applied to a scalar and a higher dimensional variable, the scalars are promoted. For example, we can do max(x, 0) gives an expression with the shape of x whose elements are the maximum of the corresponding element of x and 0.","category":"page"},{"location":"examples/#Examples-1","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#","page":"Examples","title":"Examples","text":"Basic   usage\nControl\nTomography\nTime   series\nSection   allocation\nBinary   knapsack\nEntropy   maximization\nLogistic   regression\nOptimization with Complex   Variables","category":"page"},{"location":"#Convex.jl-Convex-Optimization-in-Julia-1","page":"Home","title":"Convex.jl - Convex Optimization in Julia","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Convex.jl is a Julia package for Disciplined Convex Programming (DCP). Convex.jl makes it easy to describe optimization problems in a natural, mathematical syntax, and to solve those problems using a variety of different (commercial and open-source) solvers. Convex.jl can solve","category":"page"},{"location":"#","page":"Home","title":"Home","text":"linear programs\nmixed-integer linear programs and mixed-integer second-order cone   programs\ndcp-compliant convex programs including\nsecond-order cone programs (SOCP)\nexponential cone programs\nsemidefinite programs (SDP)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Convex.jl supports many solvers, including Mosek, Gurobi, ECOS, SCS and GLPK, through the MathProgBase interface.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Note that Convex.jl was previously called CVX.jl. This package is under active development; we welcome bug reports and feature requests. For usage questions, please contact us via the Julia Discourse.","category":"page"},{"location":"types/#Basic-Types-1","page":"Basic Types","title":"Basic Types","text":"","category":"section"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"The basic building block of Convex.jl is called an expression, which can represent a variable, a constant, or a function of another expression. We discuss each kind of expression in turn.","category":"page"},{"location":"types/#Variables-1","page":"Basic Types","title":"Variables","text":"","category":"section"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"The simplest kind of expression in Convex.jl is a variable. Variables in Convex.jl are declared using the Variable keyword, along with the dimensions of the variable. :","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"# Scalar variable\nx = Variable()\n\n# Column vector variable\nx = Variable(5)\n\n# Matrix variable\nx = Variable(4, 6)","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"Variables may also be declared as having special properties, such as being","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"(entrywise) positive: x = Variable(4, Positive())\n(entrywise) negative: x = Variable(4, Negative())\nintegral: x = Variable(4, :Int)\nbinary: x = Variable(4, :Bin)\n(for a matrix) being symmetric, with nonnegative eigenvalues (ie,   positive semidefinite): z = Semidefinite(4)","category":"page"},{"location":"types/#Constants-1","page":"Basic Types","title":"Constants","text":"","category":"section"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"Numbers, vectors, and matrices present in the Julia environment are wrapped automatically into a Constant expression when used in a Convex.jl expression.","category":"page"},{"location":"types/#Expressions-1","page":"Basic Types","title":"Expressions","text":"","category":"section"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"Expressions in Convex.jl are formed by applying any atom (mathematical function defined in Convex.jl) to variables, constants, and other expressions. For a list of these functions, see Operations. Atoms are applied to expressions using operator overloading. For example, 2+2 calls Julia's built-in addition operator, while 2+x calls the Convex.jl addition method and returns a Convex.jl expression. Many of the useful language features in Julia, such as arithmetic, array indexing, and matrix transpose are overloaded in Convex.jl so they may be used with variables and expressions just as they are used with native Julia types.","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"Expressions that are created must be DCP-compliant. More information on DCP can be found here. :","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"x = Variable(5)\n# The following are all expressions\ny = sum(x)\nz = 4 * x + y\nz_1 = z[1]","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"Convex.jl allows the values of the expressions to be evaluated directly.","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"x = Variable()\ny = Variable()\nz = Variable()\nexpr = x + y + z\nproblem = minimize(expr, x >= 1, y >= x, 4 * z >= y)\nsolve!(problem, SCSSolver())\n\n# Once the problem is solved, we can call evaluate() on expr:\nevaluate(expr)","category":"page"},{"location":"types/#Constraints-1","page":"Basic Types","title":"Constraints","text":"","category":"section"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"Constraints in Convex.jl are declared using the standard comparison operators <=, >=, and ==. They specify relations that must hold between two expressions. Convex.jl does not distinguish between strict and non-strict inequality constraints.","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"x = Variable(5, 5)\n# Equality constraint\nconstraint = x == 0\n# Inequality constraint\nconstraint = x >= 1","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"Matrices can also be constrained to be positive semidefinite.","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"x = Variable(3, 3)\ny = Variable(3, 1)\nz = Variable()\n# constrain [x y; y' z] to be positive semidefinite\nconstraint = ([x y; y' z] in :SDP)\n# or equivalently,\nconstraint = ([x y; y' z] ⪰ 0)","category":"page"},{"location":"types/#Objective-1","page":"Basic Types","title":"Objective","text":"","category":"section"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"The objective of the problem is a scalar expression to be maximized or minimized by using maximize or minimize respectively. Feasibility problems can be expressed by either giving a constant as the objective, or using problem = satisfy(constraints).","category":"page"},{"location":"types/#Problem-1","page":"Basic Types","title":"Problem","text":"","category":"section"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"A problem in Convex.jl consists of a sense (minimize, maximize, or satisfy), an objective (an expression to which the sense verb is to be applied), and zero or more constraints that must be satisfied at the solution. Problems may be constructed as","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"problem = minimize(objective, constraints)\n# or\nproblem = maximize(objective, constraints)\n# or\nproblem = satisfy(constraints)","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"Constraints can be added at any time before the problem is solved.","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"# No constraints given\nproblem = minimize(objective)\n# Add some constraint\nproblem.constraints += constraint\n# Add many more constraints\nproblem.constraints += [constraint1, constraint2, ...]","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"A problem can be solved by calling solve!","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"solve!(problem, solver)","category":"page"},{"location":"types/#","page":"Basic Types","title":"Basic Types","text":"passing a solver such as SCSSolver() from the package SCS as the second argument. After the problem is solved, problem.status records the status returned by the optimization solver, and can be :Optimal, :Infeasible, :Unbounded, :Indeterminate or :Error. If the status is :Optimal, problem.optval will record the optimum value of the problem. The optimal value for each variable x participating in the problem can be found in x.value. The optimal value of an expression can be found by calling the evaluate() function on the expression as follows: evaluate(expr).","category":"page"}]
}
