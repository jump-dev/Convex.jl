var documenterSearchIndex = {"docs":
[{"location":"introduction/quick_tutorial/#Quick-Tutorial","page":"Quick Tutorial","title":"Quick Tutorial","text":"Consider a constrained least squares problem\n\nbeginaligned\nbeginarrayll\ntextminimize  Ax - b_2^2 \ntextsubject to  x geq 0\nendarray\nendaligned\n\nwith variable xin mathbfR^n, and problem data A in mathbfR^m times n, b in mathbfR^m.\n\nThis problem can be solved in Convex.jl as follows:\n\nusing Convex, SCS\nm = 4;  n = 5\nA = randn(m, n); b = randn(m)\nx = Variable(n)\nproblem = minimize(sumsquares(A * x - b), [x >= 0])\nsolve!(problem, SCS.Optimizer; silent = true)\nproblem.status\nproblem.optval\nx.value","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Convex-Optimization-in-Julia","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Madeleine-Udell-ISMP-2015","page":"Convex Optimization in Julia","title":"Madeleine Udell | ISMP 2015","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Convex.jl-team","page":"Convex Optimization in Julia","title":"Convex.jl team","text":"Convex.jl: Madeleine Udell, Karanveer Mohan, David Zeng, Jenny Hong","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Collaborators/Inspiration:","page":"Convex Optimization in Julia","title":"Collaborators/Inspiration:","text":"CVX: Michael Grant, Stephen Boyd\nCVXPY: Steven Diamond, Eric Chu, Stephen Boyd\nJuliaOpt: Miles Lubin, Iain Dunning, Joey Huchette\n\ninitial package installation\n\nMake the Convex.jl module available\n\nusing Convex, SparseArrays, LinearAlgebra\nusing SCS # first order splitting conic solver [O'Donoghue et al., 2014]\n\nGenerate random problem data\n\nm = 50;\nn = 100;\nA = randn(m, n)\nx♮ = sprand(n, 1, 0.5) # true (sparse nonnegative) parameter vector\nnoise = 0.1 * randn(m)    # gaussian noise\nb = A * x♮ + noise      # noisy linear observations\n\nCreate a (column vector) variable of size n.\n\nx = Variable(n)\n\nnonnegative elastic net with regularization\n\nλ = 1\nμ = 1\nproblem = minimize(\n    square(norm(A * x - b)) + λ * square(norm(x)) + μ * norm(x, 1),\n    x >= 0,\n)\n\nSolve the problem by calling solve!\n\nsolve!(problem, SCS.Optimizer; silent = true)\n\nprintln(\"problem status is \", problem.status)\nprintln(\"optimal value is \", problem.optval)\n\nusing Interact, Plots\n# Interact.WebIO.install_jupyter_nbextension() # might be helpful if you see `WebIO` warnings in Jupyter\n@manipulate throttle = 0.1 for λ in 0:0.1:5, μ in 0:0.1:5\n    global A\n    problem = minimize(\n        square(norm(A * x - b)) + λ * square(norm(x)) + μ * norm(x, 1),\n        x >= 0,\n    )\n    solve!(problem, SCS.Optimizer; silent = true)\n    histogram(evaluate(x), xlims = (0, 3.5), label = \"x\")\nend\nnothing # hide","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Quick-convex-prototyping","page":"Convex Optimization in Julia","title":"Quick convex prototyping","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Variables","page":"Convex Optimization in Julia","title":"Variables","text":"Scalar variable\n\nx = Variable()\n\n(Column) vector variable\n\ny = Variable(4)\n\nMatrix variable\n\nZ = Variable(4, 4)","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Expressions","page":"Convex Optimization in Julia","title":"Expressions","text":"Convex.jl allows you to use a wide variety of functions on variables and on expressions to form new expressions.\n\nx + 2x\n\ne = y[1] + logdet(Z) + sqrt(x) + minimum(y)","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Examine-the-expression-tree","page":"Convex Optimization in Julia","title":"Examine the expression tree","text":"e.children[2]","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Constraints","page":"Convex Optimization in Julia","title":"Constraints","text":"A constraint is convex if convex combinations of feasible points are also feasible. Equivalently, feasible sets are convex sets.\n\nIn other words, convex constraints are of the form\n\nconvexExpr <= 0\nconcaveExpr >= 0\naffineExpr == 0\n\nx <= 0\n\nsquare(x) <= sum(y)\n\nM = Z\nfor i in 1:length(y)\n    global M += rand(size(Z)...) * y[i]\nend\nM ⪰ 0","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Problems","page":"Convex Optimization in Julia","title":"Problems","text":"x = Variable()\ny = Variable(4)\nobjective = 2 * x + 1 - sqrt(sum(y))\nconstraint = x >= maximum(y)\np = minimize(objective, constraint)\n\nSolve the problem:\n\nsolve!(p, SCS.Optimizer; silent = true)\np.status\n\nevaluate(x)\n\nCan evaluate expressions directly:\n\nevaluate(objective)","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Pass-to-solver","page":"Convex Optimization in Julia","title":"Pass to solver","text":"call a MathProgBase solver suited for your problem class\n\nsee the list of Convex.jl operations to find which cones you're using\nsee the list of solvers for an up-to-date list of solvers and which cones they support\n\nto solve problem using a different solver, just import the solver package and pass the solver to the solve! method:\n\nusing Mosek\nsolve!(p, Mosek.Optimizer)","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Warmstart","page":"Convex Optimization in Julia","title":"Warmstart","text":"Generate random problem data:\n\nm = 50;\nn = 100;\nA = randn(m, n)\nx♮ = sprand(n, 1, 0.5) # true (sparse nonnegative) parameter vector\nnoise = 0.1 * randn(m)    # gaussian noise\nb = A * x♮ + noise      # noisy linear observations\n\nCreate a (column vector) variable of size n.\n\nx = Variable(n)\n\nnonnegative elastic net with regularization\n\nλ = 1\nμ = 1\nproblem = minimize(\n    square(norm(A * x - b)) + λ * square(norm(x)) + μ * norm(x, 1),\n    x >= 0,\n)\n@time solve!(problem, SCS.Optimizer; silent = true)\nλ = 1.5\n@time solve!(problem, SCS.Optimizer; silent = true)#, warmstart = true) # FIXME","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#DCP-examples","page":"Convex Optimization in Julia","title":"DCP examples","text":"affine\n\nx = Variable(4)\ny = Variable(2)\nsum(x) + y[2]\n\n2 * maximum(x) + 4 * sum(y) - sqrt(y[1] + x[1]) - 7 * minimum(x[2:4])\n\nnot DCP compliant\n\nlog(x) + square(x)\n\nComposition fcirc g where f is convex increasing and g is convex\n\nsquare(pos(x))\n\nComposition fcirc g where f is convex decreasing and g is concave\n\ninvpos(sqrt(x))\n\nComposition fcirc g where f is concave increasing and g is concave\n\nsqrt(sqrt(x))\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/general_examples/huber_regression/#Huber-regression","page":"Huber regression","title":"Huber regression","text":"This example can be found here: https://web.stanford.edu/~boyd/papers/pdf/cvx_applications.pdf. Here we set big_example = false to only generate a small example which takes less time to run.\n\nbig_example = false\nif big_example\n    n = 300\n    number_tests = 50\nelse\n    n = 50\n    number_tests = 10\nend\n\nGenerate data for Huber regression.\n\nusing Random\nRandom.seed!(1);\nnumber_samples = round(Int, 1.5 * n);\nbeta_true = 5 * randn(n);\nX = randn(n, number_samples);\nY = zeros(number_samples);\nv = randn(number_samples);\nnothing #hide\n\nGenerate data for different values of p. Solve the resulting problems.\n\nusing Convex, SCS, Distributions\nlsq_data = zeros(number_tests);\nhuber_data = zeros(number_tests);\nprescient_data = zeros(number_tests);\np_vals = range(0, stop = 0.15, length = number_tests);\nfor i in 1:length(p_vals)\n    p = p_vals[i]\n    # Generate the sign changes.\n    factor = 2 * rand(Binomial(1, 1 - p), number_samples) .- 1\n    Y = factor .* X' * beta_true + v\n\n    # Form and solve a standard regression problem.\n    beta = Variable(n)\n    fit = norm(beta - beta_true) / norm(beta_true)\n    cost = norm(X' * beta - Y)\n    prob = minimize(cost)\n    solve!(prob, SCS.Optimizer; silent = true)\n    lsq_data[i] = evaluate(fit)\n\n    # Form and solve a prescient regression problem,\n    # that is, where the sign changes are known.\n    cost = norm(factor .* (X' * beta) - Y)\n    solve!(minimize(cost), SCS.Optimizer; silent = true)\n    prescient_data[i] = evaluate(fit)\n\n    # Form and solve the Huber regression problem.\n    cost = sum(huber(X' * beta - Y, 1))\n    solve!(minimize(cost), SCS.Optimizer; silent = true)\n    huber_data[i] = evaluate(fit)\nend\n\nusing Plots\n\nplot(p_vals, huber_data, label = \"Huber\", xlabel = \"p\", ylabel = \"Fit\")\nplot!(p_vals, lsq_data, label = \"Least squares\")\nplot!(p_vals, prescient_data, label = \"Prescient\")\n\n# Plot the relative reconstruction error for Huber and prescient regression,\n# zooming in on smaller values of p.\nindices = findall(p_vals .<= 0.08);\nplot(p_vals[indices], huber_data[indices], label = \"Huber\")\nplot!(p_vals[indices], prescient_data[indices], label = \"Prescient\")\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/general_examples/dualization/#Dualization","page":"Dualization","title":"Dualization","text":"Sometimes it can be much faster to solve the dual problem than the primal problem. Some solvers will automatically dualize the problem when heuristics deem it beneficial, and alternative DCP modeling languages like CVX will also automatically dualize the problem in some cases. Convex.jl does not automatically dualize any problem, but it is easy to manually do so with Dualization.jl. Here, we will solve a simple semidefinite program (from issue #492) to show how easy it is to dualize the problem, and that it can potentially provide speed ups.\n\nFirst we load our packages:\n\nusing LinearAlgebra\nusing Convex\nusing SCS\nusing Random\nusing Dualization\n\nThen we setup some test data.\n\nRandom.seed!(2022)\np = 50\nΣ = Symmetric(randn(p, p))\nΣ = Σ * Σ'\n\nNow we formulate and solve our primal problem:\n\nd = Variable(p)\nproblem = maximize(sum(d), 0 ≤ d, d ≤ 1, Σ ⪰ Diagonal(d))\n@time solve!(problem, SCS.Optimizer; silent = true)\n\nTo solve the dual problem instead, we simply call dual_optimizer on our optimizer function:\n\n@time solve!(problem, dual_optimizer(SCS.Optimizer); silent = true)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/general_examples/lasso_regression/#Lasso,-Ridge-and-Elastic-Net-Regressions","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"This notebook presents a simple implementation of Lasso and elastic net regressions.","category":"section"},{"location":"examples/general_examples/lasso_regression/#Load-Packages-and-Extra-Functions","page":"Lasso, Ridge and Elastic Net Regressions","title":"Load Packages and Extra Functions","text":"using DelimitedFiles, LinearAlgebra, Statistics, Plots, Convex, SCS","category":"section"},{"location":"examples/general_examples/lasso_regression/#Loading-Data","page":"Lasso, Ridge and Elastic Net Regressions","title":"Loading Data","text":"We use the diabetes data from Efron et al, downloaded from https://web.stanford.edu/~hastie/StatLearnSparsity_files/DATA/diabetes.html and then converted from a tab to a comma delimited file.\n\nAll data series are standardised (see below) to have zero means and unit standard deviation, which improves the numerical stability. (Efron et al do not standardise the scale of the response variable.)\n\nx, header =\n    readdlm(joinpath(@__DIR__, \"aux_files/diabetes.csv\"), ',', header = true)\nx = (x .- mean(x, dims = 1)) ./ std(x, dims = 1) # standardise\n(Y, X) = (x[:, end], x[:, 1:end-1]); # to get traditional names\nxNames = header[1:end-1]","category":"section"},{"location":"examples/general_examples/lasso_regression/#Lasso,-Ridge-and-Elastic-Net-Regressions-2","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"(a)  The regression is Y = Xb + u, where Y and u are T times 1, X is T times K, and b is the K-vector of regression coefficients.\n\n(b) We want to minimize (Y-Xb)(Y-Xb)T + gamma sum b_i + lambda sum b_i^2.\n\n(c) We can equally well minimise bQb - 2cb + gamma sum b_i + lambda sum b_i^2, where Q = XXT and c=XYT.\n\n(d) Lasso: gamma0lambda=0; Ridge: gamma=0lambda0; elastic net: gamma0lambda0.\n\n\"\"\"\n    LassoEN(Y,X,γ,λ)\n\nDo Lasso (set γ>0,λ=0), ridge (set γ=0,λ>0) or elastic net regression (set γ>0,λ>0).\n\n\n# Input\n- `Y::Vector`:     T-vector with the response (dependent) variable\n- `X::VecOrMat`:   TxK matrix of covariates (regressors)\n- `γ::Number`:     penalty on sum(abs.(b))\n- `λ::Number`:     penalty on sum(b.^2)\n\n\"\"\"\nfunction LassoEN(Y, X, γ, λ = 0)\n    (T, K) = (size(X, 1), size(X, 2))\n\n    b_ls = X \\ Y                    #LS estimate of weights, no restrictions\n\n    Q = X'X / T\n    c = X'Y / T                      #c'b = Y'X*b\n\n    b = Variable(K)              #define variables to optimize over\n    L1 = quadform(b, Q)            #b'Q*b\n    L2 = dot(c, b)                 #c'b\n    L3 = norm(b, 1)                #sum(|b|)\n    L4 = sumsquares(b)            #sum(b^2)\n\n    if λ > 0\n        # u'u/T + γ*sum(|b|) + λ*sum(b^2), where u = Y-Xb\n        problem = minimize(L1 - 2 * L2 + γ * L3 + λ * L4)\n    else\n        # u'u/T + γ*sum(|b|) where u = Y-Xb\n        problem = minimize(L1 - 2 * L2 + γ * L3)\n    end\n    solve!(problem, SCS.Optimizer; silent = true)\n    problem.status == Convex.MOI.OPTIMAL ? b_i = vec(evaluate(b)) : b_i = NaN\n\n    return b_i, b_ls\nend\n\nThe next cell makes a Lasso regression for a single value of γ.\n\nK = size(X, 2)\nγ = 0.25\n\n(b, b_ls) = LassoEN(Y, X, γ)\n\nprintln(\"OLS and Lasso coeffs (with γ=$γ)\")\nprintln([[\"\" \"OLS\" \"Lasso\"]; xNames b_ls b])","category":"section"},{"location":"examples/general_examples/lasso_regression/#Redo-the-Lasso-Regression-with-Different-Gamma-Values","page":"Lasso, Ridge and Elastic Net Regressions","title":"Redo the Lasso Regression with Different Gamma Values","text":"We now loop over gamma values.\n\nRemark: it would be quicker to put this loop inside the LassoEN() function so as to not recreate L1-L4.\n\nnγ = 101\nγM = range(0; stop = 1.5, length = nγ)             #different γ values\n\nbLasso = fill(NaN, size(X, 2), nγ)       #results for γM[i] are in bLasso[:,i]\nfor i in 1:nγ\n    sol, _ = LassoEN(Y, X, γM[i])\n    bLasso[:, i] .= sol\nend\n\nplot(\n    log10.(γM),\n    bLasso',\n    title = \"Lasso regression coefficients\",\n    xlabel = \"log10(γ)\",\n    label = permutedims(xNames),\n    size = (600, 400),\n)","category":"section"},{"location":"examples/general_examples/lasso_regression/#Ridge-Regression","page":"Lasso, Ridge and Elastic Net Regressions","title":"Ridge Regression","text":"We use the same function to do a ridge regression. Alternatively, do b = inv(X'X/T + λ*I)*X'Y/T.\n\nnλ = 101\nλM = range(0; stop = 7.5, length = nλ)\n\nbRidge = fill(NaN, size(X, 2), nλ)\nfor i in 1:nλ\n    sol, _ = LassoEN(Y, X, 0, λM[i])\n    bRidge[:, i] .= sol\nend\n\nplot(\n    log10.(λM),\n    bRidge',\n    title = \"Ridge regression coefficients\",\n    xlabel = \"log10(λ)\",\n    label = permutedims(xNames),\n    size = (600, 400),\n)","category":"section"},{"location":"examples/general_examples/lasso_regression/#Elastic-Net-Regression","page":"Lasso, Ridge and Elastic Net Regressions","title":"Elastic Net Regression","text":"λ = 0.5\nprintln(\"redo the Lasso regression, but with λ=$λ: an elastic net regression\")\n\nbEN = fill(NaN, size(X, 2), nγ)\nfor i in 1:nγ\n    sol, _ = LassoEN(Y, X, γM[i], λ)\n    bEN[:, i] .= sol\nend\n\nplot(\n    log10.(γM),\n    bEN',\n    title = \"Elastic Net regression coefficients\",\n    xlabel = \"log10(γ)\",\n    label = permutedims(xNames),\n    size = (600, 400),\n)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/general_examples/optimal_advertising/#Optimal-advertising","page":"Optimal advertising","title":"Optimal advertising","text":"This example is taken from https://web.stanford.edu/~boyd/papers/pdf/cvx_applications.pdf.\n\nSetup:\n\nWe have m adverts and n time slots\nThe total traffic in time slot t is T_t\nThe number of ad i displayed in period t is D_it geq 0\nWe require sum_i=1^m D_it leq T_t since we cannot show more than T_t ads during time slot t.\nWe require sum_t=1^n D_it geq c_i to fulfill a contract to show advertisement i at least c_i times.\n\nGoal: choose D_it.\n\nFor some empirical P_it with 0 leq P_it leq 1, we obtain C_it = P_itD_it clicks for ad i, which pays us some number R_i  0 up to a budget B_i. The ad revenue for ad i is S_i = min( R_i sum_t C_it B_i ) which is concave in D. We aim to maximize sum_i S_i.\n\nusing Random\nusing Distributions: LogNormal\nRandom.seed!(1);\n\nm = 5; # number of adverts\nn = 24; # number of time slots\nSCALE = 10000;\nB = rand(LogNormal(8), m) .+ 10000;\nB = round.(B, digits = 3); # Budget\n\nP_ad = rand(m);\nP_time = rand(1, n);\nP = P_ad * P_time;\n\nT = sin.(range(-2 * pi / 2, stop = 2 * pi - 2 * pi / 2, length = n)) * SCALE;\nT .+= -minimum(T) + SCALE; # traffic\nc = rand(m); # contractual minimum\nc *= 0.6 * sum(T) / sum(c);\nc = round.(c, digits = 3);\nR = [rand(LogNormal(minimum(c) / c[i]), 1) for i in 1:m]; # revenue\nnothing #hide\n\n# Form and solve the optimal advertising problem.\nusing Convex, SCS;\nD = Variable(m, n);\nSi = vcat([min(R[i] * dot(P[i, :], D[i, :]'), B[i]) for i in 1:m]...);\nproblem =\n    maximize(sum(Si), [D >= 0, sum(D, dims = 1)' <= T, sum(D, dims = 2) >= c]);\nsolve!(problem, SCS.Optimizer; silent = true)\n\nPlot traffic.\n\nusing Plots\nplot(1:length(T), T, xlabel = \"hour\", ylabel = \"Traffic\")\n\nPlot P.\n\nheatmap(P)\n\nPlot optimal D.\n\nheatmap(evaluate(D))\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"manual/performance_tips/#Performance-tips","page":"Performance tips","title":"Performance tips","text":"There are three phases of building and solving a Convex.jl problem:\n\nBuilding the expression tree. This is everything that happens before solve! is called, for example, creating variables, problems, constraints, and other expressions.\nFormulating the problem in MathOptInterface. This happens automatically when solve! is called, and (as long as solve! is not called with silent=true) emits a log message when completed, containing the amount of time it took and the total amount of memory allocations.\nSolving the problem. This happens in solve! after the problem is formulated. When silent=false, typically solvers will print information about the problem and their progress when solving it.\n\nIf you are experiencing performance issues, it is important to isolate which of these three steps is taking the most time. Typically, step (1) should be the fastest, then step (2), then step (3).\n\nIf building the expression tree (step 1 above) is the slowest part, there is likely a performance bug in Convex.jl. Please file an issue with a reproducible minimal example; it may be able to be quickly fixed.\nIf formulating the problem is much slower than solving it, there may be a performance issue in Convex.jl. See Faster problem formulation: Avoid scalar indexing below for one tip to speed things up.\nIf solving the problem is the slowest part, you may still be able to improve performance. See Speeding up solve times: dualization and Speeding up solve times: digging into the final formulation below. Also consider trying other solvers. For large problems, first-order solvers like SCS and COSMO may be faster (albeit with higher convergence tolerances) than second-order solvers like Clarabel and Hypatia.\n\nIf you're struggling to improve performance, feel free to ask for help in the community forum. Before asking a question, make sure to read the post make it easier to help you, which contains a number of tips on how to ask a good question.","category":"section"},{"location":"manual/performance_tips/#Faster-problem-formulation:-Avoid-scalar-indexing","page":"Performance tips","title":"Faster problem formulation: Avoid scalar indexing","text":"When manipulating Convex.jl expressions, try to use a \"vectorized\" style, and avoid loops and scalar indexing. For example:\n\nfunction bad_my_dot(x, y) # avoid this kind of code for Convex.jl expressions!\n    s = 0.0\n    for i in eachindex(x, y)\n        s += x[i]*y[i]\n    end\n    return s\nend\n\nis usually perfectly reasonable Julia code (although perhaps s should be initialized as s=zero(promote_type(eltype(x), eltype(y)))) to be more generic), since loops are typically fast in Julia.\n\nHowever, when using Convex.jl expressions like x = Variable(100), calling bad_my_dot(x, y) with y = rand(100) will create 100 separate IndexAtoms, each of which has some overhead. That is because Convex maintains a lazy expression tree representing the problem. Instead, the vectorized form sum(x .* y) or LinearAlgebra.dot(x, y) will be more efficient.\n\nIndexing style typically should have no effect on solve times (step (3) above), only on constructing the expression tree and formulating the problem.","category":"section"},{"location":"manual/performance_tips/#Speeding-up-solve-times:-dualization","page":"Performance tips","title":"Speeding up solve times: dualization","text":"Depending on the solver and the problem, it can sometimes speed things up quite a lot to pass the solver the dual problem to solve rather than the primal problem. This can be accomplished easily with Dualization.jl, simply by passing dual_optimizer(optimizer) instead of optimizer to solve!. See Dualization for an example.","category":"section"},{"location":"manual/performance_tips/#Speeding-up-solve-times:-digging-into-the-final-formulation","page":"Performance tips","title":"Speeding up solve times: digging into the final formulation","text":"Convex.jl is built around conic programming, and it works with conic solvers. This may be a different optimization methodology than you may be used to, and it works by formulating problems in terms of affine objective functions and affine-function-in-cone constraints for a variety of convex cones. Convex.jl reformulates all problems to this form. For example, the objective function you give Convex will not actually be executed line-by-line; instead, it will be reformulated to a possibly very-different looking form for the conic solver to handle.\n\nConvex.jl provides an abstraction by automatically performing these reformulations, but as often is the case, performance leaks through. To have the best performance, you might need to learn more details about the various cones supported by your solver, by MathOptInterface, and how Convex.jl reformulates your problem. There could be mathematically equivalent formulations that are faster, and in some cases it could be that Convex's automatic reformulations can be improved.\n\nOne option to view the final problem after these reformulations is to write it to disk with Convex.write_to_file and inspect it manually or with other software.","category":"section"},{"location":"changelog/#Release-notes","page":"Release notes","title":"Release notes","text":"The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.","category":"section"},{"location":"changelog/#v0.16.4-(November-19,-2024)","page":"Release notes","title":"v0.16.4 (November 19, 2024)","text":"","category":"section"},{"location":"changelog/#Added","page":"Release notes","title":"Added","text":"Added support for indexing with BitVector and BitMatrix (#708) (#710)","category":"section"},{"location":"changelog/#v0.16.3-(September-10,-2024)","page":"Release notes","title":"v0.16.3 (September 10, 2024)","text":"","category":"section"},{"location":"changelog/#Fixed","page":"Release notes","title":"Fixed","text":"Fixed an incorrect type assertion in real_operate (#704)","category":"section"},{"location":"changelog/#v0.16.2-(June-21,-2024)","page":"Release notes","title":"v0.16.2 (June 21, 2024)","text":"","category":"section"},{"location":"changelog/#Fixed-2","page":"Release notes","title":"Fixed","text":"Fixed a bug adding constraints that had ConstVexity functions (#699)","category":"section"},{"location":"changelog/#v0.16.1-(June-18,-2024)","page":"Release notes","title":"v0.16.1 (June 18, 2024)","text":"","category":"section"},{"location":"changelog/#Added-2","page":"Release notes","title":"Added","text":"Added a dims argument to LogSumExpAtom and logsumexp that reduces the summation along dims (#692)\nAdded support for AbstractArray arguments to relative_entropy and log_perspective (#695)","category":"section"},{"location":"changelog/#Other","page":"Release notes","title":"Other","text":"Minor updates to the README (#687), (#688), (#689), (#690)\nAdded API documentation (#610)\nFixed solve! docstring (#693)","category":"section"},{"location":"changelog/#v0.16.0-(May-21,-2024)","page":"Release notes","title":"v0.16.0 (May 21, 2024)","text":"This release contains a large number of changes, including some breaking changes. However, despite the large number of changes, most user code should not need to change.\n\nIf you encounter an issue updating to this release, please open a GitHub issue, or post on the JuMP community forum: https://jump.dev/forum.","category":"section"},{"location":"changelog/#Breaking","page":"Release notes","title":"Breaking","text":"This release involved a substantial rewrite of Convex.jl to integrate better with MathOptInterface. (#504), (#551), (#584), (#588), (#637)\nx + A will error if x is a scalar variable and A is an array. Instead, use x * ones(size(A)) + A.\nThe RelativeEntropyAtom now returns a scalar value instead of elementwise values. This does not affect the result of relative_entropy.\nThe function constant should be used instead of the type Constant (which now refers to exclusively real constants).\nThe constraint a <= b now produces a - b in Nonpositives() instead of b - a in Nonnegatives(). The primal solutions are equivalent, but the dual variable associated with such constraints is now reversed in sign. (Following the convention in MathOptInterface, the dual of a <= b is always negative, regardless of optimization sense.) (#593)\nThe structs LtConstraint, GtConstraint, EqConstraint SOCConstraint, ExpConstraint, SDPConstraint, GeoMeanEpiConeConstraint, GeoMeanHypoConeConstraint, and RelativeEntropyEpiCone, have been replaced by Constraint{S} where S<:MOI.AbstractSet (#590), (#597), (#598), (#599), (#601), (#602), (#604), (#623), (#632), (#648), (#663), (#665)\nThe set GeomMeanEpiCone has been renamed to GeometricMeanEpiConeSquare and GeomMeanHypoCone has been renamed to GeometricMeanHypoConeSquare (#638)\nSubtle breaking change: scalar row indexing like x[i, :] now produces a column vector instead of a row vector. This better aligns with Julia Base, but it can result in subtle differences, particularly for code like x[i, :] * y[i, :]': this used to be equivalent to the inner product, but it is now the outer product. In Base Julia, this is the outer product, so the previous code may have been silently broken (#624)\nThe syntaxes dot(*), dot(/) and dot(^) have been removed in favor of explicit broadcasting (x .* y, x ./ y, and x .^ y). These were (mild) type piracy. In addition, vecdot(x,y) has been removed. Call dot(vec(x), vec(y)) instead. (#524)\nThe function constraints, used to get constraints associated to an individual variable, has been renamed get_constraints (#527)\nDCP violations now throw a DCPViolationError exception, rather than a warning. Relatedly, Convex.emit_dcp_warnings has been removed (#523)\nThe strict inequalities > and < have been deprecated. They will be removed in the next breaking release. Note that these never enforced strict inequalities, but instead were equivalent to >= and <= respectively (#555)\nThe functions norm_inf, norm_1, and norm_fro have been deprecated. They will be removed in the next breaking release (#567)\nThe syntax x in :PSD to create a semidefinite constraint is deprecated and will be removed in the next breaking release (#578)\nFixed setting a Constant objective function. This is breaking because it now has an objective sense instead of ignoring the objective. (#581)\nquadform now errors when fixed variables are used instead of silently giving incorrect answers if the value of the fixed variable is modified between solves (#586)\nThe Context struct has been refactored and various fields have been changed. The internal details are now considered private. (#645)\nThe keyword argument silent_solver has been deprecated to silent. (#670)\nConcatenating lists of constraints using + (and +=) has been deprecated. (#659)","category":"section"},{"location":"changelog/#Added-3","page":"Release notes","title":"Added","text":"SDP, SOC, and exponential cone constraints now have dual values populated (#504)\ngeomean supports more than 2 arguments (#504)\nAdded Convex.Optimizer (#511), (#530), (#534)\nAdded write_to_file (#531), (#591)\nAdded entropy_elementwise (#570)\nnorm on AbstractExpr objects now supports matrices (treating them like vectors), matching Base's behavior (#528)\nAdded root_det (#605)\nAdded VcatAtom which is a more efficient implementation of vcat (#607)\nAdded support for SparseArrays.SparseMatrixCSC in Constant. This fixed performance problems with some atoms (#631)\nsolve! now reports the time and memory allocation during compilation from the DCP expression graph to MathOptInterface (#633)\nAdded support for using Problem as an atom (#646)\nshow(::IO, ::Problem) now includes some problem statistics (#650)\nshow(::IO, ::Problem) now prints less of the expression tree by default (#661)\nA new example for quantum conditional entropy has been added. (#671)\nsolve! now returns the problem itself (#658)","category":"section"},{"location":"changelog/#Fixed-3","page":"Release notes","title":"Fixed","text":"sumlargesteigs now enforces that it's argument is hermitian. (#504)\nType piracy of imag and real has been removed. This should not affect use of Convex. (#504)\nFix dot to now correctly complex-conjugates its first argument (#524)\nFixed ambiguities identified by Aqua.jl (#642), (#647)\nAdd tests and fix  a number of bugs in various atoms (#546), (#547), (#550), (#554), (#556), (#558), (#559), (#561), (#562), (#563), (#565), (#566), (#567), (#568), (#608), (#609), (#617), (#626), (#654), (#655)\nFixed performance issues in a number of issues related to scalar indexing (#618), (#619), (#620), (#621), (#625), (#634)\nFixed show for Problem (#649)\nsumsquares has a more efficient formulation (#678)\nFixed vexity of lieb_ando (#684)","category":"section"},{"location":"changelog/#Other-2","page":"Release notes","title":"Other","text":"Improved the documentation (#506), (#517), (#529), (#571), (#573), (#574), (#576), (#579), (#587), (#594), (#628), (#652), (#656), (#666), (#674), (#686)\nRefactored the tests into a functional form (#532)\nUpdated Project.toml (#535)\nAdded test/Project.toml (#536)\nRefactored imports to explicitly overload methods (#537)\nTidied and renamed various atoms and files clarity. This should be non-breaking as no public API was changed. (#538), (#539), (#540), (#541), (#543), (#545), (#549), (#553), (#582), (#583)\nRemoved the unused file src/problem_depot/problems/benchmark.jl (#560)\nAdded various tests to improve code coverage (#522), (#572), (#575), (#577), (#580)\nUpdated versions in GitHub actions (#596), (#612), (#629)\nAdded license headers (#606)","category":"section"},{"location":"changelog/#v0.15.4-(October-24,-2023)","page":"Release notes","title":"v0.15.4 (October 24, 2023)","text":"Convex's piracy of hcat and vcat was made less severe, allowing precompilation of Convex.jl on Julia 1.10.","category":"section"},{"location":"changelog/#v0.15.3-(February-11,-2023)","page":"Release notes","title":"v0.15.3 (February 11, 2023)","text":"Add support for LDLFactorizations v0.10 #496.\nReplace randn(m, 1) with randn(m) to be more Julian #498.\nAdd support for indexing expressions with CartesianIndex #500.","category":"section"},{"location":"changelog/#v0.15.2-(August-10,-2022)","page":"Release notes","title":"v0.15.2 (August 10, 2022)","text":"Add support for LDLFactorizations v0.9 #493.\nFix use of deprecated functions from AbstractTrees #494.","category":"section"},{"location":"changelog/#v0.15.1-(March-28,-2022)","page":"Release notes","title":"v0.15.1 (March 28, 2022)","text":"Use OrderedDict internally for reproducible results, issue: #488, fix: #489.","category":"section"},{"location":"changelog/#v0.15.0-(March-2,-2022)","page":"Release notes","title":"v0.15.0 (March 2, 2022)","text":"","category":"section"},{"location":"changelog/#Breaking-changes","page":"Release notes","title":"Breaking changes","text":"Minimum required version of Julia is now v1.6\nUpdated to MathOptInterface v1.0\nAs a consequence, many previously deprecated solver calls may stop working. For example, instead of () -> SCS.Optimizer(verbose = 0), use MOI.OptimizerWithAttributes(SCS.Optimizer, \"verbose\" => 0).","category":"section"},{"location":"changelog/#v0.14.18-(November-14,-2021)","page":"Release notes","title":"v0.14.18 (November 14, 2021)","text":"Fix typo in logisticloss for length-1 expressions which caused errors (reported in #458, fixed in #469).","category":"section"},{"location":"changelog/#v0.14.17-(November-14,-2021)","page":"Release notes","title":"v0.14.17 (November 14, 2021)","text":"Updated to become compatible with MathOptInterface v0.10, which enables compatibility with the latest version of many solvers (#467, #468).","category":"section"},{"location":"changelog/#v0.14.16-(September-25,-2021)","page":"Release notes","title":"v0.14.16 (September 25, 2021)","text":"Improve numerical stability when evaluating logsumexp (#457). Thanks @JinraeKim!","category":"section"},{"location":"changelog/#v0.14.15-(September-15,-2021)","page":"Release notes","title":"v0.14.15 (September 15, 2021)","text":"Use sparse factorization for checking for positive semi-definiteness in quadform when possible (#457). Thanks @mtanneau!\nAdd assume_psd=false argument to skip checking for positive semi-definiteness in quadform (#456).","category":"section"},{"location":"changelog/#v0.14.14-(September-8,-2021)","page":"Release notes","title":"v0.14.14 (September 8, 2021)","text":"Increase the tolerance used in checking if a matrix is positive-semi definite in quadform (#453). Thanks @numbermaniac!","category":"section"},{"location":"changelog/#v0.14.13-(July-25,-2021)","page":"Release notes","title":"v0.14.13 (July 25, 2021)","text":"fix quadform for positive semi-definite matrices (fixes a regression introduced in v0.14.11 that required strictly positive semi-definite inputs) #450.","category":"section"},{"location":"changelog/#v0.14.12-(July-19,-2021)","page":"Release notes","title":"v0.14.12 (July 19, 2021)","text":"fix size of result of evaluate on IndexAtoms #448. Thanks @hurak!","category":"section"},{"location":"changelog/#v0.14.11-(July-5,-2021)","page":"Release notes","title":"v0.14.11 (July 5, 2021)","text":"fix quadform in the complex case #444. Thanks @lrnv!","category":"section"},{"location":"changelog/#v0.14.10-(May-20,-2021)","page":"Release notes","title":"v0.14.10 (May 20, 2021)","text":"declare compatibility with BenchmarkTools v1.0 #441","category":"section"},{"location":"changelog/#v0.14.9-(May-18,-2021)","page":"Release notes","title":"v0.14.9 (May 18, 2021)","text":"fix some tests in lp_dual_abs_atom #439. Thanks @moehle!","category":"section"},{"location":"changelog/#v0.14.8-(May-4,-2021)","page":"Release notes","title":"v0.14.8 (May 4, 2021)","text":"a complete port of cvxquad thanks to @dstahlke, yielding new functions quantum_relative_entropy, quantum_entropy, trace_logm, trace_mpower, and lieb_ando, and cones GeomMeanHypoCone, GeomMeanEpiCone, and RelativeEntropyEpiCone (#418). Thanks a ton for the awesome contribution @dstahlke!","category":"section"},{"location":"changelog/#v0.14.7-(April-22,-2021)","page":"Release notes","title":"v0.14.7 (April 22, 2021)","text":"declare compatibility with BenchmarkTools v0.7 #434","category":"section"},{"location":"changelog/#v0.14.6-(March-28,-2021)","page":"Release notes","title":"v0.14.6 (March 28, 2021)","text":"Use MOI.instantiate to create the optimizer, which allows users to pass an MOI.OptimizerWithAttributes to configure solver settings #431. Thanks @odow!","category":"section"},{"location":"changelog/#v0.14.5-(March-14,-2021)","page":"Release notes","title":"v0.14.5 (March 14, 2021)","text":"allow sumlargest(x,k), sumsmallest(x,k), and sumlargesteigs(x,k) for k=0 (simply returns Constant(0)). (#429).","category":"section"},{"location":"changelog/#v0.14.4-(March-14,-2021)","page":"Release notes","title":"v0.14.4 (March 14, 2021)","text":"fixed a bug where the values of variables were being converted to Float64 even if the problem was solved in high precision. (#427).","category":"section"},{"location":"changelog/#v0.14.3-(March-10,-2021)","page":"Release notes","title":"v0.14.3 (March 10, 2021)","text":"update compatibility bounds for BenchmarkTools 0.6","category":"section"},{"location":"changelog/#v0.14.2-(February-15,-2021)","page":"Release notes","title":"v0.14.2 (February 15, 2021)","text":"added lasso, ridge, and elastic net regression examples (#420). Thanks to @PaulSoderlind!","category":"section"},{"location":"changelog/#v0.14.1-(January-24,-2021)","page":"Release notes","title":"v0.14.1 (January 24, 2021)","text":"there was a bug causing conj to act in-place (reported in #416), which has been fixed (#417). This bug appears to have existed since the introduction of conj in Convex.jl v0.5.0.","category":"section"},{"location":"changelog/#v0.14.0-(January-17,-2021)","page":"Release notes","title":"v0.14.0 (January 17, 2021)","text":"","category":"section"},{"location":"changelog/#Breaking-changes-2","page":"Release notes","title":"Breaking changes","text":"Changes to the sign of atoms:\nThe sign of sumlargesteigs has been changed from Positive() to  NoSign(), to allow non-positive-semidefinite inputs (#409). This has the potential to break code that required that sign to be positive. If you run into this problem, please file an issue so we can figure out a workaround.\nThe sign of eigmin and eigmax has been changed from Positive() to  NoSign() (#413). This is a bugfix because in general eigmin and eigmax do not need to return a positive quantity (for non-positive-semidefinite inputs). Again, this has the potential to break code that required that sign to be positive. If you run into this problem, please file an issue so we can figure out a workaround.\nRemoval of deprecations:\nlambdamin and lambdamax has been deprecated to eigmin and eigmax since Convex v0.13.0. This deprecation has been removed, so your code must be updated to call eigmin or eigmax instead (#412).\nnorm(x, p) where x is a matrix expression has been deprecated to opnorm(x,p) since Convex v0.8.0. This deprecation has been removed, so your code must be updated to call opnorm(x, p) instead (#412). Currently, norm(x,p) for a matrix\nexpression x will error, but in Convex.jl v0.15.0 it will return norm(vec(x), p).\nConvex.clearmemory() has been deprecated and unnecessary since Convex v0.12.5. This deprecation has been removed, so if this function is in your code, just delete it (#412).\nvecnorm(x, p) has been deprecated to norm(vec(x), p) since Convex v0.8.0. This deprecation has been removed, so your code must be updated to call norm(vec(x),p) instead (#412).\nOther changes:\nConvex.DCP_WARNINGS was introduced in Convex v0.13.1 to allow turning off Convex.jl's DCP warnings. This has been removed in favor of the function Convex.emit_dcp_warnings() (Commit 481fa02).","category":"section"},{"location":"changelog/#Other-changes","page":"Release notes","title":"Other changes","text":"updated nuclearnorm and sumlargesteigs to allow complex variables, and allow the argument of sumlargesteigs to be non-positive-semi-definite (#409). Thanks to @dstahlke!","category":"section"},{"location":"changelog/#v0.13.8-(December-2,-2020)","page":"Release notes","title":"v0.13.8 (December 2, 2020)","text":"add unary + for Sign and ComplexSign to allow single-argument hcat and vcat to work (#405). Thanks to @dstahlke!","category":"section"},{"location":"changelog/#v0.13.7-(September-11,-2020)","page":"Release notes","title":"v0.13.7 (September 11, 2020)","text":"fix #403 by adding the keyword argument silent_solver to solve!.","category":"section"},{"location":"changelog/#v0.13.6-(September-8,-2020)","page":"Release notes","title":"v0.13.6 (September 8, 2020)","text":"fix #401 by allowing diagm(x).","category":"section"},{"location":"changelog/#v0.13.5-(August-25,-2020)","page":"Release notes","title":"v0.13.5 (August 25, 2020)","text":"fix #398 by allowing fix!'d variables in quadform.","category":"section"},{"location":"changelog/#v0.13.4-(July-27,-2020)","page":"Release notes","title":"v0.13.4 (July 27, 2020)","text":"You can now create your own variable types by subtyping AbstractVariable. See the docs for more information. You can also add constraints directly to a variable using add_constraint! (#358).\nFunctions vexity(x::Variable), sign(x::Variable), and evaluate(x::Variable) should now be the preferred way to access properties of a variable; likewise use set_value! to set the initial value of a variable (#358).\nTo create integer or binary constraints, use the VarType enum (for example, Variable(BinVar)). Access or set this via vartype and vartype! (#358).","category":"section"},{"location":"changelog/#v0.13.3-(March-22,-2020)","page":"Release notes","title":"v0.13.3 (March 22, 2020)","text":"Make add_constraint! actually add the constraint to the problem.","category":"section"},{"location":"changelog/#v0.13.2-(March-14,-2020)","page":"Release notes","title":"v0.13.2 (March 14, 2020)","text":"Add Convex.MAXDIGITS. Thanks to @riccardomurri!","category":"section"},{"location":"changelog/#v0.13.1-(March-6,-2020)","page":"Release notes","title":"v0.13.1 (March 6, 2020)","text":"Allow disabling DCP warnings (#372)\nRestore export of Constraint (#371)","category":"section"},{"location":"changelog/#v0.13.0-(February-28,-2020)","page":"Release notes","title":"v0.13.0 (February 28, 2020)","text":"","category":"section"},{"location":"changelog/#Major-changes","page":"Release notes","title":"Major changes","text":"The intermediate layer has changed from MathProgBase.jl to MathOptInterface.jl (#330). To solve problems, one should pass a MathOptInterface optimizer constructor, such as SCS.Optimizer, or MOI.OptimizerWithAttributes(SCS.Optimizer, \"verbose\" => 0).\nlambdamin and lambdamax have been deprecated in favor of eigmin and eigmax (#357).\nMany \"internal\" functions and types are no longer exported, such as the atoms, types corresponding to constraints and vexities, etc. (#357).\nevaluate(x::Variable) and evaluate(c::Constant) now return scalars and vectors as appropriate, instead of (1,1)- and (d,1)-matrices (#359). This affects functions which used to return (1,1)-matrices; for example, now evaluate(quadform(...)) yields a scalar.","category":"section"},{"location":"examples/general_examples/svm/#Support-vector-machine","page":"Support vector machine","title":"Support vector machine","text":"","category":"section"},{"location":"examples/general_examples/svm/#Support-Vector-Machine-(SVM)","page":"Support vector machine","title":"Support Vector Machine (SVM)","text":"We are given two sets of points in bf R^n, x_1 ldots x_N and y_1 ldots y_M, and wish to find a function f(x) = w^T x - b that linearly separates the points, that is, f(x_i) geq 1 for i = 1 ldots N and f(y_i) leq -1 for i = 1 ldots M. That is, the points are separated by two hyperplanes, w^T x - b = 1 and w^T x - b = -1.\n\nPerfect linear separation is not always possible, so we seek to minimize the amount that these inequalities are violated. The violation of point x_i is textmax 1 + b - w^T x_i 0, and the violation of point y_i is textmax 1 - b + w^T y_i 0. We tradeoff the error sum_i=1^N textmax 1 + b - w^T x_i 0 + sum_i=1^M textmax 1 - b + w^T y_i 0 with the distance between the two hyperplanes, which we want to be large, via minimizing w^2.\n\nWe can write this problem as\n\nbeginarrayll\n    textminimize    w^2 + C * (sum_i=1^N textmax 1 + b - w^T x_i 0 + sum_i=1^M textmax 1 - b + w^T y_i 0)\nendarray\n\nwhere w in bf R^n and b in bf R are our optimization variables.\n\nWe can solve the problem as follows.\n\nusing Convex, SCS\n\n# Generate data.\nn = 2; # dimensionality of data\nC = 10; # inverse regularization parameter in the objective\nN = 10; # number of positive examples\nM = 10; # number of negative examples\n\nusing Distributions: MvNormal\n# positive data points\npos_data = rand(MvNormal([1.0, 2.0], 1.0), N);\n# negative data points\nneg_data = rand(MvNormal([-1.0, 2.0], 1.0), M);\nnothing #hide\n\nfunction svm(pos_data, neg_data)\n    # Create variables for the separating hyperplane w'*x = b.\n    w = Variable(n)\n    b = Variable()\n    # Form the objective.\n    obj =\n        sumsquares(w) +\n        C * sum(max(1 + b - w' * pos_data, 0)) +\n        C * sum(max(1 - b + w' * neg_data, 0))\n    # Form and solve problem.\n    problem = minimize(obj)\n    solve!(problem, SCS.Optimizer; silent = true)\n    return evaluate(w), evaluate(b)\nend;\nnothing #hide\n\nw, b = svm(pos_data, neg_data);\nnothing #hide\n\n# Plot our results.\nusing Plots\n\nGenerate the separating hyperplane\n\nline_x = -2:0.1:2;\nline_y = (-w[1] * line_x .+ b) / w[2];\nnothing #hide\n\nPlot the positive points, negative points, and separating hyperplane.\n\nplot(pos_data[1, :], pos_data[2, :], st = :scatter, label = \"Positive points\")\nplot!(neg_data[1, :], neg_data[2, :], st = :scatter, label = \"Negative points\")\nplot!(line_x, line_y, label = \"Separating hyperplane\")\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/general_examples/worst_case_analysis/#Worst-case-risk-analysis","page":"Worst case risk analysis","title":"Worst case risk analysis","text":"Generate data for worst-case risk analysis.\n\nusing Random\nusing LinearAlgebra\nRandom.seed!(2);\nn = 5;\nr = abs.(randn(n, 1)) / 15;\nSigma = 0.9 * rand(n, n) .- 0.15;\nSigma_nom = Sigma' * Sigma;\nSigma_nom .-= (maximum(Sigma_nom) - 0.9)\nSigma_nom .+= (1e-4 - eigmin(Sigma_nom)) * I(n) # ensure positive-definite\n\nForm and solve portfolio optimization problem. Here we minimize risk while requiring a 0.1 return.\n\nusing Convex, SCS\nw = Variable(n);\nret = dot(r, w);\nrisk = sum(quadform(w, Sigma_nom));\nproblem = minimize(risk, [sum(w) == 1, ret >= 0.1, norm(w, 1) <= 2])\nsolve!(problem, SCS.Optimizer; silent = true)\n\nwval = vec(evaluate(w))\n\nForm and solve worst-case risk analysis problem.\n\nSigma = Semidefinite(n);\nDelta = Variable(n, n);\nrisk = sum(quadform(wval, Sigma));\nproblem = maximize(\n    risk,\n    [\n        Sigma == Sigma_nom + Delta,\n        diag(Delta) == 0,\n        abs(Delta) <= 0.2,\n        Delta == Delta',\n    ],\n);\nsolve!(problem, SCS.Optimizer; silent = true)\n\nprintln(\n    \"standard deviation = \",\n    round(sqrt(wval' * Sigma_nom * wval), sigdigits = 2),\n);\nprintln(\n    \"worst-case standard deviation = \",\n    round(sqrt(evaluate(risk)), sigdigits = 2),\n);\nprintln(\"worst-case Delta = \");\nprintln(round.(evaluate(Delta), sigdigits = 2));\nnothing #hide\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/general_examples/max_entropy/#Entropy-Maximization","page":"Entropy Maximization","title":"Entropy Maximization","text":"Here is a constrained entropy maximization problem:\n\nbeginarrayll\n    textmaximize    -sum_i=1^n x_i log x_i \n    textsubject to  mathbf1 x = 1 \n                   Ax leq b\nendarray\n\nwhere x in mathbfR^n is our optimization variable and A in mathbfR^m times n b in mathbfR^m.\n\nTo solve this, we can simply use the entropy operation Convex.jl provides.\n\nusing Convex, SCS\n\nn = 25;\nm = 15;\nA = randn(m, n);\nb = rand(m, 1);\n\nx = Variable(n);\nproblem = maximize(entropy(x), sum(x) == 1, A * x <= b)\nsolve!(problem, SCS.Optimizer; silent = true)\n\nevaluate(x)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"introduction/faq/#FAQ","page":"FAQ","title":"FAQ","text":"","category":"section"},{"location":"introduction/faq/#Where-can-I-get-help?","page":"FAQ","title":"Where can I get help?","text":"For usage questions, please start a new post on the Julia Discourse.\n\nIf you have a reproducible example of a bug or if you have a feature request, please open a GitHub issue.","category":"section"},{"location":"introduction/faq/#How-does-Convex.jl-differ-from-JuMP?","page":"FAQ","title":"How does Convex.jl differ from JuMP?","text":"Convex.jl and JuMP are both modelling languages for mathematical programming embedded in Julia, and both interface with solvers via MathOptInterface.\n\nConvex.jl converts problems to a standard conic form. This approach requires (and certifies) that the problem is convex and DCP compliant, and guarantees global optimality of the resulting solution (if the solver succeeds. For some models, the solver may experience numerical difficulty).\n\nJuMP allows nonlinear programming through an interface that learns about functions via their derivatives. This approach is more flexible (for example, you can optimize non-convex functions), but can't guarantee global optimality if your function is not convex, or warn you if you've entered a non-convex formulation.\n\nFor linear programming, the difference is more stylistic. JuMP's syntax is scalar-based and similar to AMPL and GAMS making it easy and fast to create constraints by indexing and summation (like sum(x[i] for i in 1:n)).\n\nConvex.jl allows (and prioritizes) linear algebraic and functional constructions (like max(x, y) <= A * z); indexing and summation are also supported in Convex.jl, but are somewhat slower than in JuMP.\n\nJuMP also lets you efficiently solve a sequence of problems when new constraints are added or when coefficients are modified, whereas Convex.jl parses the problem again whenever the solve! method is called.","category":"section"},{"location":"introduction/faq/#Where-can-I-learn-more-about-Convex-Optimization?","page":"FAQ","title":"Where can I learn more about Convex Optimization?","text":"See the freely available book Convex Optimization by Boyd and Vandenberghe for general background on convex optimization.\n\nFor help understanding the rules of Disciplined Convex Programming, see the DCP tutorial website.","category":"section"},{"location":"introduction/faq/#Are-there-similar-packages-available-in-other-languages?","page":"FAQ","title":"Are there similar packages available in other languages?","text":"See CVXPY in Python and CVX in Matlab.","category":"section"},{"location":"introduction/faq/#How-does-Convex.jl-work?","page":"FAQ","title":"How does Convex.jl work?","text":"For a detailed discussion of how Convex.jl works, see our paper.","category":"section"},{"location":"introduction/faq/#How-do-I-cite-this-package?","page":"FAQ","title":"How do I cite this package?","text":"If you use Convex.jl for published work, we encourage you to cite the software using the following BibTeX citation:\n\n@article{convexjl,\n    title = {Convex Optimization in {J}ulia},\n    author ={Udell, Madeleine and Mohan, Karanveer and Zeng, David and Hong, Jenny and Diamond, Steven and Boyd, Stephen},\n    year = {2014},\n    journal = {SC14 Workshop on High Performance Technical Computing in Dynamic Languages},\n    archivePrefix = \"arXiv\",\n    eprint = {1410.4821},\n    primaryClass = \"math-oc\",\n}","category":"section"},{"location":"developer/problem_depot/#Problem-Depot","page":"Problem Depot","title":"Problem Depot","text":"Convex.jl has a submodule, ProblemDepot which holds a collection of convex optimization problems. The problems are used by Convex itself to test and benchmark its code, but can also be used by solvers to test and benchmark their code. These tests have been used with many solvers at ConvexTests.jl.\n\nProblemDepot has two main methods for accessing these problems: Convex.ProblemDepot.run_tests and Convex.ProblemDepot.benchmark_suite.\n\nFor example, to test the solver SCS on all the problems of the depot except the mixed-integer problems (which it cannot handle), run\n\nusing Convex, SCS, Test\nconst MOI = Convex.MOI\n@testset \"SCS\" begin\n    Convex.ProblemDepot.run_tests(; exclude=[r\"mip\"]) do p\n        solve!(p, MOI.OptimizerWithAttributes(SCS.Optimizer, \"verbose\" => 0, \"eps_abs\" => 1e-6))\n    end\nend","category":"section"},{"location":"developer/problem_depot/#How-to-write-a-ProblemDepot-problem","page":"Problem Depot","title":"How to write a ProblemDepot problem","text":"The problems are organized into folders in src/problem_depot/problems. Each is written as a function, annotated by @add_problem, and a name, which is used to group the problems. For example, here is a simple problem:\n\n@add_problem affine function affine_negate_atom(handle_problem!, ::Val{test}, atol, rtol, ::Type{T}) where {T, test}\n    x = Variable()\n    p = minimize(-x, [x <= 0])\n    if test\n        @test vexity(p) == AffineVexity()\n    end\n    handle_problem!(p)\n    if test\n        @test p.optval ≈ 0 atol=atol rtol=rtol\n        @test evaluate(-x) ≈ 0 atol=atol rtol=rtol\n    end\nend\n\nThe @add_problem call adds the problem to the registry of problems in Convex.ProblemDepot.PROBLEMS, which in turn is used by Convex.ProblemDepot.run_tests and Convex.ProblemDepot.benchmark_suite. Next, affine is the grouping of the problem; this problem came from one of the affine tests, and in particular is testing the negation atom. Next is the function signature:\n\nfunction affine_negate_atom(handle_problem!, ::Val{test}, atol, rtol, ::Type{T}) where {T, test}\n\nthis should be the same for every problem, except for the name, which is a description of the problem. It should include what kind of atoms it uses (affine in this case), so that certain kinds of atoms can be ruled out by the exclude keyword to Convex.ProblemDepot.run_tests and Convex.ProblemDepot.benchmark_suite; for example, many solvers cannot solve mixed-integer problems, so mip is included in the name of such problems.\n\nThen begins the body of the problem. It is setup like any other Convex.jl problem, only handle_problem! is called instead of solve!. This allows particular solvers to be used (via for example, choosing handle_problem! = p -> solve!(p, solver)), or for any other function of the problem. Tests should be included and gated behind if test blocks, so that tests can be skipped for benchmarking, or in the case that the problem is not in fact solved during handle_problem!.\n\nThe fact that the problems may not be solved during handle_problem! brings with it a small complication: any command that assumes the problem has been solved should be behind an if test check. For example, in some of the problems, real(evaluate(x)) is used, for a variable x; perhaps as\n\nx_re = real(evaluate(x))\nif test\n    @test x_re = ...\nend\n\nHowever, if the problem x is used in has not been solved, then evaluate(x) === nothing, and real(nothing) throws an error. So instead, this should be rewritten as\n\nif test\n    x_re = real(evaluate(x))\n    @test x_re = ...\nend","category":"section"},{"location":"developer/problem_depot/#Benchmark-only-problems","page":"Problem Depot","title":"Benchmark-only problems","text":"To add problems for benchmarking without tests, place problems in src/problem_depot/problems/benchmark, and include benchmark in the name. These problems will be automatically skipped during run_tests calls. For example, to benchmark the time it takes to add an SDP constraint, we have the problem\n\n@add_problem constraints_benchmark function sdp_constraint(handle_problem!, args...)\n    p = satisfy()\n    x = Variable(44, 44) # 990 vectorized entries\n    push!(p.constraints, x ⪰ 0)\n    handle_problem!(p)\n    nothing\nend\n\nHowever, this \"problem\" has no tests or interesting content for testing, so we skip it during testing. Note, we use args... in the function signature so that it may be called with the standard function signature\n\nf(handle_problem!, ::Val{test}, atol, rtol, ::Type{T}) where {T, test}","category":"section"},{"location":"manual/advanced/#Advanced-Features","page":"Advanced Features","title":"Advanced Features","text":"","category":"section"},{"location":"manual/advanced/#DCP-Errors","page":"Advanced Features","title":"DCP Errors","text":"When a problem is solved that involves an expression which is not of DCP form, an error is emitted. For example,\n\njulia> using Convex, SCS\n\njulia> x = Variable();\n\njulia> y = Variable();\n\njulia> p = minimize(log(x) + square(y), [x >= 0, y >= 0]);\n\njulia> solve!(p, SCS.Optimizer; silent = true)\n┌ Warning: Problem not DCP compliant: objective is not DCP\n└ @ Convex ~/.julia/dev/Convex/src/problems.jl:73\nERROR: DCPViolationError: Expression not DCP compliant. This either means that your problem is not convex, or that we could not prove it was convex using the rules of disciplined convex programming. For a list of supported operations, see https://jump.dev/Convex.jl/stable/operations/. For help writing your problem as a disciplined convex program, please post a reproducible example on https://jump.dev/forum.\nStacktrace:\n[...]\n\nSee Extended formulations and the DCP ruleset for more discussion on why these errors occur.","category":"section"},{"location":"manual/advanced/#Dual-Variables","page":"Advanced Features","title":"Dual Variables","text":"Convex.jl also returns the optimal dual variables for a problem. These are stored in the dual field associated with each constraint.\n\nusing Convex, SCS\nx = Variable();\nconstraint = x >= 0;\np = minimize(x, [constraint]);\nsolve!(p, SCS.Optimizer; silent = true)\nconstraint.dual","category":"section"},{"location":"manual/advanced/#Warmstarting","page":"Advanced Features","title":"Warmstarting","text":"If you're solving the same problem many times with different values of a parameter, Convex.jl can initialize many solvers with the solution to the previous problem, which sometimes speeds up the solution time. This is called a warm start.\n\nTo use this feature, pass the optional argument warmstart=true to the solve! method.\n\nusing Convex, SCS\nn = 1_000\ny = rand(n);\nx = Variable(n)\nlambda = Variable(Positive())\nfix!(lambda, 100)\nproblem = minimize(sumsquares(y - x) + lambda * sumsquares(x - 10))\n@time solve!(problem, SCS.Optimizer)\n# Now warmstart. If the solver takes advantage of warmstarts, this run will be\n# faster\nfix!(lambda, 105)\n@time solve!(problem, SCS.Optimizer; warmstart = true)","category":"section"},{"location":"manual/advanced/#Fixing-and-freeing-variables","page":"Advanced Features","title":"Fixing and freeing variables","text":"Convex.jl allows you to fix a variable x to a value by calling the fix! method. Fixing the variable essentially turns it into a constant. Fixed variables are sometimes also called parameters.\n\nfix!(x, v) fixes the variable x to the value v.\n\nfix!(x) fixes x to its current value, which might be the value obtained by solving another problem involving the variable x.\n\nTo allow the variable x to vary again, call free!(x).\n\nFixing and freeing variables can be particularly useful as a tool for performing alternating minimization on nonconvex problems. For example, we can find an approximate solution to a nonnegative matrix factorization problem with alternating minimization as follows. We use warmstarts to speed up the solution.\n\nn, k = 10, 1\nA = rand(n, k) * rand(k, n)\nx = Variable(n, k)\ny = Variable(k, n)\nproblem = minimize(sum_squares(A - x * y), [x >= 0, y >= 0])\n# initialize value of y\nset_value!(y, rand(k, n))\n# we'll do 10 iterations of alternating minimization\nfor i in 1:10\n    # first solve for x. With y fixed, the problem is convex.\n    fix!(y)\n    solve!(problem, SCS.Optimizer; warmstart = i > 1 ? true : false)\n    # Now solve for y with x fixed at the previous solution.\n    free!(y)\n    fix!(x)\n    solve!(problem, SCS.Optimizer; warmstart = true)\n    free!(x)\nend","category":"section"},{"location":"manual/advanced/#Custom-Variable-Types","page":"Advanced Features","title":"Custom Variable Types","text":"By making subtypes of Convex.AbstractVariable that conform to the appropriate interface (see the Convex.AbstractVariable docstring for details), one can easily provide custom variable types for specific constructions. These aren't always necessary though; for example, one can define the following function probabilityvector:\n\nusing Convex\nfunction probability_vector(d::Int)\n    x = Variable(d, Positive())\n    add_constraint!(x, sum(x) == 1)\n    return x\nend\n\nand then use, say, p = probabilityvector(3) in any Convex.jl problem. The constraints that the entries of p are non-negative and sum to 1 will be automatically added to any problem p is used in.\n\nCustom types are necessary when one wants to dispatch on custom variables, use them as callable types, or provide a different implementation. Continuing with the probability vector example, let's say we often use probability vectors variables in taking expectation values, and we want to use function notation for this. To do so, we define:\n\nusing Convex\nmutable struct ProbabilityVector <: Convex.AbstractVariable\n    head::Symbol\n    size::Tuple{Int,Int}\n    value::Union{Convex.Value,Nothing}\n    vexity::Convex.Vexity\n    function ProbabilityVector(d)\n        return new(:ProbabilityVector, (d, 1), nothing, Convex.AffineVexity())\n    end\nend\nConvex.get_constraints(p::ProbabilityVector) = [ sum(p) == 1 ]\nConvex.sign(::ProbabilityVector) = Convex.Positive()\nConvex.vartype(::ProbabilityVector) = Convex.ContVar\n(p::ProbabilityVector)(x) = dot(p, x)\n\nwarning: Warning\nCustom variable types must be mutable, otherwise variables with the same size and value would be treated as the same object.\n\nThen one can call p = ProbabilityVector(3) to construct a our custom variable which can be used in Convex, which already encodes the appropriate constraints (non-negative and sums to 1), and which can act on constants via p(x). For example,\n\nusing SCS\np = ProbabilityVector(3)\nprob = minimize(p([1.0, 2.0, 3.0]))\nsolve!(prob, SCS.Optimizer; silent = false);\nevaluate(p)\n\nSubtypes of AbstractVariable must have the fields head and size. Then they must also\n\neither have a field value, or implement Convex._value and Convex.set_value!\neither have a field vexity, or implement Convex.vexity and Convex.vexity! (though the latter is only necessary if you wish to support Convex.fix! and Convex.free!)\nhave a field constraints or implement Convex.get_constraints (optionally implement Convex.add_constraint! to be able to add constraints to your variable after its creation),\neither have a field sign or implement Convex.sign, and\neither have a field vartype, or implement Convex.vartype (optionally, implement Convex.vartype! to be able to change a variable's vartype after construction).","category":"section"},{"location":"manual/advanced/#Printing-and-the-tree-structure","page":"Advanced Features","title":"Printing and the tree structure","text":"A Convex problem is structured as a tree, with the root being the problem object, with branches to the objective and the set of constraints. The objective is an AbstractExpr which itself is a tree, with each atom being a node and having children which are other atoms, variables, or constants. Convex provides children methods from AbstractTrees.jl so that the tree-traversal functions of that package can be used with Convex.jl problems and structures. This is what allows powers the printing of problems, expressions, and constraints. The depth to which the tree corresponding to a problem, expression, or constraint is printed is controlled by the global variable Convex.MAXDEPTH, which defaults to 3. This can be changed by for example, setting\n\nConvex.MAXDEPTH[] = 5\n\nLikewise, Convex.MAXWIDTH, which defaults to 15, controls the \"width\" of the printed tree. For example, when printing a problem with 20 constraints, only the first MAXWIDTH of the constraints will be printed. Vertical dots, ⋮, will be printed indicating that some constraints were omitted in the printing.\n\nA related setting is Convex.MAXDIGITS, which controls printing the internal IDs of atoms: if the string representation of an ID is longer than double the value of MAXDIGITS, then it is shortened by printing only the first and last MAXDIGITS characters.\n\nThe AbstractTrees methods can also be used to analyze the structure of a Convex.jl problem. For example,\n\nusing Convex, AbstractTrees\nx = Variable()\np = maximize( log(x), x >= 1, x <= 3 )\nfor leaf in AbstractTrees.Leaves(p)\n    println(\"Here's a leaf: $(summary(leaf))\")\nend\n\nWe can also iterate over the problem in various orders. The following descriptions are taken from the AbstractTrees.jl docstrings, which have more information.","category":"section"},{"location":"manual/advanced/#PostOrderDFS","page":"Advanced Features","title":"PostOrderDFS","text":"Iterator to visit the nodes of a tree, guaranteeing that children will be visited before their parents.\n\nfor (i, node) in enumerate(AbstractTrees.PostOrderDFS(p))\n    println(\"Here's node $i via PostOrderDFS: $(summary(node))\")\nend","category":"section"},{"location":"manual/advanced/#PreOrderDFS","page":"Advanced Features","title":"PreOrderDFS","text":"Iterator to visit the nodes of a tree, guaranteeing that parents will be visited before their children.\n\nfor (i, node) in enumerate(AbstractTrees.PreOrderDFS(p))\n    println(\"Here's node $i via PreOrderDFS: $(summary(node))\")\nend","category":"section"},{"location":"manual/advanced/#StatelessBFS","page":"Advanced Features","title":"StatelessBFS","text":"Iterator to visit the nodes of a tree, guaranteeing that all nodes of a level will be visited before their children.\n\nfor (i, node) in enumerate(AbstractTrees.StatelessBFS(p))\n    println(\"Here's node $i via StatelessBFS: $(summary(node))\")\nend","category":"section"},{"location":"manual/solvers/#Solvers","page":"Solvers","title":"Solvers","text":"Convex.jl depends on third-party solvers to solve optimization problems. Therefore, you will need to install one before you can solve problems with Convex.jl.\n\nInstall a solver using the Julia package manager, replacing \"SCS\" by Julia package name as appropriate:\n\nimport Pkg\nPkg.add(\"SCS\")\n\nThe JuMP documentation has a list of support solvers and a list of problem classes they support.\n\nTo use a specific solver, you can use the following syntax:\n\nusing Convex, SCS\nx = Variable();\np = minimize(x, [x >= 1]);\nsolve!(p, SCS.Optimizer)\n\nA different solver can be used by replacing SCS as appropriate. For example, GLPK is a mixed-inter linear solver:\n\nusing GLPK\nsolve!(p, GLPK.Optimizer)\n\nMany of the solvers also allow options to be passed using MOI.OptimizerWithAttributes. For example, to set a time limit (in milliseconds) with GLPK, use:\n\nimport Convex: MOI\nsolver = MOI.OptimizerWithAttributes(GLPK.Optimizer, \"tm_lim\" => 60_000.0)\nsolve!(p, solver)\n\nAs another example, if we wish to turn off printing for the SCS solver (that is, run in quiet mode), we can do so as follows:\n\nsilent_scs = MOI.OptimizerWithAttributes(SCS.Optimizer, MOI.Silent() => true)\nsolve!(p, silent_scs)\n\nAnother option is to use the solver-independent silent keyword argument to solve!:\n\nsolve!(p, SCS.Optimizer; silent=true)\n\nSee each solver's documentation for more information on solver-dependent options.","category":"section"},{"location":"examples/optimization_with_complex_variables/quantum_conditional_entropy/#Continuity-of-the-quantum-conditional-entropy","page":"Continuity of the quantum conditional entropy","title":"Continuity of the quantum conditional entropy","text":"The quantum conditional entropy is given by\n\nS(AB)_rho = S(rho^AB) - S(rho^B)\n\nwhere S is the von Neumann entropy,\n\nS(rho) = - texttr(rho log rho)\n\nand rho is a positive semidefinite trace-1 matrix (density matrix).\n\nHere, rho^AB represents an operator on the tensor product of two finite-dimensional Hilbert spaces A and B (with dimensions d_A and d_B respectively), so we can regard rho_AB as a matrix on the vector space mathbbC^d_Ad_B. Moreover, rho^B denotes the partial trace of rho^AB over the system A, so rho^B is a matrix on mathbbC^d_B.\n\nOne question is how much can S(AB)_rho vary between two density matrices rho and sigma as a function of the trace-distance texttrdist(rho sigma) = rho-sigma_1 = frac12 texttrleft(sqrt(rho-sigma)^dagger (rho-sigma)right) (that is, half of the nuclear norm). Here the trace distance is meaningful as it is the quantum analog to the total variation distance, and has an interpretation in terms of the maximal possible probability to distinguish between rho and sigma by measurement.\n\nThe Alicki-Fannes-Winter (AFW) bound (Winter 2015, Lemma 2) states that if rho and sigma are density matrices, then texttrdist(rho sigma) leq varepsilon leq 1, then\n\n S(AB)_rho - S(AB)_sigma leq 2 varepsilon log d_A + (1 + varepsilon) h left(fracvarepsilon1+varepsilonright)\n\nwhere h(x) = -xlog x  - (1-x)log(1-x) is the binary entropy.\n\nWe can illustrate this bound by computing\n\n max_rho  S(AB)_rho - S(AB)_sigma\n\nfor a fixed state sigma, and comparing to the AFW bound.\n\nWe will choose d_A=d_B=2, and sigma as the maximally entangled state:\n\nsigma = frac12beginpmatrix1  0  0  1 0  0  0  0  0  0  0  0  1  0  0  1endpmatrix\n\nmath\n\nFirst, we can formulate the conditional entropy in terms of the relative entropy using the relationship\n\nS(AB)_rho = - D(rho^AB  I^A otimes rho^B)\n\nmath\n\nwhere D is the quantum relative entropy and I^A is the d_A-dimensional identity matrix. Thus:\n\nusing Convex\nusing LinearAlgebra: I\n\nfunction quantum_conditional_entropy(ρ_AB, d_A, d_B)\n    ρ_B = partialtrace(ρ_AB, 1, [d_A, d_B])\n    return -quantum_relative_entropy(ρ_AB, kron(I(d_A), ρ_B))\nend\n\nNow we setup the problem data:\n\nϵ = 0.1\nd_A = d_B = 2\nσ_AB = 0.5 * [\n    1 0 0 1\n    0 0 0 0\n    0 0 0 0\n    1 0 0 1\n]\n\nAnd we build and solve problem itself\n\nusing SCS\n\nρ_AB = HermitianSemidefinite(d_A * d_B)\nadd_constraint!(ρ_AB, tr(ρ_AB) == 1)\n\nproblem = maximize(\n    quantum_conditional_entropy(ρ_AB, d_A, d_B),\n    0.5 * nuclearnorm(ρ_AB - σ_AB) ≤ ϵ,\n)\n\nsolve!(problem, SCS.Optimizer; silent = false)\n\nWe can then check the observed difference in relative entropies:\n\ndifference = evaluate(\n    quantum_conditional_entropy(ρ_AB, d_A, d_B) -\n    quantum_conditional_entropy(σ_AB, d_A, d_B),\n)\n\nWe can compare to the bound:\n\nh(x) = -x * log(x) - (1 - x) * log(1 - x)\nbound = 2 * ϵ * log(d_A) + (1 + ϵ) * h(ϵ / (1 + ϵ))\n\nIn fact, in this case we know the maximizer is given by\n\nρ_max = σ_AB * (1 - ϵ) + ϵ * (I(d_A * d_B) - σ_AB) / (d_A * d_B - 1)\n\nWe can check that ρ_AB obtained the right value:\n\nnorm(evaluate(ρ_AB) - ρ_max)\n\nHere we see a result within the expected tolerances of SCS.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/portfolio_optimization/portfolio_optimization/#Portfolio-Optimization","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"In this problem, we will find the portfolio allocation that minimizes risk while achieving a given expected return R_texttarget.\n\nSuppose that we know the mean returns mu in mathbfR^n and the covariance Sigma in mathbfR^n times n of the n assets. We would like to find a portfolio allocation w in mathbfR^n, sum_i w_i = 1, minimizing the risk of the portfolio, which we measure as the variance w^T Sigma w of the portfolio. The requirement that the portfolio allocation achieve the target expected return can be expressed as w^T mu = R_texttarget. We suppose further that our portfolio allocation must comply with some lower and upper bounds on the allocation, w_textlower leq w leq w_textupper.\n\nThis problem can be written as\n\nbeginarrayll\n    textminimize    w^T Sigma w \n    textsubject to  w^T mu = R_texttarget \n                       sum_i w_i = 1 \n                       w_textlower leq w leq w_textupper\nendarray\n\nwhere w in mathbfR^n is our optimization variable.\n\nusing Convex, SCS\n\n# generate problem data\nμ = [11.5; 9.5; 6] / 100          #expected returns\nΣ = [\n    166 34 58              #covariance matrix\n    34 64 4\n    58 4 100\n] / 100^2\n\nn = length(μ)                   #number of assets\n\nR_target = 0.1\nw_lower = 0\nw_upper = 0.5;\nnothing #hide\n\nIf you want to try the optimization with more assets, uncomment and run the next cell. It creates a vector or average returns and a variance-covariance matrix that have scales similar to the numbers above.\n\nusing Random Random.seed!(123)\n\nn = 15                                      #number of assets, CHANGE IT?\n\nμ = (6 .+ (11.5-6)*rand(n))/100             #mean A = randn(n,n) Σ = (A * A' + diagm(0=>rand(n)))/500;       #covariance matrix\n\nw = Variable(n)\nret = dot(w, μ)\nrisk = quadform(w, Σ)\n\np = minimize(risk, ret >= R_target, sum(w) == 1, w_lower <= w, w <= w_upper)\n\nsolve!(p, SCS.Optimizer)\n\nOptimal portfolio weights:\n\nevaluate(w)\n\nsum(evaluate(w))\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/general_examples/DCP_analysis/#DCP-analysis","page":"DCP analysis","title":"DCP analysis","text":"using Convex\nx = Variable();\ny = Variable();\nexpr = quadoverlin(x - y, 1 - max(x, y))\n\nWe can see from the printing of the expression that this quadoverlin (qol) atom is convex with positive sign. We can query these programmatically using the vexity and sign functions:\n\nprintln(\"expression convexity = \", vexity(expr));\nprintln(\"expression sign = \", sign(expr));\nnothing #hide\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/general_examples/control/#Control","page":"Control","title":"Control","text":"A simple control problem on a system usually involves a variable x(t) that denotes the state of the system over time, and a variable u(t) that denotes the input into the system over time. Linear constraints are used to capture the evolution of the system over time:\n\nx(t) = Ax(t - 1) + Bu(t)  textfor  t = 1ldots T\n\nwhere the numerical matrices A and B are called the dynamics and input matrices, respectively.\n\nThe goal of the control problem is to find a sequence of inputs u(t) that will allow the state x(t) to achieve specified values at certain times. For example, we can specify initial and final states of the system:\n\nbeginaligned\n  x(0) = x_i \n  x(T) = x_f\nendaligned\n\nAdditional states between the initial and final states can also be specified. These are known as waypoint constraints. Often, the input and state of the system will have physical meaning, so we often want to find a sequence inputs that also minimizes a least squares objective like the following:\n\nsum_t = 0^T Fx(t)^2_2 + sum_t = 1^TGu(t)^2_2\n\nwhere F and G are numerical matrices.\n\nWe'll now apply the basic format of the control problem to an example of controlling the motion of an object in a fluid over T intervals, each of h seconds. The state of the system at time interval t will be given by the position and the velocity of the object, denoted p(t) and v(t), while the input will be forces applied to the object, denoted by f(t). By the basic laws of physics, the relationship between force, velocity, and position must satisfy:\n\n  beginaligned\n    p(t+1) = p(t) + h v(t) \n    v(t+1) = v(t) + h a(t)\n  endaligned\n\nHere, a(t) denotes the acceleration at time t, for which we use a(t) = f(t)  m + g - d v(t), where m, d, g are constants for the mass of the object, the drag coefficient of the fluid, and the acceleration from gravity, respectively.\n\nAdditionally, we have our initial/final position/velocity conditions:\n\n  beginaligned\n    p(1) = p_i\n    v(1) = v_i\n    p(T+1) = p_f\n    v(T+1) = 0\n  endaligned\n\nOne reasonable objective to minimize would be\n\n  textobjective = mu sum_t = 1^T+1 (v(t))^2 + sum_t = 1^T (f(t))^2\n\nWe would like to keep both the forces small to perhaps save fuel, and keep the velocities small for safety concerns. Here mu serves as a parameter to control which part of the objective we deem more important, keeping the velocity small or keeping the force small.\n\nThe following code builds and solves our control example:\n\nusing Convex, SCS, Plots\n\n# Some constraints on our motion\n# The object should start from the origin, and end at rest\ninitial_velocity = [-20; 100]\nfinal_position = [100; 100]\n\nT = 100 # The number of timesteps\nh = 0.1 # The time between time intervals\nmass = 1 # Mass of object\ndrag = 0.1 # Drag on object\ng = [0, -9.8] # Gravity on object\n\n# Declare the variables we need\nposition = Variable(2, T)\nvelocity = Variable(2, T)\nforce = Variable(2, T - 1)\n\n# Create a problem instance\nmu = 1\n\n# Add constraints on our variables\nconstraints = Constraint[\n    position[:, i+1] == position[:, i] + h * velocity[:, i] for i in 1:T-1\n]\n\nfor i in 1:T-1\n    acceleration = force[:, i] / mass + g - drag * velocity[:, i]\n    push!(constraints, velocity[:, i+1] == velocity[:, i] + h * acceleration)\nend\n\n# Add position constraints\npush!(constraints, position[:, 1] == 0)\npush!(constraints, position[:, T] == final_position)\n\n# Add velocity constraints\npush!(constraints, velocity[:, 1] == initial_velocity)\npush!(constraints, velocity[:, T] == 0)\n\n# Solve the problem\nproblem = minimize(sumsquares(force), constraints)\nsolve!(problem, SCS.Optimizer; silent = true)\n\nWe can plot the trajectory taken by the object.\n\npos = evaluate(position)\nplot([pos[1, 1]], [pos[2, 1]], st = :scatter, label = \"initial point\")\nplot!([pos[1, T]], [pos[2, T]], st = :scatter, label = \"final point\")\nplot!(pos[1, :], pos[2, :], label = \"trajectory\")\n\nWe can also see how the magnitude of the force changes over time.\n\nplot(vec(sum(evaluate(force) .^ 2, dims = 1)), label = \"force (magnitude)\")\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"release_notes/#Release-notes","page":"Release notes","title":"Release notes","text":"The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.","category":"section"},{"location":"release_notes/#[v0.16.4](https://github.com/jump-dev/Convex.jl/releases/tag/v0.16.4)-(November-19,-2024)","page":"Release notes","title":"v0.16.4 (November 19, 2024)","text":"","category":"section"},{"location":"release_notes/#Added","page":"Release notes","title":"Added","text":"Added support for indexing with BitVector and BitMatrix (#708) (#710)","category":"section"},{"location":"release_notes/#[v0.16.3](https://github.com/jump-dev/Convex.jl/releases/tag/v0.16.3)-(September-10,-2024)","page":"Release notes","title":"v0.16.3 (September 10, 2024)","text":"","category":"section"},{"location":"release_notes/#Fixed","page":"Release notes","title":"Fixed","text":"Fixed an incorrect type assertion in real_operate (#704)","category":"section"},{"location":"release_notes/#[v0.16.2](https://github.com/jump-dev/Convex.jl/releases/tag/v0.16.2)-(June-21,-2024)","page":"Release notes","title":"v0.16.2 (June 21, 2024)","text":"","category":"section"},{"location":"release_notes/#Fixed-2","page":"Release notes","title":"Fixed","text":"Fixed a bug adding constraints that had ConstVexity functions (#699)","category":"section"},{"location":"release_notes/#[v0.16.1](https://github.com/jump-dev/Convex.jl/releases/tag/v0.16.1)-(June-18,-2024)","page":"Release notes","title":"v0.16.1 (June 18, 2024)","text":"","category":"section"},{"location":"release_notes/#Added-2","page":"Release notes","title":"Added","text":"Added a dims argument to LogSumExpAtom and logsumexp that reduces the summation along dims (#692)\nAdded support for AbstractArray arguments to relative_entropy and log_perspective (#695)","category":"section"},{"location":"release_notes/#Other","page":"Release notes","title":"Other","text":"Minor updates to the README (#687), (#688), (#689), (#690)\nAdded API documentation (#610)\nFixed solve! docstring (#693)","category":"section"},{"location":"release_notes/#[v0.16.0](https://github.com/jump-dev/Convex.jl/releases/tag/v0.16.0)-(May-21,-2024)","page":"Release notes","title":"v0.16.0 (May 21, 2024)","text":"This release contains a large number of changes, including some breaking changes. However, despite the large number of changes, most user code should not need to change.\n\nIf you encounter an issue updating to this release, please open a GitHub issue, or post on the JuMP community forum: https://jump.dev/forum.","category":"section"},{"location":"release_notes/#Breaking","page":"Release notes","title":"Breaking","text":"This release involved a substantial rewrite of Convex.jl to integrate better with MathOptInterface. (#504), (#551), (#584), (#588), (#637)\nx + A will error if x is a scalar variable and A is an array. Instead, use x * ones(size(A)) + A.\nThe RelativeEntropyAtom now returns a scalar value instead of elementwise values. This does not affect the result of relative_entropy.\nThe function constant should be used instead of the type Constant (which now refers to exclusively real constants).\nThe constraint a <= b now produces a - b in Nonpositives() instead of b - a in Nonnegatives(). The primal solutions are equivalent, but the dual variable associated with such constraints is now reversed in sign. (Following the convention in MathOptInterface, the dual of a <= b is always negative, regardless of optimization sense.) (#593)\nThe structs LtConstraint, GtConstraint, EqConstraint SOCConstraint, ExpConstraint, SDPConstraint, GeoMeanEpiConeConstraint, GeoMeanHypoConeConstraint, and RelativeEntropyEpiCone, have been replaced by Constraint{S} where S<:MOI.AbstractSet (#590), (#597), (#598), (#599), (#601), (#602), (#604), (#623), (#632), (#648), (#663), (#665)\nThe set GeomMeanEpiCone has been renamed to GeometricMeanEpiConeSquare and GeomMeanHypoCone has been renamed to GeometricMeanHypoConeSquare (#638)\nSubtle breaking change: scalar row indexing like x[i, :] now produces a column vector instead of a row vector. This better aligns with Julia Base, but it can result in subtle differences, particularly for code like x[i, :] * y[i, :]': this used to be equivalent to the inner product, but it is now the outer product. In Base Julia, this is the outer product, so the previous code may have been silently broken (#624)\nThe syntaxes dot(*), dot(/) and dot(^) have been removed in favor of explicit broadcasting (x .* y, x ./ y, and x .^ y). These were (mild) type piracy. In addition, vecdot(x,y) has been removed. Call dot(vec(x), vec(y)) instead. (#524)\nThe function constraints, used to get constraints associated to an individual variable, has been renamed get_constraints (#527)\nDCP violations now throw a DCPViolationError exception, rather than a warning. Relatedly, Convex.emit_dcp_warnings has been removed (#523)\nThe strict inequalities > and < have been deprecated. They will be removed in the next breaking release. Note that these never enforced strict inequalities, but instead were equivalent to >= and <= respectively (#555)\nThe functions norm_inf, norm_1, and norm_fro have been deprecated. They will be removed in the next breaking release (#567)\nThe syntax x in :PSD to create a semidefinite constraint is deprecated and will be removed in the next breaking release (#578)\nFixed setting a Constant objective function. This is breaking because it now has an objective sense instead of ignoring the objective. (#581)\nquadform now errors when fixed variables are used instead of silently giving incorrect answers if the value of the fixed variable is modified between solves (#586)\nThe Context struct has been refactored and various fields have been changed. The internal details are now considered private. (#645)\nThe keyword argument silent_solver has been deprecated to silent. (#670)\nConcatenating lists of constraints using + (and +=) has been deprecated. (#659)","category":"section"},{"location":"release_notes/#Added-3","page":"Release notes","title":"Added","text":"SDP, SOC, and exponential cone constraints now have dual values populated (#504)\ngeomean supports more than 2 arguments (#504)\nAdded Convex.Optimizer (#511), (#530), (#534)\nAdded write_to_file (#531), (#591)\nAdded entropy_elementwise (#570)\nnorm on AbstractExpr objects now supports matrices (treating them like vectors), matching Base's behavior (#528)\nAdded root_det (#605)\nAdded VcatAtom which is a more efficient implementation of vcat (#607)\nAdded support for SparseArrays.SparseMatrixCSC in Constant. This fixed performance problems with some atoms (#631)\nsolve! now reports the time and memory allocation during compilation from the DCP expression graph to MathOptInterface (#633)\nAdded support for using Problem as an atom (#646)\nshow(::IO, ::Problem) now includes some problem statistics (#650)\nshow(::IO, ::Problem) now prints less of the expression tree by default (#661)\nA new example for quantum conditional entropy has been added. (#671)\nsolve! now returns the problem itself (#658)","category":"section"},{"location":"release_notes/#Fixed-3","page":"Release notes","title":"Fixed","text":"sumlargesteigs now enforces that it's argument is hermitian. (#504)\nType piracy of imag and real has been removed. This should not affect use of Convex. (#504)\nFix dot to now correctly complex-conjugates its first argument (#524)\nFixed ambiguities identified by Aqua.jl (#642), (#647)\nAdd tests and fix  a number of bugs in various atoms (#546), (#547), (#550), (#554), (#556), (#558), (#559), (#561), (#562), (#563), (#565), (#566), (#567), (#568), (#608), (#609), (#617), (#626), (#654), (#655)\nFixed performance issues in a number of issues related to scalar indexing (#618), (#619), (#620), (#621), (#625), (#634)\nFixed show for Problem (#649)\nsumsquares has a more efficient formulation (#678)\nFixed vexity of lieb_ando (#684)","category":"section"},{"location":"release_notes/#Other-2","page":"Release notes","title":"Other","text":"Improved the documentation (#506), (#517), (#529), (#571), (#573), (#574), (#576), (#579), (#587), (#594), (#628), (#652), (#656), (#666), (#674), (#686)\nRefactored the tests into a functional form (#532)\nUpdated Project.toml (#535)\nAdded test/Project.toml (#536)\nRefactored imports to explicitly overload methods (#537)\nTidied and renamed various atoms and files clarity. This should be non-breaking as no public API was changed. (#538), (#539), (#540), (#541), (#543), (#545), (#549), (#553), (#582), (#583)\nRemoved the unused file src/problem_depot/problems/benchmark.jl (#560)\nAdded various tests to improve code coverage (#522), (#572), (#575), (#577), (#580)\nUpdated versions in GitHub actions (#596), (#612), (#629)\nAdded license headers (#606)","category":"section"},{"location":"release_notes/#[v0.15.4](https://github.com/jump-dev/Convex.jl/releases/tag/v0.15.4)-(October-24,-2023)","page":"Release notes","title":"v0.15.4 (October 24, 2023)","text":"Convex's piracy of hcat and vcat was made less severe, allowing precompilation of Convex.jl on Julia 1.10.","category":"section"},{"location":"release_notes/#[v0.15.3](https://github.com/jump-dev/Convex.jl/releases/tag/v0.15.3)-(February-11,-2023)","page":"Release notes","title":"v0.15.3 (February 11, 2023)","text":"Add support for LDLFactorizations v0.10 #496.\nReplace randn(m, 1) with randn(m) to be more Julian #498.\nAdd support for indexing expressions with CartesianIndex #500.","category":"section"},{"location":"release_notes/#[v0.15.2](https://github.com/jump-dev/Convex.jl/releases/tag/v0.15.2)-(August-10,-2022)","page":"Release notes","title":"v0.15.2 (August 10, 2022)","text":"Add support for LDLFactorizations v0.9 #493.\nFix use of deprecated functions from AbstractTrees #494.","category":"section"},{"location":"release_notes/#[v0.15.1](https://github.com/jump-dev/Convex.jl/releases/tag/v0.15.1)-(March-28,-2022)","page":"Release notes","title":"v0.15.1 (March 28, 2022)","text":"Use OrderedDict internally for reproducible results, issue: #488, fix: #489.","category":"section"},{"location":"release_notes/#[v0.15.0](https://github.com/jump-dev/Convex.jl/releases/tag/v0.15.0)-(March-2,-2022)","page":"Release notes","title":"v0.15.0 (March 2, 2022)","text":"","category":"section"},{"location":"release_notes/#Breaking-changes","page":"Release notes","title":"Breaking changes","text":"Minimum required version of Julia is now v1.6\nUpdated to MathOptInterface v1.0\nAs a consequence, many previously deprecated solver calls may stop working. For example, instead of () -> SCS.Optimizer(verbose = 0), use MOI.OptimizerWithAttributes(SCS.Optimizer, \"verbose\" => 0).","category":"section"},{"location":"release_notes/#[v0.14.18](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.18)-(November-14,-2021)","page":"Release notes","title":"v0.14.18 (November 14, 2021)","text":"Fix typo in logisticloss for length-1 expressions which caused errors (reported in #458, fixed in #469).","category":"section"},{"location":"release_notes/#[v0.14.17](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.17)-(November-14,-2021)","page":"Release notes","title":"v0.14.17 (November 14, 2021)","text":"Updated to become compatible with MathOptInterface v0.10, which enables compatibility with the latest version of many solvers (#467, #468).","category":"section"},{"location":"release_notes/#[v0.14.16](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.16)-(September-25,-2021)","page":"Release notes","title":"v0.14.16 (September 25, 2021)","text":"Improve numerical stability when evaluating logsumexp (#457). Thanks @JinraeKim!","category":"section"},{"location":"release_notes/#[v0.14.15](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.15)-(September-15,-2021)","page":"Release notes","title":"v0.14.15 (September 15, 2021)","text":"Use sparse factorization for checking for positive semi-definiteness in quadform when possible (#457). Thanks @mtanneau!\nAdd assume_psd=false argument to skip checking for positive semi-definiteness in quadform (#456).","category":"section"},{"location":"release_notes/#[v0.14.14](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.14)-(September-8,-2021)","page":"Release notes","title":"v0.14.14 (September 8, 2021)","text":"Increase the tolerance used in checking if a matrix is positive-semi definite in quadform (#453). Thanks @numbermaniac!","category":"section"},{"location":"release_notes/#[v0.14.13](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.13)-(July-25,-2021)","page":"Release notes","title":"v0.14.13 (July 25, 2021)","text":"fix quadform for positive semi-definite matrices (fixes a regression introduced in v0.14.11 that required strictly positive semi-definite inputs) #450.","category":"section"},{"location":"release_notes/#[v0.14.12](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.12)-(July-19,-2021)","page":"Release notes","title":"v0.14.12 (July 19, 2021)","text":"fix size of result of evaluate on IndexAtoms #448. Thanks @hurak!","category":"section"},{"location":"release_notes/#[v0.14.11](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.11)-(July-5,-2021)","page":"Release notes","title":"v0.14.11 (July 5, 2021)","text":"fix quadform in the complex case #444. Thanks @lrnv!","category":"section"},{"location":"release_notes/#[v0.14.10](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.10)-(May-20,-2021)","page":"Release notes","title":"v0.14.10 (May 20, 2021)","text":"declare compatibility with BenchmarkTools v1.0 #441","category":"section"},{"location":"release_notes/#[v0.14.9](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.9)-(May-18,-2021)","page":"Release notes","title":"v0.14.9 (May 18, 2021)","text":"fix some tests in lp_dual_abs_atom #439. Thanks @moehle!","category":"section"},{"location":"release_notes/#[v0.14.8](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.8)-(May-4,-2021)","page":"Release notes","title":"v0.14.8 (May 4, 2021)","text":"a complete port of cvxquad thanks to @dstahlke, yielding new functions quantum_relative_entropy, quantum_entropy, trace_logm, trace_mpower, and lieb_ando, and cones GeomMeanHypoCone, GeomMeanEpiCone, and RelativeEntropyEpiCone (#418). Thanks a ton for the awesome contribution @dstahlke!","category":"section"},{"location":"release_notes/#[v0.14.7](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.7)-(April-22,-2021)","page":"Release notes","title":"v0.14.7 (April 22, 2021)","text":"declare compatibility with BenchmarkTools v0.7 #434","category":"section"},{"location":"release_notes/#[v0.14.6](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.6)-(March-28,-2021)","page":"Release notes","title":"v0.14.6 (March 28, 2021)","text":"Use MOI.instantiate to create the optimizer, which allows users to pass an MOI.OptimizerWithAttributes to configure solver settings #431. Thanks @odow!","category":"section"},{"location":"release_notes/#[v0.14.5](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.5)-(March-14,-2021)","page":"Release notes","title":"v0.14.5 (March 14, 2021)","text":"allow sumlargest(x,k), sumsmallest(x,k), and sumlargesteigs(x,k) for k=0 (simply returns Constant(0)). (#429).","category":"section"},{"location":"release_notes/#[v0.14.4](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.4)-(March-14,-2021)","page":"Release notes","title":"v0.14.4 (March 14, 2021)","text":"fixed a bug where the values of variables were being converted to Float64 even if the problem was solved in high precision. (#427).","category":"section"},{"location":"release_notes/#[v0.14.3](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.3)-(March-10,-2021)","page":"Release notes","title":"v0.14.3 (March 10, 2021)","text":"update compatibility bounds for BenchmarkTools 0.6","category":"section"},{"location":"release_notes/#[v0.14.2](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.2)-(February-15,-2021)","page":"Release notes","title":"v0.14.2 (February 15, 2021)","text":"added lasso, ridge, and elastic net regression examples (#420). Thanks to @PaulSoderlind!","category":"section"},{"location":"release_notes/#[v0.14.1](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.1)-(January-24,-2021)","page":"Release notes","title":"v0.14.1 (January 24, 2021)","text":"there was a bug causing conj to act in-place (reported in #416), which has been fixed (#417). This bug appears to have existed since the introduction of conj in Convex.jl v0.5.0.","category":"section"},{"location":"release_notes/#[v0.14.0](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.0)-(January-17,-2021)","page":"Release notes","title":"v0.14.0 (January 17, 2021)","text":"","category":"section"},{"location":"release_notes/#Breaking-changes-2","page":"Release notes","title":"Breaking changes","text":"Changes to the sign of atoms:\nThe sign of sumlargesteigs has been changed from Positive() to  NoSign(), to allow non-positive-semidefinite inputs (#409). This has the potential to break code that required that sign to be positive. If you run into this problem, please file an issue so we can figure out a workaround.\nThe sign of eigmin and eigmax has been changed from Positive() to  NoSign() (#413). This is a bugfix because in general eigmin and eigmax do not need to return a positive quantity (for non-positive-semidefinite inputs). Again, this has the potential to break code that required that sign to be positive. If you run into this problem, please file an issue so we can figure out a workaround.\nRemoval of deprecations:\nlambdamin and lambdamax has been deprecated to eigmin and eigmax since Convex v0.13.0. This deprecation has been removed, so your code must be updated to call eigmin or eigmax instead (#412).\nnorm(x, p) where x is a matrix expression has been deprecated to opnorm(x,p) since Convex v0.8.0. This deprecation has been removed, so your code must be updated to call opnorm(x, p) instead (#412). Currently, norm(x,p) for a matrix\nexpression x will error, but in Convex.jl v0.15.0 it will return norm(vec(x), p).\nConvex.clearmemory() has been deprecated and unnecessary since Convex v0.12.5. This deprecation has been removed, so if this function is in your code, just delete it (#412).\nvecnorm(x, p) has been deprecated to norm(vec(x), p) since Convex v0.8.0. This deprecation has been removed, so your code must be updated to call norm(vec(x),p) instead (#412).\nOther changes:\nConvex.DCP_WARNINGS was introduced in Convex v0.13.1 to allow turning off Convex.jl's DCP warnings. This has been removed in favor of the function Convex.emit_dcp_warnings() (Commit 481fa02).","category":"section"},{"location":"release_notes/#Other-changes","page":"Release notes","title":"Other changes","text":"updated nuclearnorm and sumlargesteigs to allow complex variables, and allow the argument of sumlargesteigs to be non-positive-semi-definite (#409). Thanks to @dstahlke!","category":"section"},{"location":"release_notes/#[v0.13.8](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.8)-(December-2,-2020)","page":"Release notes","title":"v0.13.8 (December 2, 2020)","text":"add unary + for Sign and ComplexSign to allow single-argument hcat and vcat to work (#405). Thanks to @dstahlke!","category":"section"},{"location":"release_notes/#[v0.13.7](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.7)-(September-11,-2020)","page":"Release notes","title":"v0.13.7 (September 11, 2020)","text":"fix #403 by adding the keyword argument silent_solver to solve!.","category":"section"},{"location":"release_notes/#[v0.13.6](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.6)-(September-8,-2020)","page":"Release notes","title":"v0.13.6 (September 8, 2020)","text":"fix #401 by allowing diagm(x).","category":"section"},{"location":"release_notes/#[v0.13.5](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.5)-(August-25,-2020)","page":"Release notes","title":"v0.13.5 (August 25, 2020)","text":"fix #398 by allowing fix!'d variables in quadform.","category":"section"},{"location":"release_notes/#[v0.13.4](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.4)-(July-27,-2020)","page":"Release notes","title":"v0.13.4 (July 27, 2020)","text":"You can now create your own variable types by subtyping AbstractVariable. See the docs for more information. You can also add constraints directly to a variable using add_constraint! (#358).\nFunctions vexity(x::Variable), sign(x::Variable), and evaluate(x::Variable) should now be the preferred way to access properties of a variable; likewise use set_value! to set the initial value of a variable (#358).\nTo create integer or binary constraints, use the VarType enum (for example, Variable(BinVar)). Access or set this via vartype and vartype! (#358).","category":"section"},{"location":"release_notes/#[v0.13.3](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.3)-(March-22,-2020)","page":"Release notes","title":"v0.13.3 (March 22, 2020)","text":"Make add_constraint! actually add the constraint to the problem.","category":"section"},{"location":"release_notes/#[v0.13.2](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.2)-(March-14,-2020)","page":"Release notes","title":"v0.13.2 (March 14, 2020)","text":"Add Convex.MAXDIGITS. Thanks to @riccardomurri!","category":"section"},{"location":"release_notes/#[v0.13.1](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.1)-(March-6,-2020)","page":"Release notes","title":"v0.13.1 (March 6, 2020)","text":"Allow disabling DCP warnings (#372)\nRestore export of Constraint (#371)","category":"section"},{"location":"release_notes/#[v0.13.0](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.0)-(February-28,-2020)","page":"Release notes","title":"v0.13.0 (February 28, 2020)","text":"","category":"section"},{"location":"release_notes/#Major-changes","page":"Release notes","title":"Major changes","text":"The intermediate layer has changed from MathProgBase.jl to MathOptInterface.jl (#330). To solve problems, one should pass a MathOptInterface optimizer constructor, such as SCS.Optimizer, or MOI.OptimizerWithAttributes(SCS.Optimizer, \"verbose\" => 0).\nlambdamin and lambdamax have been deprecated in favor of eigmin and eigmax (#357).\nMany \"internal\" functions and types are no longer exported, such as the atoms, types corresponding to constraints and vexities, etc. (#357).\nevaluate(x::Variable) and evaluate(c::Constant) now return scalars and vectors as appropriate, instead of (1,1)- and (d,1)-matrices (#359). This affects functions which used to return (1,1)-matrices; for example, now evaluate(quadform(...)) yields a scalar.","category":"section"},{"location":"examples/portfolio_optimization/portfolio_optimization2/#Portfolio-Optimization-Markowitz-Efficient-Frontier","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"In this problem, we will find the unconstrained portfolio allocation where we introduce the weighting parameter lambda (0 leq lambda leq 1) and minimize lambda * textrisk - (1-lambda)* textexpected return. By varying the values of lambda, we trace out the efficient frontier.\n\nSuppose that we know the mean returns mu in mathbfR^n of each asset and the covariance Sigma in mathbfR^n times n between the assets. Our objective is to find a portfolio allocation that minimizes the risk (which we measure as the variance w^T Sigma w) and maximizes the expected return (w^T mu) of the portfolio of the simultaneously. We require w in mathbfR^n and sum_i w_i = 1.\n\nThis problem can be written as\n\nbeginarrayll\n    textminimize    lambda*w^T Sigma w - (1-lambda)*w^T mu \n    textsubject to  sum_i w_i = 1\nendarray\n\nwhere w in mathbfR^n is the vector containing weights allocated to each asset.\n\nusing Convex, SCS    #We are using SCS solver. Install using Pkg.add(\"SCS\")\n\n# generate problem data\nμ = [11.5; 9.5; 6] / 100          #expected returns\nΣ = [\n    166 34 58              #covariance matrix\n    34 64 4\n    58 4 100\n] / 100^2\n\nn = length(μ)                   #number of assets\n\nIf you want to try the optimization with more assets, uncomment and run the next cell. It creates a vector or average returns and a variance-covariance matrix that have scales similar to the numbers above.\n\nusing Random Random.seed!(123)\n\nn = 15                                      #number of assets, CHANGE IT?\n\nμ = (6 .+ (11.5-6)*rand(n))/100             #mean A = randn(n,n) Σ = (A * A' + diagm(0=>rand(n)))/500;       #covariance matrix\n\nFirst we solve without any bounds on w\n\nN = 101\nλ_vals = range(0.01, stop = 0.99, length = N)\n\nw = Variable(n)\nret = dot(w, μ)\nrisk = quadform(w, Σ)\n\nMeanVarA = zeros(N, 2)\nfor i in 1:N\n    λ = λ_vals[i]\n    p = minimize(λ * risk - (1 - λ) * ret, sum(w) == 1)\n    solve!(p, SCS.Optimizer; silent = true)\n    MeanVarA[i, :] = [evaluate(ret), evaluate(risk)]\nend\n\nNow we solve with the bounds 0le w_i le 1\n\nw_lower = 0                     #bounds on w\nw_upper = 1\n\nMeanVarB = zeros(N, 2)   #repeat, but with 0<w[i]<1\nfor i in 1:N\n    λ = λ_vals[i]\n    p = minimize(\n        λ * risk - (1 - λ) * ret,\n        sum(w) == 1,\n        w_lower <= w,     #w[i] is bounded\n        w <= w_upper,\n    )\n    solve!(p, SCS.Optimizer; silent = true)\n    MeanVarB[i, :] = [evaluate(ret), evaluate(risk)]\nend\n\nusing Plots\nplot(\n    sqrt.([MeanVarA[:, 2] MeanVarB[:, 2]]),\n    [MeanVarA[:, 1] MeanVarB[:, 1]],\n    xlim = (0, 0.25),\n    ylim = (0, 0.15),\n    title = \"Markowitz Efficient Frontier\",\n    xlabel = \"Standard deviation\",\n    ylabel = \"Expected return\",\n    label = [\"no bounds on w\" \"with 0<w<1\"],\n)\nscatter!(sqrt.(diag(Σ)), μ, color = :red, label = \"assets\")\n\nWe now instead impose a restriction on  sum_i w_i - 1, allowing for varying degrees of leverage.\n\nLmax = 0.5\n\nMeanVarC = zeros(N, 2)   #repeat, but with restriction on Sum(|w[i]|)\nfor i in 1:N\n    λ = λ_vals[i]\n    p = minimize(\n        λ * risk - (1 - λ) * ret,\n        sum(w) == 1,\n        (norm(w, 1) - 1) <= Lmax,\n    )\n    solve!(p, SCS.Optimizer; silent = true)\n    MeanVarC[i, :] = [evaluate(ret), evaluate(risk)]\nend\n\nplot(\n    sqrt.([MeanVarA[:, 2] MeanVarB[:, 2] MeanVarC[:, 2]]),\n    [MeanVarA[:, 1] MeanVarB[:, 1] MeanVarC[:, 1]],\n    xlim = (0, 0.25),\n    ylim = (0, 0.15),\n    title = \"Markowitz Efficient Frontier\",\n    xlabel = \"Standard deviation\",\n    ylabel = \"Expected return\",\n    label = [\"no bounds on w\" \"with 0<w<1\" \"restriction on sum(|w|)\"],\n)\nscatter!(sqrt.(diag(Σ)), μ, color = :red, label = \"assets\")\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/optimization_with_complex_variables/power_flow_optimization/#Power-flow-optimization","page":"Power flow optimization","title":"Power flow optimization","text":"The data for example is taken from MATPOWER website. MATPOWER is Matlab package for solving power flow and optimal power flow problems.\n\nusing Convex, SCS\nusing Test\nusing MAT   #Pkg.add(\"MAT\")\naux(str) = joinpath(@__DIR__, \"aux_files\", str) # path to auxiliary files\n\nTOL = 1e-2;\ninput = matopen(aux(\"Data.mat\"))\nvarnames = names(input)\nData = read(input, \"inj\", \"Y\");\n\nn = size(Data[2], 1);\nY = Data[2];\ninj = Data[1];\nW = ComplexVariable(n, n);\nobjective = real(sum(diag(W)));\nc1 = Constraint[];\nfor i in 2:n\n    push!(c1, dot(Y[i, :], W[i, :]) == inj[i])\nend\nc2 = isposdef(W)\nc3 = real(W[1, 1]) == 1.06^2;\npush!(c1, c2)\npush!(c1, c3)\np = maximize(objective, c1);\nsolve!(p, SCS.Optimizer; silent = true)\n\np.optval\n\nevaluate(objective)\n\noutput = matopen(joinpath(@__DIR__, \"Res.mat\"))\nnames(output)\noutputData = read(output, \"Wres\");\nWres = outputData\nreal_diff = real(evaluate(W)) - real(Wres);\nimag_diff = imag(evaluate(W)) - imag(Wres);\n@test real_diff ≈ zeros(n, n) atol = TOL\n@test imag_diff ≈ zeros(n, n) atol = TOL\n\nreal_diff = real(evaluate(W)) - (real(evaluate(W)))';\nimag_sum = imag(evaluate(W)) + (imag(evaluate(W)))';\n@test real_diff ≈ zeros(n, n) atol = TOL\n@test imag_diff ≈ zeros(n, n) atol = TOL\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"reference/api/#API","page":"API","title":"API","text":"info: Info\nSee Supported operations for a list of the operations supported by Convex.","category":"section"},{"location":"reference/api/#Convex.AbstractVariable","page":"API","title":"Convex.AbstractVariable","text":"abstract type AbstractVariable <: AbstractExpr end\n\nAn AbstractVariable should have head field, and a size field to conform to the AbstractExpr interface, and implement methods (or use the field-access fallbacks) for\n\n_value, set_value!: get or set the numeric value of the variable.   _value should return nothing when no numeric value is set. Note:   evaluate is the user-facing method to access the value of x.\nvexity, vexity!: get or set the vexity of the variable. The   vexity should be AffineVexity() unless the variable has been   fix!'d, in which case it is ConstVexity().\nsign, vartype, and get_constraints: get the Sign, VarType,   numeric type, and a (possibly empty) vector of constraints which are   to be applied to any problem in which the variable is used.\n\nOptionally, also implement sign!, vartype!, and add_constraint! to allow users to modify those values or add a constraint.\n\n\n\n\n\n","category":"type"},{"location":"reference/api/#Convex._value","page":"API","title":"Convex._value","text":"_value(x::AbstractVariable)\n\nRaw access to the current value of x; used internally by Convex.jl.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.set_value!","page":"API","title":"Convex.set_value!","text":"set_value!(x::AbstractVariable, v)\n\nSets the current value of x to v.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.get_constraints","page":"API","title":"Convex.get_constraints","text":"get_constraints(x::AbstractVariable)\n\nReturns the current constraints carried by x.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.add_constraint!","page":"API","title":"Convex.add_constraint!","text":"add_constraint!(x::AbstractVariable, C::Constraint)\n\nAdds an constraint to those carried by x.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.vexity","page":"API","title":"Convex.vexity","text":"vexity(x::AbstractVariable)\n\nReturns the current vexity of x.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.vexity!","page":"API","title":"Convex.vexity!","text":"vexity!(x::AbstractVariable, v::Vexity)\n\nSets the current vexity of x to v. Should only be called by fix! and free!.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Base.sign-Tuple{Convex.AbstractVariable}","page":"API","title":"Base.sign","text":"Base.sign(x::AbstractVariable)\n\nReturns the current sign of x.\n\n\n\n\n\n","category":"method"},{"location":"reference/api/#Convex.sign!","page":"API","title":"Convex.sign!","text":"sign!(x::AbstractVariable, s::Sign)\n\nSets the current sign of x to s.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.VarType","page":"API","title":"Convex.VarType","text":"VarType\n\nDescribe the type of a AbstractVariable: either continuous (ContVar), integer-valued (IntVar), or binary (BinVar).\n\n\n\n\n\n","category":"type"},{"location":"reference/api/#Convex.vartype","page":"API","title":"Convex.vartype","text":"vartype(x::AbstractVariable)\n\nReturns the current VarType of x.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.vartype!","page":"API","title":"Convex.vartype!","text":"vartype!(x::AbstractVariable, vt::VarType)\n\nSets the current VarType of x to vt.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.fix!","page":"API","title":"Convex.fix!","text":"fix!(x::AbstractVariable, v = value(x))\n\nFixes x to v. It is subsequently treated as a constant in future optimization problems. See also free!.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.free!","page":"API","title":"Convex.free!","text":"free!(x::AbstractVariable)\n\nFrees a previously fix!'d variable x, to treat it once again as a variable to optimize over.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.evaluate","page":"API","title":"Convex.evaluate","text":"evaluate(x::AbstractVariable)\n\nReturns the current value of x if assigned; errors otherwise.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.solve!","page":"API","title":"Convex.solve!","text":"solve!(\n    problem::Problem,\n    optimizer_factory;\n    silent::Bool = false,\n    warmstart::Bool = false,\n)\n\nSolves the problem, populating problem.optval with the optimal value, as well as the values of the variables (accessed by evaluate) and constraint duals (accessed by cons.dual), where applicable. Returns the input problem.\n\nOptional keyword arguments:\n\nsilent: whether or not Convex and the solver should be silent (and not emit output or logs) during the solution process. When silent=false, Convex will print the formulation time, and warn if the problem was not solved optimally.\nwarmstart (default: false): whether the solver should start the optimization from a previous optimal value (according to the current primal value of the variables in the problem, which can be set by set_value!.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.MAXDEPTH","page":"API","title":"Convex.MAXDEPTH","text":"MAXDEPTH\n\nControls depth of tree printing globally for Convex.jl; defaults to 3. Set via\n\nConvex.MAXDEPTH[] = 5\n\n\n\n\n\n","category":"constant"},{"location":"reference/api/#Convex.MAXWIDTH","page":"API","title":"Convex.MAXWIDTH","text":"MAXWIDTH\n\nControls width of tree printing globally for Convex.jl; defaults to 3. Set via\n\nConvex.MAXWIDTH[] = 10\n\n\n\n\n\n","category":"constant"},{"location":"reference/api/#Convex.MAXDIGITS","page":"API","title":"Convex.MAXDIGITS","text":"MAXDIGITS\n\nWhen priting IDs of variables, only show the initial and final digits if the full ID has more than double the number of digits specified here.  So, with the default setting MAXDIGITS=3, any ID longer than 7 digits would be shortened; for example, ID 14656210999710729289 would be printed as 146…289.\n\nThis setting controls tree printing globally for Convex.jl; defaults to 3.\n\nSet via:\n\nConvex.MAXDIGITS[] = 3\n\n\n\n\n\n","category":"constant"},{"location":"reference/api/#Convex.ProblemDepot.run_tests","page":"API","title":"Convex.ProblemDepot.run_tests","text":"run_tests(\n    handle_problem!::Function;\n    problems::Union{Nothing, Vector{String}, Vector{Regex}} = nothing;\n    exclude::Vector{Regex} = Regex[],\n    T=Float64, atol=1e-3, rtol=0.0,\n)\n\nRun a set of tests. handle_problem! should be a function that takes one argument, a Convex.jl Problem and processes it (e.g. solve! the problem with a specific solver).\n\nUse exclude to exclude a subset of sets; automatically excludes r\"benchmark\". Optionally, pass a second argument problems to only allow certain problems (specified by exact names or regex). The test tolerances specified by atol and rtol. Set T to choose a numeric type for the problem. Currently this is only used for choosing the type parameter of the underlying MathOptInterface model, but not for the actual problem data.\n\nExamples\n\nrun_tests(exclude=[r\"mip\"]) do p\n    solve!(p, SCS.Optimizer; silent=true)\nend\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.ProblemDepot.benchmark_suite","page":"API","title":"Convex.ProblemDepot.benchmark_suite","text":"benchmark_suite(\n    handle_problem!::Function,\n    problems::Union{Nothing, Vector{String}, Vector{Regex}} = nothing;\n    exclude::Vector{Regex} = Regex[],\n    test = Val(false),\n    T=Float64, atol=1e-3, rtol=0.0,\n)\n\nCreate a benchmarksuite of benchmarks. `handleproblem!should be a function that takes one argument, a Convex.jlProblemand processes it (e.g.solve!the problem with a specific solver). Pass a second argumentproblems` to specify run benchmarks only with certain problems (specified by exact names or regex).\n\nUse exclude to exclude a subset of benchmarks. Optionally, pass a second argument problems to only allow certain problems (specified by exact names or regex). Set test=true to also check the answers, with tolerances specified by atol and rtol. Set T to choose a numeric type for the problem. Currently this is only used for choosing the type parameter of the underlying MathOptInterface model, but not for the actual problem data.\n\nExamples\n\nbenchmark_suite(exclude=[r\"mip\"]) do p\n    solve!(p, SCS.Optimizer; silent=true)\nend\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.ProblemDepot.foreach_problem","page":"API","title":"Convex.ProblemDepot.foreach_problem","text":"foreach_problem(apply::Function, [class::String],\n    problems::Union{Nothing, Vector{String}, Vector{Regex}} = nothing;\n    exclude::Vector{Regex} = Regex[])\n\nProvides a convience method for iterating over problems in PROBLEMS. For each problem in PROBLEMS, apply the function apply, which takes two arguments: the name of the function associated to the problem, and the function associated to the problem itself.\n\nOptionally, pass a second argument class to only iterate over a class of problems (class should satsify class ∈ keys(PROBLEMS)), and pass third argument problems to only allow certain problems (specified by exact names or regex). Use the exclude keyword argument to exclude problems by regex.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.ProblemDepot.PROBLEMS","page":"API","title":"Convex.ProblemDepot.PROBLEMS","text":"const PROBLEMS = Dict{String, Dict{String, Function}}()\n\nA \"depot\" of Convex.jl problems, subdivided into categories. Each problem is stored as a function with the signature\n\nf(handle_problem!, ::Val{test}, atol, rtol, ::Type{T}) where {T, test}\n\nwhere handle_problem! specifies what to do with the Problem instance (e.g., solve! it with a chosen solver), an option test to choose whether or not to test the values (assuming it has been solved), tolerances for the tests, and a numeric type in which the problem should be specified (currently, this is not respected and all problems are specified in Float64 precision).\n\nSee also run_tests and benchmark_suite for helpers to use these problems in testing or benchmarking.\n\nExamples\n\njulia> PROBLEMS[\"affine\"][\"affine_diag_atom\"]\naffine_diag_atom (generic function with 1 method)\n\n\n\n\n\n","category":"constant"},{"location":"reference/api/#Convex.conic_form!","page":"API","title":"Convex.conic_form!","text":"conic_form!(context::Context, a::AbstractExpr)\n\nReturn the conic form for a. If it as already been created, it is directly accessed in context[a], otherwise, it is created by calling Convex.new_conic_form! and then cached in context so that the next call with the same expression does not create a duplicate one.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.new_conic_form!","page":"API","title":"Convex.new_conic_form!","text":"new_conic_form!(context::Context, a::AbstractExpr)\n\nCreate a new conic form for a and return it, assuming that no conic form for a has already been created, that is !haskey(context, a) as this is already checked in conic_form! which calls this function.\n\n\n\n\n\n","category":"function"},{"location":"reference/api/#Convex.write_to_file","page":"API","title":"Convex.write_to_file","text":"write_to_file(problem::Problem{Float64}, filename::String)\n\nWrite the current problem to the file at filename.\n\nThe file format is inferred from the filename extension. Supported file types depend on the model type.\n\nCurrently, Float64 is the only supported coefficient type. This may be relaxed in future if file formats support other types.\n\n\n\n\n\n","category":"function"},{"location":"examples/time_series/time_series/#Time-Series-Analysis","page":"Time Series Analysis","title":"Time Series Analysis","text":"A time series is a sequence of data points, each associated with a time. In our example, we will work with a time series of daily temperatures in the city of Melbourne, Australia over a period of a few years. Let x be the vector of the time series, and x_i denote the temperature in Melbourne on day i. Here is a picture of the time series:\n\nusing Plots, Convex, ECOS, DelimitedFiles\nconst MOI = Convex.MOI\n\naux(str) = joinpath(@__DIR__, \"aux_files\", str) # path to auxiliary files\n\ntemps = readdlm(aux(\"melbourne_temps.txt\"), ',')\nn = size(temps, 1)\nplot(\n    1:n,\n    temps[1:n],\n    ylabel = \"Temperature (°C)\",\n    label = \"data\",\n    xlabel = \"Time (days)\",\n    xticks = 0:365:n,\n)\n\nWe can quickly compute the mean of the time series to be 112. If we were to always guess the mean as the temperature of Melbourne on a given day, the RMS error of our guesswork would be 41. We'll try to lower this RMS error by coming up with better ways to model the temperature than guessing the mean.\n\nA simple way to model this time series would be to find a smooth curve that approximates the yearly ups and downs. We can represent this model as a vector s where s_i denotes the temperature on the i-th day. To force this trend to repeat yearly, we simply want\n\ns_i = s_i + 365\n\nfor each applicable i.\n\nWe also want our model to have two more properties:\n\nThe first is that the temperature on each day in our model should be relatively close to the actual temperature of that day.\nThe second is that our model needs to be smooth, so the change in temperature from day to day should be relatively small. The following objective would capture both properties:\n\nsum_i = 1^n (s_i - x_i)^2 + lambda sum_i = 2^n(s_i - s_i - 1)^2\n\nwhere lambda is the smoothing parameter. The larger lambda is, the smoother our model will be.\n\nThe following code uses Convex to find and plot the model:\n\nyearly = Variable(n)\neq_constraints = [yearly[i] == yearly[i-365] for i in 365+1:n]\n\nsmoothing = 100\nsmooth_objective = sumsquares(yearly[1:n-1] - yearly[2:n])\nproblem = minimize(\n    sumsquares(temps - yearly) + smoothing * smooth_objective,\n    eq_constraints,\n);\nsolve!(\n    problem,\n    MOI.OptimizerWithAttributes(ECOS.Optimizer, \"maxit\" => 200, \"verbose\" => 0),\n)\nresiduals = temps - evaluate(yearly)\n\n# Plot smooth fit\nplot(1:n, temps[1:n], label = \"data\")\nplot!(\n    1:n,\n    evaluate(yearly)[1:n],\n    linewidth = 2,\n    label = \"smooth fit\",\n    ylabel = \"Temperature (°C)\",\n    xticks = 0:365:n,\n    xlabel = \"Time (days)\",\n)\n\nWe can also plot the residual temperatures, r, defined as r = x - s.\n\n# Plot residuals for a few days\nplot(1:100, residuals[1:100], ylabel = \"Residuals\", xlabel = \"Time (days)\")\n\nroot_mean_square_error = sqrt(sum(x -> x^2, residuals) / length(residuals))\n\nOur smooth model has a RMS error of 27, a significant improvement from just guessing the mean, but we can do better.\n\nWe now make the hypothesis that the residual temperature on a given day is some linear combination of the previous 5 days. Such a model is called autoregressive. We are essentially trying to fit the residuals as a function of other parts of the data itself. We want to find a vector of coefficients a such that\n\ntextr(i) approx sum_j = 1^5 a_j textr(i - j)\n\nThis can be done by simply minimizing the following sum of squares objective\n\nsum_i = 6^n left(textr(i) - sum_j = 1^5 a_j textr(i - j)right)^2\n\nThe following Convex code solves this problem and plots our autoregressive model against the actual residual temperatures:\n\n# Generate the residuals matrix\nar_len = 5\n\nresiduals_mat = Matrix{Float64}(undef, length(residuals) - ar_len, ar_len)\nfor i in 1:ar_len\n    residuals_mat[:, i] = residuals[ar_len-i+1:n-i]\nend\n\n# Solve autoregressive problem\nar_coef = Variable(ar_len)\nproblem =\n    minimize(sumsquares(residuals_mat * ar_coef - residuals[ar_len+1:end]))\nsolve!(\n    problem,\n    MOI.OptimizerWithAttributes(ECOS.Optimizer, \"maxit\" => 200, \"verbose\" => 0),\n)\n\nPlot autoregressive fit of daily fluctuations for a few days:\n\nar_range = 1:145\nday_range = ar_range .+ ar_len\nplot(\n    day_range,\n    residuals[day_range],\n    label = \"fluctuations from smooth fit\",\n    ylabel = \"Temperature difference (°C)\",\n)\nplot!(\n    day_range,\n    residuals_mat[ar_range, :] * evaluate(ar_coef),\n    label = \"autoregressive estimate\",\n    xlabel = \"Time (days)\",\n)\n\nNow, we can add our autoregressive model for the residual temperatures to our smooth model to get an better fitting model for the daily temperatures in the city of Melbourne:\n\ntotal_estimate = evaluate(yearly)\ntotal_estimate[ar_len+1:end] += residuals_mat * evaluate(ar_coef)\n\nWe can plot the final fit of data across the whole time range:\n\nplot(1:n, temps, label = \"data\", ylabel = \"Temperature (°C)\")\nplot!(\n    1:n,\n    total_estimate,\n    label = \"estimate\",\n    xticks = 0:365:n,\n    xlabel = \"Time (days)\",\n)\n\nThe RMS error of this final model is sim 23:\n\nroot_mean_square_error =\n    sqrt(sum(x -> x^2, total_estimate - temps) / length(temps))\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#Convex.jl-Convex-Optimization-in-Julia","page":"Home","title":"Convex.jl - Convex Optimization in Julia","text":"Convex.jl is a Julia package for Disciplined Convex Programming (DCP).\n\nConvex.jl makes it easy to describe optimization problems in a natural, mathematical syntax, and to solve those problems using a variety of different (commercial and open-source) solvers.\n\nConvex.jl can be used to solve:\n\nlinear programs\nmixed-integer linear programs and mixed-integer second-order cone programs\nDCP-compliant convex programs including\nsecond-order cone programs (SOCP)\nexponential cone programs\nsemidefinite programs (SDP)","category":"section"},{"location":"#Resources-for-getting-started","page":"Home","title":"Resources for getting started","text":"There are a few ways to get started with Convex:\n\nRead the Installation guide\nRead the introductory tutorial Quick Tutorial\nRead the list of Supported operations\nBrowse some of our examples\n\ntip: Tip\nNeed help? Join the community forum to search for answers to commonly asked questions.Before asking a question, make sure to read the post make it easier to help you, which contains a number of tips on how to ask a good question.","category":"section"},{"location":"#How-the-documentation-is-structured","page":"Home","title":"How the documentation is structured","text":"Having a high-level overview of how this documentation is structured will help you know where to look for certain things.\n\nExamples contain worked examples of solving problems with Convex. Start here if you are new to Convex, or you have a particular problem class you want to model.\nThe Manual contains short code-snippets that explain how to achieve specific tasks in Convex. Look here if you want to know how to achieve a particular task.\nThe Developer docs section contains information for people contributing to Convex development. Don't worry about this section if you are using Convex to formulate and solve problems as a user.","category":"section"},{"location":"examples/general_examples/chebyshev_center/#Chebyshev-center","page":"Chebyshev center","title":"Chebyshev center","text":"Boyd & Vandenberghe, \"Convex Optimization\" Joëlle Skaf - 08/16/05\n\nAdapted for Convex.jl by Karanveer Mohan and David Zeng - 26/05/14\n\nThe goal is to find the largest Euclidean ball (that is, its center and radius) that lies in a polyhedron described by affine inequalities in this fashion: P = x  a_i*x leq b_i i=1ldotsm  where x in mathbbR^2.\n\nusing Convex\nusing LinearAlgebra\nimport SCS\n\nGenerate the input data\n\na1 = [2; 1];\na2 = [2; -1];\na3 = [-1; 2];\na4 = [-1; -2];\nb = ones(4, 1);\nnothing #hide\n\nCreate and solve the model\n\nr = Variable(1)\nx_c = Variable(2)\nconstraints = [\n    a1' * x_c + r * norm(a1, 2) <= b[1],\n    a2' * x_c + r * norm(a2, 2) <= b[2],\n    a3' * x_c + r * norm(a3, 2) <= b[3],\n    a4' * x_c + r * norm(a4, 2) <= b[4],\n]\np = maximize(r, constraints)\nsolve!(p, SCS.Optimizer; silent = true)\n\nGenerate the figure\n\nx = range(-1.5, stop = 1.5, length = 100);\ntheta = 0:pi/100:2*pi;\nusing Plots\nplot(x, x -> -x * a1[1] / a1[2] + b[1] / a1[2])\nplot!(x, x -> -x * a2[1] / a2[2] + b[2] / a2[2])\nplot!(x, x -> -x * a3[1] / a3[2] + b[3] / a3[2])\nplot!(x, x -> -x * a4[1] / a4[2] + b[4] / a4[2])\nplot!(\n    evaluate(x_c)[1] .+ evaluate(r) * cos.(theta),\n    evaluate(x_c)[2] .+ evaluate(r) * sin.(theta),\n    linewidth = 2,\n)\nplot!(\n    title = \"Largest Euclidean ball lying in a 2D polyhedron\",\n    legend = nothing,\n)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/optimization_with_complex_variables/povm_simulation/#POVM-simulation","page":"POVM simulation","title":"POVM simulation","text":"This notebook shows how we can check how much depolarizing noise a qubit positive operator-valued measure (POVM) can take before it becomes simulable by projective measurements. The general method is described in arXiv:1609.06139. The question of simulability by projective measurements boils down to an SDP problem. Eq. (8) from the paper defines the noisy POVM that we obtain subjecting a POVM mathbfM to a depolarizing channel Phi_t:\n\nleftPhi_tleft(mathbfMright)right_i = t M_i + (1-t)fracmathrmtr(M_i)d mathbb1\n\nIf this visibility tin01 is one, the POVM mathbfM is simulable.\n\nWe will use Convex.jl to solve the SDP problem.\n\nusing Convex, SCS, LinearAlgebra\n\nFor the qubit case, a four outcome qubit POVM mathbfM inmathcalP(24) is simulable if and only if\n\nM_1=N_12^++N_13^++N_14^+\n\nM_2=N_12^-+N_23^++N_24^+\n\nM_3=N_13^-+N_23^-+N_34^+\n\nM_4=N_14^-+N_24^-+N_34^-\n\nwhere Hermitian operators N_ij^pm satisfy N_ij^pmgeq0 and N_ij^++N_ij^-=p_ijmathbb1, where ij , ij=1234 and p_ijgeq0 as well as sum_ijp_ij=1, that is, the p_ij values form a probability vector. This forms an SDP feasibility problem, which we can rephrase as an optimization problem by adding depolarizing noise to the left-hand side of the above equations and maximizing the visibility t:\n\nmax_tin01 t\n\nsuch that\n\ntM_1+(1-t)mathrmtr(M_1)fracmathbb12=N_12^++N_13^++N_14^+\n\ntM_2+(1-t)mathrmtr(M_2)fracmathbb12=N_12^-+N_23^++N_24^+\n\ntM_3+(1-t)mathrmtr(M_3)fracmathbb12=N_13^-+N_23^-+N_34^+\n\ntM_4+(1-t)mathrmtr(M_4)fracmathbb12=N_14^-+N_24^-+N_34^-\n\n.\n\nWe organize these constraints in a function that takes a four-output qubit POVM as its argument:\n\nfunction get_visibility(K)\n    noise = real([tr(K[i]) * I(2) / 2 for i in 1:size(K, 1)])\n    P = [[ComplexVariable(2, 2) for i in 1:2] for j in 1:6]\n    q = Variable(6, Positive())\n    t = Variable(1, Positive())\n    constraints = Constraint[isposdef(P[i][j]) for i in 1:6 for j in 1:2]\n    push!(constraints, sum(q) == 1)\n    push!(constraints, t <= 1)\n    append!(constraints, [P[i][1] + P[i][2] == q[i] * I(2) for i in 1:6])\n    push!(\n        constraints,\n        t * K[1] + (1 - t) * noise[1] == P[1][1] + P[2][1] + P[3][1],\n    )\n    push!(\n        constraints,\n        t * K[2] + (1 - t) * noise[2] == P[1][2] + P[4][1] + P[5][1],\n    )\n    push!(\n        constraints,\n        t * K[3] + (1 - t) * noise[3] == P[2][2] + P[4][2] + P[6][1],\n    )\n    push!(\n        constraints,\n        t * K[4] + (1 - t) * noise[4] == P[3][2] + P[5][2] + P[6][2],\n    )\n    p = maximize(t, constraints)\n    return solve!(p, SCS.Optimizer; silent = true)\nend\n\nWe check this function using the tetrahedron measurement (see Appendix B in arXiv:quant-ph/0702021). This measurement is non-simulable, so we expect a value below one.\n\nfunction dp(v)\n    return I(2) + v[1] * [0 1; 1 0] + v[2] * [0 -im; im 0] + v[3] * [1 0; 0 -1]\nend\nb = [\n    1 1 1\n    -1 -1 1\n    -1 1 -1\n    1 -1 -1\n] / sqrt(3)\nM = [dp(b[i, :]) for i in 1:size(b, 1)] / 4;\np = get_visibility(M)\n\np.optval\n\nThis value matches the one we obtained using PICOS.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/general_examples/trade_off_curves/#Regularized-least-squares","page":"Regularized least-squares","title":"Regularized least-squares","text":"Here we solve some constrained least-squares problems with 1-norm regularization, and plot how the solution changes with increasing regularization.\n\nusing Random\nRandom.seed!(1)\nm = 25;\nn = 10;\nA = randn(m, n);\nb = randn(m, 1);\nnothing #hide\n\nusing Convex, SCS, LinearAlgebra\n\ngammas = exp10.(range(-4, stop = 2, length = 100));\n\nx_values = zeros(n, length(gammas));\nx = Variable(n);\nfor i in 1:length(gammas)\n    cost = sumsquares(A * x - b) + gammas[i] * norm(x, 1)\n    problem = minimize(cost, [norm(x, Inf) <= 1])\n    solve!(problem, SCS.Optimizer; silent = true)\n    x_values[:, i] = evaluate(x)\nend\n\nPlot the regularization path.\n\nusing Plots\nplot(\n    title = \"Entries of x vs lambda\",\n    xaxis = :log,\n    xlabel = \"lambda\",\n    ylabel = \"x\",\n)\nfor i in 1:n\n    plot!(gammas, x_values[i, :], label = \"x$i\")\nend\nplot!()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"introduction/dcp/#Extended-formulations-and-the-DCP-ruleset","page":"Extended formulations and the DCP ruleset","title":"Extended formulations and the DCP ruleset","text":"Convex.jl works by transforming the problem (which possibly has nonsmooth, nonlinear constructions like the nuclear norm, the log determinant, and so forth—into) a linear optimization problem subject to conic constraints.\n\nThe transformed problem often involves adding auxiliary variables, and it is called an \"extended formulation,\" since the original problem has been extended with additional variables.\n\nCreating an extended formulation relies on the problem being modeled by combining Convex.jl's \"atoms\" or primitives according to certain rules which ensure convexity, called the disciplined convex programming (DCP) ruleset. If these atoms are combined in a way that does not ensure convexity, the extended formulations are often invalid.","category":"section"},{"location":"introduction/dcp/#A-valid-formulation","page":"Extended formulations and the DCP ruleset","title":"A valid formulation","text":"As a simple example, consider the problem:\n\nusing Convex, SCS\nx = Variable();\nmodel_min = minimize(abs(x), [x >= 1, x <= 2]);\nsolve!(model_min, SCS.Optimizer; silent = true)\nx.value\n\nThe optimum occurs at x = 1, but let us imagine we want to solve this problem via Convex.jl using a linear programming (LP) solver.\n\nSince abs is a nonlinear function, we need to reformulate the problem to pass it to the LP solver. We do this by introducing an auxiliary variable t and instead solving:\n\nusing Convex, SCS\nx = Variable();\nt = Variable();\nmodel_min_extended = minimize(t, [x >= 1, x <= 2, t >= x, t >= -x]);\nsolve!(model_min_extended, SCS.Optimizer; silent = true)\nx.value\n\nThat is, we add the constraints t >= x and t >= -x, and replace abs(x) by t. Since we are minimizing over t and the smallest possible t satisfying these constraints is the absolute value of x, we get the right answer. This reformulation worked because we were minimizing abs(x), and that is a valid way to use the primitive abs.","category":"section"},{"location":"introduction/dcp/#An-invalid-formulation","page":"Extended formulations and the DCP ruleset","title":"An invalid formulation","text":"The reformulation of abx(x) works only if we are minimizing t.\n\nWhy? Well, let us consider the same reformulation for a maximization problem. The original problem is:\n\nusing Convex\nx = Variable();\nmodel_max = maximize(abs(x), [x >= 1, x <= 2])\n\nThis time, problem is DCP reports false. If we attempt to solve the problem, an error is thrown:\n\njulia> solve!(model_max, SCS.Optimizer; silent = true)\n┌ Warning: Problem not DCP compliant: objective is not DCP\n└ @ Convex ~/.julia/dev/Convex/src/problems.jl:73\nERROR: DCPViolationError: Expression not DCP compliant. This either means that your problem is not convex, or that we could not prove it was convex using the rules of disciplined convex programming. For a list of supported operations, see https://jump.dev/Convex.jl/stable/operations/. For help writing your problem as a disciplined convex program, please post a reproducible example on https://jump.dev/forum.\nStacktrace:\n[...]\n\nThe error is thrown because, if we do the same reformulation as before, we arrive at the problem:\n\nusing Convex, SCS\nx = Variable();\nt = Variable();\nmodel_max_extended = maximize(t, [x >= 1, x <= 2, t >= x, t >= -x]);\nsolve!(model_max_extended, SCS.Optimizer; silent = true)\n\nwhose solution is unbounded.\n\nIn other words, we can get the wrong answer by using the extended reformulation, because the extended formulation was only valid for a minimization problem.\n\nConvex.jl always creates the extended reformulation, but because they are only guaranteed to be valid when the DCP ruleset is followed, Convex.jl will programmatically check the whether or not these DCP rules were satisfied and error if they were not.","category":"section"},{"location":"examples/general_examples/svm_l1regularization/#SVM-with-L1-regularization","page":"SVM with L^1 regularization","title":"SVM with L^1 regularization","text":"# Generate data for SVM classifier with L1 regularization.\nusing Random\nRandom.seed!(3);\nn = 20;\nm = 1000;\nTEST = m;\nDENSITY = 0.2;\nbeta_true = randn(n, 1);\nidxs = randperm(n)[1:round(Int, (1 - DENSITY) * n)];\nbeta_true[idxs] .= 0\noffset = 0;\nsigma = 45;\nX = 5 * randn(m, n);\nY = sign.(X * beta_true .+ offset .+ sigma * randn(m, 1));\nX_test = 5 * randn(TEST, n);\nnothing #hide\n\n# Form SVM with L1 regularization problem.\nusing Convex, SCS, ECOS\n\nbeta = Variable(n);\nv = Variable();\nloss = sum(pos(1 - Y .* (X * beta - v)));\nreg = norm(beta, 1);\n\n# Compute a trade-off curve and record train and test error.\nTRIALS = 100\ntrain_error = zeros(TRIALS);\ntest_error = zeros(TRIALS);\nlambda_vals = exp10.(range(-2, stop = 0, length = TRIALS);)\nbeta_vals = zeros(length(beta), TRIALS);\nfor i in 1:TRIALS\n    lambda = lambda_vals[i]\n    problem = minimize(loss / m + lambda * reg)\n    solve!(problem, SCS.Optimizer; silent = true)\n    train_error[i] =\n        sum(\n            float(\n                sign.(X * beta_true .+ offset) .!=\n                sign.(evaluate(X * beta - v)),\n            ),\n        ) / m\n    test_error[i] =\n        sum(\n            float(\n                sign.(X_test * beta_true .+ offset) .!=\n                sign.(evaluate(X_test * beta - v)),\n            ),\n        ) / TEST\n    beta_vals[:, i] = evaluate(beta)\nend\n\nPlot the train and test error over the trade-off curve.\n\nusing Plots\nplot(lambda_vals, train_error, label = \"Train error\");\nplot!(lambda_vals, test_error, label = \"Test error\");\nplot!(xscale = :log, yscale = :log, ylabel = \"errors\", xlabel = \"lambda\")\n\nPlot the regularization path for beta.\n\nplot()\nfor i in 1:n\n    plot!(lambda_vals, vec(beta_vals[i, :]), label = \"beta$i\")\nend\nplot!(xscale = :log, ylabel = \"betas\", xlabel = \"lambda\")\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/#Fidelity-in-quantum-information-theory","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"This example is inspired from a lecture of John Watrous in the course on Theory of Quantum Information.\n\nThe Fidelity between two Hermitian semidefinite matrices P and Q is defined as:\n\nF(P Q) = P^12Q^12_texttr = max_U mathrmtr(P^12U Q^12)\n\nwhere the trace norm cdot_texttr is the sum of the singular values, and the maximization goes over the set of all unitary matrices U. This quantity can be expressed as the optimal value of the following complex-valued SDP:\n\nbeginarrayll\n  textmaximize   frac12texttr(Z+Z^dagger) \n  textsubject to \n   leftbeginarrayccPZZ^daggerQendarrayright succeq 0\n   Z in mathbf C^n times n\nendarray\n\nusing Convex, SCS, LinearAlgebra\n\nn = 20\nP = randn(n, n) + im * randn(n, n)\nP = P * P'\nQ = randn(n, n) + im * randn(n, n)\nQ = Q * Q'\nZ = ComplexVariable(n, n)\nobjective = 0.5 * real(tr(Z + Z'))\nconstraint = [P Z; Z' Q] ⪰ 0\nproblem = maximize(objective, constraint)\nsolve!(problem, SCS.Optimizer; silent = true)\n\ncomputed_fidelity = evaluate(objective)\n\n# Verify that computer fidelity is equal to actual fidelity\nP1, P2 = eigen(P)\nsqP = P2 * diagm([p1^0.5 for p1 in P1]) * P2'\nQ1, Q2 = eigen(Q)\nsqQ = Q2 * diagm([q1^0.5 for q1 in Q1]) * Q2'\n\nactual_fidelity = sum(svdvals(sqP * sqQ))\n\nWe can see that the actual fidelity value is very close the computed fidelity value.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"developer/credits/#Credits","page":"Credits","title":"Credits","text":"Convex.jl was created, developed, and maintained by:\n\nJenny Hong\nKaranveer Mohan\nMadeleine Udell\nDavid Zeng\n\nConvex.jl is currently developed and maintained by the Julia community; see Contributors for more.\n\nThe Convex.jl developers also thank:\n\nthe JuliaOpt team: Iain Dunning, Joey Huchette and Miles Lubin\nStephen Boyd, co-author of the book Convex Optimization\nSteven Diamond, developer of CVXPY and of a DCP tutorial website to teach disciplined convex programming.\nMichael Grant, developer of CVX.\nJohn Duchi and Hongseok Namkoong for  developing the representation of power cones in terms of SOCP constraints  used in this package.","category":"section"},{"location":"examples/mixed_integer/section_allocation/#Section-Allocation","page":"Section Allocation","title":"Section Allocation","text":"Suppose you have n students in a class who need to be assigned to m discussion sections. Each student needs to be assigned to exactly one section. Each discussion section should have between 6 and 10 students. Suppose an n times m preference matrix P is given, where P_ij gives student i's ranking for section j (1 would mean it is the student's top choice, 10,000 or a large number would mean the student can not attend that section).\n\nThe goal will be to get an allocation matrix X, where X_ij = 1 if student i is assigned to section j and 0 otherwise.\n\nusing Convex, GLPK\naux(str) = joinpath(@__DIR__, \"aux_files\", str) # path to auxiliary files\n\nLoad our preference matrix, P\n\ninclude(aux(\"data.jl\"))\n\nX = Variable(size(P), BinVar)\n\nWe want every student to be assigned to exactly one section. So, every row must have exactly one non-zero entry. In other words, the sum of all the columns for every row is 1. We also want each section to have between 6 and 10 students, so the sum of all the rows for every column should be between these.\n\nconstraints =\n    [sum(X, dims = 2) == 1, sum(X, dims = 1) <= 10, sum(X, dims = 1) >= 6];\nnothing #hide\n\nOur objective is simple sum(X .* P), which can be more efficiently represented as vec(X)' * vec(P). Since each entry of X is either 0 or 1, this is basically summing up the rankings of students that were assigned to them. If all students got their first choice, this value will be the number of students since the ranking of the first choice is 1.\n\np = minimize(vec(X)' * vec(P), constraints)\n\nsolve!(p, GLPK.Optimizer)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"introduction/installation/#Installation","page":"Installation","title":"Installation","text":"Install Convex.jl using the Julia package manager:\n\nusing Pkg\nPkg.add(\"Convex\")\n\nThis does not install any solvers. If you don't have a solver installed already, you will want to install a solver such as SCS by running:\n\nPkg.add(\"SCS\")\n\nTo solve certain problems such as mixed integer programming problems you will need to install another solver as well, such as HiGHS.\n\nIf you wish to use other solvers, please read the section on Solvers.","category":"section"},{"location":"reference/atoms/#Supported-operations","page":"Supported operations","title":"Supported operations","text":"Convex.jl supports the following functions. These functions may be composed according to the DCP composition rules to form new convex, concave, or affine expressions.","category":"section"},{"location":"reference/atoms/#*","page":"Supported operations","title":"*","text":"","category":"section"},{"location":"reference/atoms/#","page":"Supported operations","title":"+","text":"","category":"section"},{"location":"reference/atoms/#-2","page":"Supported operations","title":"-","text":"","category":"section"},{"location":"reference/atoms/#/","page":"Supported operations","title":"/","text":"","category":"section"},{"location":"reference/atoms/#.*","page":"Supported operations","title":".*","text":"","category":"section"},{"location":"reference/atoms/#./","page":"Supported operations","title":"./","text":"","category":"section"},{"location":"reference/atoms/#.","page":"Supported operations","title":".^","text":"","category":"section"},{"location":"reference/atoms/#abs","page":"Supported operations","title":"abs","text":"","category":"section"},{"location":"reference/atoms/#abs2","page":"Supported operations","title":"abs2","text":"","category":"section"},{"location":"reference/atoms/#adjoint","page":"Supported operations","title":"adjoint","text":"","category":"section"},{"location":"reference/atoms/#conj","page":"Supported operations","title":"conj","text":"","category":"section"},{"location":"reference/atoms/#conv","page":"Supported operations","title":"conv","text":"","category":"section"},{"location":"reference/atoms/#diag","page":"Supported operations","title":"diag","text":"","category":"section"},{"location":"reference/atoms/#diagm","page":"Supported operations","title":"diagm","text":"","category":"section"},{"location":"reference/atoms/#dot","page":"Supported operations","title":"dot","text":"","category":"section"},{"location":"reference/atoms/#dotsort","page":"Supported operations","title":"dotsort","text":"","category":"section"},{"location":"reference/atoms/#eigmax","page":"Supported operations","title":"eigmax","text":"","category":"section"},{"location":"reference/atoms/#eigmin","page":"Supported operations","title":"eigmin","text":"","category":"section"},{"location":"reference/atoms/#entropy","page":"Supported operations","title":"entropy","text":"","category":"section"},{"location":"reference/atoms/#entropy_elementwise","page":"Supported operations","title":"entropy_elementwise","text":"","category":"section"},{"location":"reference/atoms/#exp","page":"Supported operations","title":"exp","text":"","category":"section"},{"location":"reference/atoms/#geomean","page":"Supported operations","title":"geomean","text":"","category":"section"},{"location":"reference/atoms/#hcat","page":"Supported operations","title":"hcat","text":"","category":"section"},{"location":"reference/atoms/#hinge_loss","page":"Supported operations","title":"hinge_loss","text":"","category":"section"},{"location":"reference/atoms/#huber","page":"Supported operations","title":"huber","text":"","category":"section"},{"location":"reference/atoms/#hvcat","page":"Supported operations","title":"hvcat","text":"","category":"section"},{"location":"reference/atoms/#imag","page":"Supported operations","title":"imag","text":"","category":"section"},{"location":"reference/atoms/#inner_product","page":"Supported operations","title":"inner_product","text":"","category":"section"},{"location":"reference/atoms/#invpos","page":"Supported operations","title":"invpos","text":"","category":"section"},{"location":"reference/atoms/#kron","page":"Supported operations","title":"kron","text":"","category":"section"},{"location":"reference/atoms/#lieb_ando","page":"Supported operations","title":"lieb_ando","text":"","category":"section"},{"location":"reference/atoms/#log","page":"Supported operations","title":"log","text":"","category":"section"},{"location":"reference/atoms/#log_perspective","page":"Supported operations","title":"log_perspective","text":"","category":"section"},{"location":"reference/atoms/#logdet","page":"Supported operations","title":"logdet","text":"","category":"section"},{"location":"reference/atoms/#logisticloss","page":"Supported operations","title":"logisticloss","text":"","category":"section"},{"location":"reference/atoms/#logsumexp","page":"Supported operations","title":"logsumexp","text":"","category":"section"},{"location":"reference/atoms/#matrixfrac","page":"Supported operations","title":"matrixfrac","text":"","category":"section"},{"location":"reference/atoms/#max","page":"Supported operations","title":"max","text":"","category":"section"},{"location":"reference/atoms/#maximum","page":"Supported operations","title":"maximum","text":"","category":"section"},{"location":"reference/atoms/#min","page":"Supported operations","title":"min","text":"","category":"section"},{"location":"reference/atoms/#minimum","page":"Supported operations","title":"minimum","text":"","category":"section"},{"location":"reference/atoms/#neg","page":"Supported operations","title":"neg","text":"","category":"section"},{"location":"reference/atoms/#norm","page":"Supported operations","title":"norm","text":"","category":"section"},{"location":"reference/atoms/#norm2","page":"Supported operations","title":"norm2","text":"","category":"section"},{"location":"reference/atoms/#nuclearnorm","page":"Supported operations","title":"nuclearnorm","text":"","category":"section"},{"location":"reference/atoms/#opnorm","page":"Supported operations","title":"opnorm","text":"","category":"section"},{"location":"reference/atoms/#partialtrace","page":"Supported operations","title":"partialtrace","text":"","category":"section"},{"location":"reference/atoms/#partialtranspose","page":"Supported operations","title":"partialtranspose","text":"","category":"section"},{"location":"reference/atoms/#pos","page":"Supported operations","title":"pos","text":"","category":"section"},{"location":"reference/atoms/#qol_elementwise","page":"Supported operations","title":"qol_elementwise","text":"","category":"section"},{"location":"reference/atoms/#quadform","page":"Supported operations","title":"quadform","text":"","category":"section"},{"location":"reference/atoms/#quadoverlin","page":"Supported operations","title":"quadoverlin","text":"","category":"section"},{"location":"reference/atoms/#quantum_entropy","page":"Supported operations","title":"quantum_entropy","text":"","category":"section"},{"location":"reference/atoms/#quantum_relative_entropy","page":"Supported operations","title":"quantum_relative_entropy","text":"","category":"section"},{"location":"reference/atoms/#rationalnorm","page":"Supported operations","title":"rationalnorm","text":"","category":"section"},{"location":"reference/atoms/#real","page":"Supported operations","title":"real","text":"","category":"section"},{"location":"reference/atoms/#relative_entropy","page":"Supported operations","title":"relative_entropy","text":"","category":"section"},{"location":"reference/atoms/#reshape","page":"Supported operations","title":"reshape","text":"","category":"section"},{"location":"reference/atoms/#rootdet","page":"Supported operations","title":"rootdet","text":"","category":"section"},{"location":"reference/atoms/#sigmamax","page":"Supported operations","title":"sigmamax","text":"","category":"section"},{"location":"reference/atoms/#sqrt","page":"Supported operations","title":"sqrt","text":"","category":"section"},{"location":"reference/atoms/#square","page":"Supported operations","title":"square","text":"","category":"section"},{"location":"reference/atoms/#sum","page":"Supported operations","title":"sum","text":"","category":"section"},{"location":"reference/atoms/#sumlargest","page":"Supported operations","title":"sumlargest","text":"","category":"section"},{"location":"reference/atoms/#sumlargesteigs","page":"Supported operations","title":"sumlargesteigs","text":"","category":"section"},{"location":"reference/atoms/#sumsmallest","page":"Supported operations","title":"sumsmallest","text":"","category":"section"},{"location":"reference/atoms/#sumsquares","page":"Supported operations","title":"sumsquares","text":"","category":"section"},{"location":"reference/atoms/#tr","page":"Supported operations","title":"tr","text":"","category":"section"},{"location":"reference/atoms/#trace_logm","page":"Supported operations","title":"trace_logm","text":"","category":"section"},{"location":"reference/atoms/#trace_mpower","page":"Supported operations","title":"trace_mpower","text":"","category":"section"},{"location":"reference/atoms/#transpose","page":"Supported operations","title":"transpose","text":"","category":"section"},{"location":"reference/atoms/#vcat","page":"Supported operations","title":"vcat","text":"","category":"section"},{"location":"reference/atoms/#vec","page":"Supported operations","title":"vec","text":"","category":"section"},{"location":"reference/atoms/#Base.:*-Tuple{Convex.AbstractExpr, Convex.AbstractExpr}","page":"Supported operations","title":"Base.:*","text":"Base.:*(x::Convex.AbstractExpr, y::Convex.AbstractExpr)\n\nThe binary multiplication operator x times y.\n\nExamples\n\nulia> x = Variable();\n\njulia> 2 * x\n* (affine; real)\n├─ [2;;]\n└─ real variable (id: 709…007)\n\njulia> x = Variable(3);\n\njulia> y = [1, 2, 3];\n\njulia> x' * y\n* (affine; real)\n├─ reshape (affine; real)\n│  └─ * (affine; real)\n│     ├─ 3×3 SparseArrays.SparseMatrixCSC{Int64, Int64} with 3 stored entries\n│     └─ reshape (affine; real)\n│        └─ …\n└─ [1; 2; 3;;]\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.:+-Tuple{Convex.AbstractExpr, Convex.AbstractExpr}","page":"Supported operations","title":"Base.:+","text":"Base.:+(x::Convex.AbstractExpr, y::Convex.AbstractExpr)\nBase.:+(x::Convex.Value, y::Convex.AbstractExpr)\nBase.:+(x::Convex.AbstractExpr, y::Convex.Value)\n\nThe addition operator x + y.\n\nExamples\n\nApplies to scalar expressions:\n\njulia> x = Variable();\n\njulia> x + 1\n+ (affine; real)\n├─ real variable (id: 110…477)\n└─ [1;;]\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> y = [1, 2, 3];\n\njulia> atom = x + y\n+ (affine; real)\n├─ 3-element real variable (id: 458…482)\n└─ [1; 2; 3;;]\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.:--Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.:-","text":"Base.:-(x::Convex.AbstractExpr)\n\nThe univariate negation operator -x.\n\nExamples\n\nApplies to scalar expressions:\n\njulia> x = Variable();\n\njulia> -x\nConvex.NegateAtom (affine; real)\n├─ real variable (id: 161…677)\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = -x\nConvex.NegateAtom (affine; real)\n└─ 3-element real variable (id: 137…541)\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.:--Tuple{Convex.AbstractExpr, Convex.AbstractExpr}","page":"Supported operations","title":"Base.:-","text":"Base.:-(x::Convex.AbstractExpr, y::Convex.AbstractExpr)\nBase.:-(x::Convex.Value, y::Convex.AbstractExpr)\nBase.:-(x::Convex.AbstractExpr, y::Convex.Value)\n\nThe subtraction operator x - y.\n\nExamples\n\nApplies to scalar expressions:\n\njulia> x = Variable();\n\njulia> x - 1\n+ (affine; real)\n├─ real variable (id: 161…677)\n└─ [-1;;]\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> y = [1, 2, 3];\n\njulia> atom = y - x\n+ (affine; real)\n├─ [1; 2; 3;;]\n└─ Convex.NegateAtom (affine; real)\n   └─ 3-element real variable (id: 242…661)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.:/-Tuple{Convex.AbstractExpr, Union{Number, AbstractArray}}","page":"Supported operations","title":"Base.:/","text":"Base.:/(x::Convex.AbstractExpr, y::Convex.Value)\n\nThe binary division operator fracxy.\n\nExamples\n\nApplies to a scalar expression:\n\nulia> x = Variable();\n\njulia> x / 2\n\nand element-wise to a matrix:\n\njulia> x = Variable(3);\n\njulia> atom = x / 2\n* (affine; real)\n├─ 3-element real variable (id: 129…611)\n└─ [0.5;;]\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.Broadcast.broadcasted-Tuple{typeof(*), Convex.AbstractExpr, Convex.AbstractExpr}","page":"Supported operations","title":"Base.Broadcast.broadcasted","text":"x::Convex.AbstractExpr .* y::Convex.AbstractExpr\n\nElement-wise multiplication between matrices x and y.\n\nExamples\n\njulia> x = Variable(2);\n\njulia> atom = x .* 2\n* (affine; real)\n├─ 2-element real variable (id: 197…044)\n└─ [2;;]\n\njulia> atom = x .* [2, 4]\n.* (affine; real)\n├─ 2-element real variable (id: 197…044)\n└─ [2; 4;;]\n\njulia> size(atom)\n(2, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.Broadcast.broadcasted-Tuple{typeof(/), Convex.AbstractExpr, Union{Number, AbstractArray}}","page":"Supported operations","title":"Base.Broadcast.broadcasted","text":"x::Convex.AbstractExpr ./ y::Convex.AbstractExpr\n\nElement-wise division between matrices x and y.\n\nExamples\n\njulia> x = Variable(2);\n\njulia> atom = x ./ 2\n* (affine; real)\n├─ 2-element real variable (id: 875…859)\n└─ [0.5;;]\n\njulia> atom = x ./ [2, 4]\n.* (affine; real)\n├─ 2-element real variable (id: 875…859)\n└─ [0.5; 0.25;;]\n\njulia> size(atom)\n(2, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.Broadcast.broadcasted-Tuple{typeof(^), Convex.AbstractExpr, Int64}","page":"Supported operations","title":"Base.Broadcast.broadcasted","text":"x::Convex.AbstractExpr .^ k::Int\n\nElement-wise exponentiation of x to the power of k.\n\nExamples\n\njulia> x = Variable(2);\n\njulia> atom = x .^ 2\nqol_elem (convex; positive)\n├─ 2-element real variable (id: 131…737)\n└─ [1.0; 1.0;;]\n\njulia> size(atom)\n(2, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.abs-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.abs","text":"Base.abs(x::Convex.AbstractExpr)\n\nThe epigraph of x.\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> abs(x)\nabs (convex; positive)\n└─ real variable (id: 103…720)\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = abs(x)\nabs (convex; positive)\n└─ 3-element real variable (id: 389…882)\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.abs2-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.abs2","text":"Base.abs2(x::Convex.AbstractExpr)\n\nThe epigraph of x^2.\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> abs2(x)\nqol_elem (convex; positive)\n├─ abs (convex; positive)\n│  └─ real variable (id: 319…413)\n└─ [1.0;;]\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = abs2(x)\nqol_elem (convex; positive)\n├─ abs (convex; positive)\n│  └─ 3-element real variable (id: 123…996)\n└─ [1.0; 1.0; 1.0;;]\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.adjoint-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.adjoint","text":"LinearAlgebra.adjoint(x::AbstractExpr)\n\nThe transpose of the conjugated matrix x.\n\nExamples\n\njulia> x = ComplexVariable(2, 2);\n\njulia> atom = adjoint(x)\nreshape (affine; complex)\n└─ * (affine; complex)\n   ├─ 4×4 SparseArrays.SparseMatrixCSC{Int64, Int64} with 4 stored entries\n   └─ reshape (affine; complex)\n      └─ conj (affine; complex)\n         └─ …\n\njulia> size(atom)\n(2, 2)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.conj-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.conj","text":"Base.conj(x::Convex.AbstractExpr)\n\nThe complex conjugate of x.\n\nIf x is real, this function returns x.\n\nExamples\n\nApplies to a single expression:\n\njulia> x = ComplexVariable();\n\njulia> conj(x)\nconj (affine; complex)\n└─ complex variable (id: 180…137)\n\nAnd element-wise to a matrix of expressions:\n\nconj (affine; complex)\n└─ complex variable (id: 180…137)\n\njulia> x = ComplexVariable(3);\n\njulia> atom = conj(x)\nconj (affine; complex)\n└─ 3-element complex variable (id: 104…031)\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.conv-Tuple{Union{Number, AbstractArray}, Convex.AbstractExpr}","page":"Supported operations","title":"Convex.conv","text":"Convex.conv(x::Convex.AbstractExpr, y::Convex.AbstractExpr)\n\nThe convolution between two vectors x and y.\n\nExamples\n\njulia> x = Variable(2);\n\njulia> y = [2, 4];\n\njulia> atom = conv(x, y)\n* (affine; real)\n├─ 3×2 SparseArrays.SparseMatrixCSC{Int64, Int64} with 4 stored entries\n└─ 2-element real variable (id: 663…363)\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#LinearAlgebra.diag","page":"Supported operations","title":"LinearAlgebra.diag","text":"LinearAlgebra.diag(x::Convex.AbstractExpr, k::Int = 0)\n\nReturn the k-th diagonnal of the matrix X as a column vector.\n\nExamples\n\nApplies to a single square matrix:\n\njulia> x = Variable(2, 2);\n\njulia> atom = diag(x, 0)\ndiag (affine; real)\n└─ 2×2 real variable (id: 724…318)\n\njulia> size(atom)\n(2, 1)\n\njulia> atom = diag(x, 1)\ndiag (affine; real)\n└─ 2×2 real variable (id: 147…856)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"function"},{"location":"reference/atoms/#LinearAlgebra.diagm-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"LinearAlgebra.diagm","text":"LinearAlgebra.diagm(x::Convex.AbstractExpr)\n\nCreate a diagonal matrix out of the vector x.\n\nExamples\n\njulia> x = Variable(2);\n\njulia> atom = diagm(x)\ndiagm (affine; real)\n└─ 2-element real variable (id: 541…968)\n\njulia> size(atom)\n(2, 2)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#LinearAlgebra.dot-Tuple{Convex.AbstractExpr, Convex.AbstractExpr}","page":"Supported operations","title":"LinearAlgebra.dot","text":"LinearAlgebra.dot(x::Convex.AbstractExpr, y::Convex.AbstractExpr)\n\nThe dot product x cdot y. If x is complex, it is conjugated.\n\nExamples\n\njulia> x = ComplexVariable(2);\n\njulia> y = [1, 2];\n\njulia> atom = dot(x, y)\nsum (affine; complex)\n└─ .* (affine; complex)\n   ├─ conj (affine; complex)\n   │  └─ 2-element complex variable (id: 133…443)\n   └─ [1; 2;;]\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.dotsort-Tuple{Convex.AbstractExpr, Union{Number, AbstractArray}}","page":"Supported operations","title":"Convex.dotsort","text":"dotsort(x::Convex.AbstractExpr, y::Convex.Value)\ndotsort(x::Convex.Value, y::Convex.AbstractExpr)\n\nComputes dot(sort(x), sort(y)), where x or y is constant.\n\nFor example, if x = Variable(6) and y = [1 1 1 0 0 0], this atom computes the sum of the three largest elements of x.\n\nExamples\n\njulia> x = Variable(4);\n\njulia> atom = dotsort(x, [1, 0, 0, 1])\ndotsort (convex; real)\n└─ 4-element real variable (id: 128…367)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#LinearAlgebra.eigmax-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"LinearAlgebra.eigmax","text":"LinearAlgebra.eigmax(X::Convex.AbstractExpr)\n\nThe epigraph of the maximum eigen value of X.\n\nExamples\n\nApplies to a single square matrix:\n\njulia> x = Variable(2, 2);\n\njulia> atom = eigmax(x)\neigmin (convex; real)\n└─ 2×2 real variable (id: 428…695)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#LinearAlgebra.eigmin-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"LinearAlgebra.eigmin","text":"LinearAlgebra.eigmin(X::Convex.AbstractExpr)\n\nThe hypograph of the minimum eigen value of X.\n\nExamples\n\nApplies to a single square matrix:\n\njulia> x = Variable(2, 2);\n\njulia> atom = eigmin(x)\neigmin (concave; real)\n└─ 2×2 real variable (id: 428…695)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.entropy-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Convex.entropy","text":"entropy(x::Convex.AbstractExpr)\n\nThe hypograph of sum_i -x_i log x_i.\n\nExamples\n\nApplies to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = entropy(x)\nsum (concave; real)\n└─ entropy (concave; real)\n   └─ 3-element real variable (id: 901…778)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.entropy_elementwise-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Convex.entropy_elementwise","text":"entropy_elementwise(x::Convex.AbstractExpr)\n\nThe hypograph of -x log x.\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> entropy_elementwise(x)\nentropy (concave; real)\n└─ real variable (id: 172…395)\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = entropy_elementwise(x)\nentropy (concave; real)\n└─ 3-element real variable (id: 140…126)\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.exp-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.exp","text":"Base.exp(x::Convex.AbstractExpr)\n\nThe epigraph of e^x.\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> exp(x)\nexp (convex; positive)\n└─ real variable (id: 103…720)\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = exp(x)\nexp (convex; positive)\n└─ 3-element real variable (id: 389…882)\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.geomean-Tuple{Vararg{Union{Convex.AbstractExpr, Number, AbstractArray}}}","page":"Supported operations","title":"Convex.geomean","text":"geomean(x::Convex.AbstractExpr...)\n\nThe hypograph of the geometric mean sqrtnx_1 cdot x_2 cdot ldots x_n.\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> y = Variable();\n\njulia> geomean(x, y)\ngeomean (concave; positive)\n├─ real variable (id: 163…519)\n└─ real variable (id: 107…393)\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> y = Variable(3);\n\njulia> atom = geomean(x, y)\ngeomean (concave; positive)\n├─ 3-element real variable (id: 177…782)\n└─ 3-element real variable (id: 307…913)\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.hcat-Tuple{Vararg{Convex.AbstractExpr}}","page":"Supported operations","title":"Base.hcat","text":"Base.hcat(args::AbstractExpr...)\n\nHorizontally concatenate args.\n\nExamples\n\nApplies to a matrix:\n\njulia> x = Variable(2, 2);\n\njulia> atom = hcat(x, x)\nhcat (affine; real)\n├─ 2×2 real variable (id: 111…376)\n└─ 2×2 real variable (id: 111…376)\n\njulia> size(atom)\n(2, 4)\n\nYou can also use the Julia [x x] syntax:\n\njulia> x = Variable(2, 2);\n\njulia> atom = [x x]\nhcat (affine; real)\n├─ 2×2 real variable (id: 111…376)\n└─ 2×2 real variable (id: 111…376)\n\njulia> size(atom)\n(2, 4)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.hinge_loss-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Convex.hinge_loss","text":"hinge_loss(x::Convex.AbstractExpr)\n\nThe epigraph of max(1 - x 0).\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> hinge_loss(x)\nmax (convex; positive)\n├─ + (affine; real)\n│  ├─ [1;;]\n│  └─ Convex.NegateAtom (affine; real)\n│     └─ real variable (id: 129…000)\n└─ [0;;]\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = hinge_loss(x)\nmax (convex; positive)\n├─ + (affine; real)\n│  ├─ * (constant; positive)\n│  │  ├─ [1;;]\n│  │  └─ [1.0; 1.0; 1.0;;]\n│  └─ Convex.NegateAtom (affine; real)\n│     └─ 3-element real variable (id: 125…591)\n└─ [0;;]\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.huber","page":"Supported operations","title":"Convex.huber","text":"huber(x::Convex.AbstractExpr, M::Real = 1.0)\n\nThe epigraph of the Huber loss function:\n\nbegincases\n    x^2          x le M \n    2Mx - M^2  x  M\nendcases\n\nwhere M  0.\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> huber(x, 2.5)\nhuber (convex; positive)\n└─ real variable (id: 973…369)\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = huber(x)\nhuber (convex; positive)\n└─ 3-element real variable (id: 896…728)\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"function"},{"location":"reference/atoms/#Base.hvcat-Tuple{Tuple{Vararg{Int64}}, Vararg{Union{Convex.AbstractExpr, Number, AbstractArray}}}","page":"Supported operations","title":"Base.hvcat","text":"Base.hvcat(\n    rows::Tuple{Vararg{Int}},\n    args::Union{AbstractExpr,Value}...,\n)\n\nHorizontally and vertically concatenate args in single call.\n\nrows is the number of arguments to vertically concatenate into each column.\n\nExamples\n\nApplies to a matrix:\n\nTo make the matrix:\n\na    b[1] b[2]\nc[1] c[2] c[3]\n\ndo:\n\njulia> a = Variable();\n\njulia> b = Variable(1, 2);\n\njulia> c = Variable(1, 3);\n\njulia> atom = [a b; c]  # Syntactic sugar for: hvcat((2, 1), a, b, c)\nvcat (affine; real)\n├─ hcat (affine; real)\n│  ├─ real variable (id: 429…021)\n│  └─ 1×2 real variable (id: 120…326)\n└─ hcat (affine; real)\n   └─ 1×3 real variable (id: 124…615)\n\njulia> size(atom)\n(2, 3)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.imag-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.imag","text":"Base.imag(x::Convex.AbstractExpr)\n\nReturn the imaginary component of x.\n\nExamples\n\nApplies to a single expression:\n\njulia> x = ComplexVariable();\n\njulia> imag(x)\nimag (affine; real)\n└─ complex variable (id: 407…692)\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = ComplexVariable(3);\n\njulia> atom = imag(x)\nimag (affine; real)\n└─ 3-element complex variable (id: 435…057)\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.inner_product-Tuple{Convex.AbstractExpr, Convex.AbstractExpr}","page":"Supported operations","title":"Convex.inner_product","text":"inner_product(x::AbstractExpr, y::AbstractExpr)\n\nThe inner product tr(x^top y) where x and y are square matrices.\n\nExamples\n\njulia> x = Variable(2, 2);\n\njulia> y = [1 3; 2 4];\n\njulia> atom = inner_product(x, y)\nreal (affine; real)\n└─ sum (affine; real)\n   └─ diag (affine; real)\n      └─ * (affine; real)\n         ├─ …\n         └─ …\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.invpos-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Convex.invpos","text":"invpos(x::Convex.AbstractExpr)\n\nThe epigraph of frac1x.\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> invpos(x)\nqol_elem (convex; positive)\n├─ [1.0;;]\n└─ real variable (id: 139…839)\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = invpos(x)\nqol_elem (convex; positive)\n├─ [1.0; 1.0; 1.0;;]\n└─ 3-element real variable (id: 133…285)\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.kron-Tuple{Union{Number, AbstractArray}, Convex.AbstractExpr}","page":"Supported operations","title":"Base.kron","text":"Base.kron(x::Convex.AbstractExpr, y::Convex.AbstractExpr)\n\nThe Kronecker (outer) product.\n\nExamples\n\njulia> x = Variable(2);\n\njulia> y = [1 2];\n\njulia> atom = kron(x, y)\nvcat (affine; real)\n├─ * (affine; real)\n│  ├─ index (affine; real)\n│  │  └─ 2-element real variable (id: 369…232)\n│  └─ [1 2]\n└─ * (affine; real)\n   ├─ index (affine; real)\n   │  └─ 2-element real variable (id: 369…232)\n   └─ [1 2]\n\njulia> size(atom)\n(2, 2)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.lieb_ando-Tuple{Convex.AbstractExpr, Convex.AbstractExpr, Union{Convex.Constant, AbstractMatrix}, Rational}","page":"Supported operations","title":"Convex.lieb_ando","text":"lieb_ando(\n    A::Union{AbstractMatrix,Constant},\n    B::Union{AbstractMatrix,Constant},\n    K::Union{AbstractMatrix,Constant},\n    t::Rational,\n)\n\nReturns LinearAlgebra.tr(K' * A^{1-t} * K * B^t) where A and B are positive semidefinite matrices and K is an arbitrary matrix (possibly rectangular).\n\nlieb_ando(A, B, K, t) is concave in (A, B) for t in [0, 1], and convex in (A, B) for t in [-1, 0) or (1, 2]. K is a fixed matrix.\n\nSeems numerically unstable when t is on the endpoints of these ranges.\n\nReference\n\nPorted from CVXQUAD which is based on the paper: \"Lieb's concavity theorem, matrix geometric means and semidefinite optimization\" by Hamza Fawzi and James Saunderson (arXiv:1512.03401)\n\nExamples\n\nNote that lieb_ando is implemented as a subproblem, so the returned atom is a Convex.Problem object. The Problem atom can still be used as a regular 1x1 atom in other expressions.\n\njulia> A = Semidefinite(2, 2);\n\njulia> B = Semidefinite(3, 3);\n\njulia> K = [1 2 3; 4 5 6];\n\njulia> atom = lieb_ando(A, B, K, 1 // 2)\nProblem statistics\n  problem is DCP         : true\n  number of variables    : 3 (49 scalar elements)\n  number of constraints  : 4 (157 scalar elements)\n  number of coefficients : 76\n  number of atoms        : 26\n\nSolution summary\n  termination status : OPTIMIZE_NOT_CALLED\n  primal status      : NO_SOLUTION\n  dual status        : NO_SOLUTION\n\nExpression graph\n  maximize\n   └─ real (affine; real)\n      └─ sum (affine; real)\n         └─ diag (affine; real)\n            └─ …\n  subject to\n   ├─ GeometricMeanHypoConeSquare constraint (convex)\n   │  └─ vcat (affine; real)\n   │     ├─ reshape (affine; real)\n   │     │  └─ …\n   │     ├─ reshape (affine; real)\n   │     │  └─ …\n   │     └─ reshape (affine; real)\n   │        └─ …\n   ├─ PSD constraint (convex)\n   │  └─ 6×6 real variable (id: 173…902)\n   ├─ PSD constraint (convex)\n   │  └─ 6×6 real variable (id: 173…902)\n   ⋮\n\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.log-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.log","text":"Base.log(x::Convex.AbstractExpr)\n\nThe hypograph of log(x).\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> log(x)\nlog (concave; real)\n└─ real variable (id: 103…720)\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = log(x)\nlog (concave; real)\n└─ 3-element real variable (id: 161…499)\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.log_perspective-Tuple{Convex.AbstractExpr, Convex.AbstractExpr}","page":"Supported operations","title":"Convex.log_perspective","text":"log_perspective(x::Convex.AbstractExpr, y::Convex.AbstractExpr)\n\nThe hypograph the perspective of of the log function: sum y_i*log fracx_iy_i.\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> y = Variable();\n\njulia> log_perspective(x, y)\nConvex.NegateAtom (concave; real)\n└─ relative_entropy (convex; real)\n   ├─ real variable (id: 136…971)\n   └─ real variable (id: 131…344)\n\nAnd to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> y = Variable(3);\n\njulia> atom = log_perspective(x, y)\nConvex.NegateAtom (concave; real)\n└─ relative_entropy (convex; real)\n   ├─ 3-element real variable (id: 854…248)\n   └─ 3-element real variable (id: 111…174)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#LinearAlgebra.logdet-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"LinearAlgebra.logdet","text":"LinearAlgebra.logdet(X::Convex.AbstractExpr)\n\nThe hypograph of log(det(X)).\n\nExamples\n\nApplies to a single matrix expression:\n\njulia> X = Variable(2, 2);\n\njulia> atom = logdet(X)\nlogdet (concave; real)\n└─ 2×2 real variable (id: 159…883)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.logisticloss-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Convex.logisticloss","text":"logisticloss(x::Convex.AbstractExpr)\n\nReformulation for epigraph of the logistic loss: sum_i log(e^x_i + 1).\n\nThis reformulation uses logsumexp.\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> logisticloss(x)\nlogsumexp (convex; real)\n└─ vcat (affine; real)\n   ├─ real variable (id: 444…892)\n   └─ [0;;]\n\nAnd to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = logisticloss(x)\n+ (convex; real)\n├─ logsumexp (convex; real)\n│  └─ vcat (affine; real)\n│     ├─ index (affine; real)\n│     │  └─ …\n│     └─ [0;;]\n├─ logsumexp (convex; real)\n│  └─ vcat (affine; real)\n│     ├─ index (affine; real)\n│     │  └─ …\n│     └─ [0;;]\n└─ logsumexp (convex; real)\n   └─ vcat (affine; real)\n      ├─ index (affine; real)\n      │  └─ …\n      └─ [0;;]\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.logsumexp-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Convex.logsumexp","text":"logsumexp(x::Convex.AbstractExpr)\n\nThe epigraph of logleft(sum_i e^x_iright).\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable(2, 3);\n\njulia> atom = logsumexp(x)\nlogsumexp (convex; real)\n└─ 2×3 real variable (id: 121…604)\n\njulia> size(atom)\n(1, 1)\n\njulia> atom = logsumexp(x; dims = 1)\nlogsumexp (convex; real)\n└─ 2×3 real variable (id: 121…604)\n\njulia> size(atom)\n(1, 3)\n\njulia> atom = logsumexp(x; dims = 2)\nlogsumexp (convex; real)\n└─ 2×3 real variable (id: 121…604)\n\njulia> size(atom)\n(2, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.matrixfrac-Tuple{Convex.AbstractExpr, Convex.AbstractExpr}","page":"Supported operations","title":"Convex.matrixfrac","text":"matrixfrac(x::AbstractExpr, P::AbstractExpr)\n\nThe epigraph of x^top P^-1 x.\n\nExamples\n\njulia> x = Variable(2);\n\njulia> P = Variable(2, 2);\n\njulia> atom = matrixfrac(x, P)\nmatrixfrac (convex; positive)\n├─ 2-element real variable (id: 139…388)\n└─ 2×2 real variable (id: 126…414)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.max-Tuple{Convex.AbstractExpr, Convex.AbstractExpr}","page":"Supported operations","title":"Base.max","text":"Base.max(x::Convex.AbstractExpr, y::Convex.AbstractExpr)\nBase.max(x::Convex.AbstractExpr, y::Convex.Value)\nBase.max(x::Convex.Value, y::Convex.AbstractExpr)\n\nThe hypograph of max(x y).\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> max(x, 1)\nmax (convex; real)\n├─ real variable (id: 183…974)\n└─ [1;;]\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> y = [1, 2, 3];\n\njulia> atom = max(x, y)\nmax (convex; real)\n├─ 3-element real variable (id: 153…965)\n└─ [1; 2; 3;;]\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.maximum-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.maximum","text":"Base.maximum(x::Convex.AbstractExpr)\n\nThe hypograph of max(x).\n\nExamples\n\nApplies to a matrix expression:\n\njulia> x = Variable(3);\n\njulia> atom = maximum(x)\nmaximum (convex; real)\n└─ 3-element real variable (id: 159…219)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.min-Tuple{Convex.AbstractExpr, Convex.AbstractExpr}","page":"Supported operations","title":"Base.min","text":"Base.min(x::Convex.AbstractExpr, y::Convex.AbstractExpr)\nBase.min(x::Convex.Value, y::Convex.AbstractExpr)\nBase.min(x::Convex.AbstractExpr, y::Convex.Value)\n\nThe epigraph of min(x y).\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> min(x, 1)\nmin (concave; real)\n├─ real variable (id: 183…974)\n└─ [1;;]\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> y = [1, 2, 3];\n\njulia> atom = min(x, y)\nmin (concave; real)\n├─ 3-element real variable (id: 153…965)\n└─ [1; 2; 3;;]\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.minimum-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.minimum","text":"Base.minimum(x::Convex.AbstractExpr)\n\nThe epigraph of min(x).\n\nExamples\n\nApplies to a matrix expression:\n\njulia> x = Variable(3);\n\njulia> atom = minimum(x)\nminimum (convex; real)\n└─ 3-element real variable (id: 159…219)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.neg-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Convex.neg","text":"neg(x::Convex.AbstractExpr)\n\nThe epigraph of max(-x 0).\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> neg(x)\nmax (convex; positive)\n├─ Convex.NegateAtom (affine; real)\n│  └─ real variable (id: 467…111)\n└─ [0;;]\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = neg(x)\nmax (convex; positive)\n├─ Convex.NegateAtom (affine; real)\n│  └─ 3-element real variable (id: 224…439)\n└─ [0;;]\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#LinearAlgebra.norm","page":"Supported operations","title":"LinearAlgebra.norm","text":"norm(x::AbstractExpr, p::Real = 2)\n\nComputes the p-norm ‖x‖ₚ = (∑ᵢ |xᵢ|^p)^(1/p) of a vector expression x.\n\nMatrices are vectorized (i.e., norm(x) is the same as norm(vec(x)).)\n\nThe return value depends on the value of p. Specialized cases are used for p = 1, p = 2, and p = Inf.\n\nExamples\n\njulia> x = Variable(2);\n\njulia> atom = norm(x, 1)\nsum (convex; positive)\n└─ abs (convex; positive)\n   └─ 2-element real variable (id: 779…899)\n\njulia> size(atom)\n(1, 1)\n\njulia> norm(x, 2)\nnorm2 (convex; positive)\n└─ 2-element real variable (id: 779…899)\n\njulia> norm(x, Inf)\nmaximum (convex; positive)\n└─ abs (convex; positive)\n   └─ 2-element real variable (id: 779…899)\n\njulia> norm(x, 3 // 2)\nrationalnorm (convex; positive)\n└─ 2-element real variable (id: 779…899)\n\n\n\n\n\n","category":"function"},{"location":"reference/atoms/#LinearAlgebra.norm2-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"LinearAlgebra.norm2","text":"LinearAlgebra.norm2(x::Convex.AbstractExpr)\n\nThe epigraph of the 2-norm x_2.\n\nExamples\n\nApplies to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = norm2(x)\nnorm2 (convex; positive)\n└─ 3-element real variable (id: 162…975)\n\njulia> size(atom)\n(3, 1)\n\nAnd to a complex:\n\njulia> y = ComplexVariable(3);\n\njulia> atom = norm2(y)\nnorm2 (convex; positive)\n└─ vcat (affine; real)\n   ├─ real (affine; real)\n   │  └─ 3-element complex variable (id: 120…942)\n   └─ imag (affine; real)\n      └─ 3-element complex variable (id: 120…942)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.nuclearnorm-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Convex.nuclearnorm","text":"nuclearnorm(x::Convex.AbstractExpr)\n\nThe epigraph of the nuclear norm X_*, which is the sum of the singular values of X.\n\nExamples\n\nApplies to a real-valued matrix:\n\njulia> x = Variable(2, 2);\n\njulia> atom = nuclearnorm(x)\nnuclearnorm (convex; positive)\n└─ 2×2 real variable (id: 106…758)\n\njulia> size(atom)\n(1, 1)\n\njulia> y = ComplexVariable(2, 2);\n\njulia> atom = nuclearnorm(y)\nnuclearnorm (convex; positive)\n└─ 2×2 complex variable (id: 577…313)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#LinearAlgebra.opnorm","page":"Supported operations","title":"LinearAlgebra.opnorm","text":"LinearAlgebra.opnorm(x::Convex.AbstractExpr, p::Real = 2)\n\nThe epigraph of the matrix norm X_p.\n\nExamples\n\nApplies to a real- or complex-valued matrix:\n\njulia> x = Variable(2, 2);\n\njulia> atom = LinearAlgebra.opnorm(x, 1)\nmaximum (convex; positive)\n└─ * (convex; positive)\n   ├─ [1.0 1.0]\n   └─ abs (convex; positive)\n      └─ 2×2 real variable (id: 106…758)\n\njulia> atom = LinearAlgebra.opnorm(x, 2)\nopnorm (convex; positive)\n└─ 2×2 real variable (id: 106…758)\n\njulia> atom = LinearAlgebra.opnorm(x, Inf)\nmaximum (convex; positive)\n└─ * (convex; positive)\n    ├─ abs (convex; positive)\n    │  └─ 2×2 real variable (id: 106…758)\n    └─ [1.0; 1.0;;]\n\n\njulia> y = ComplexVariable(2, 2);\n\njulia> atom = maximum (convex; positive)\n└─ * (convex; positive)\n   ├─ abs (convex; positive)\n   │  └─ 2×2 complex variable (id: 116…943)\n   └─ [1.0; 1.0;;]\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"function"},{"location":"reference/atoms/#Convex.partialtrace-Tuple{Any, Int64, Vector}","page":"Supported operations","title":"Convex.partialtrace","text":"partialtrace(x, sys::Int, dims::Vector)\n\nReturns the partial trace of x over the systh system, where dims is a vector of integers encoding the dimensions of each subsystem.\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.partialtranspose-Tuple{Union{Convex.AbstractExpr, AbstractMatrix}, Int64, Vector}","page":"Supported operations","title":"Convex.partialtranspose","text":"partialtranspose(x, sys::Int, dims::Vector)\n\nReturns the partial transpose of x over the systh system, where dims is a vector of integers encoding the dimensions of each subsystem.\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.pos-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Convex.pos","text":"pos(x::Convex.AbstractExpr)\n\nThe epigraph of max(x 0).\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> pos(x)\nmax (convex; positive)\n├─ real variable (id: 467…111)\n└─ [0;;]\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = pos(x)\nmax (convex; positive)\n├─ 3-element real variable (id: 154…809)\n└─ [0;;]\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.qol_elementwise-Tuple{Convex.AbstractExpr, Convex.AbstractExpr}","page":"Supported operations","title":"Convex.qol_elementwise","text":"qol_elementwise(x::AbstractExpr, y::AbstractExpr)\n\nThe elementwise epigraph of fracx^2y.\n\nExamples\n\njulia> x = Variable(3);\n\njulia> y = Variable(3, Positive());\n\njulia> atom = qol_elementwise(x, y)\nqol_elem (convex; positive)\n├─ 3-element real variable (id: 155…648)\n└─ 3-element positive variable (id: 227…080)\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.quadform-Tuple{Union{Number, AbstractArray}, Convex.AbstractExpr}","page":"Supported operations","title":"Convex.quadform","text":"quadform(x::AbstractExpr, A::AbstractExpr; assume_psd=false)\n\nRepresents x^top A x where either:\n\nx is a vector-valued variable and A is a positive semidefinite or negative semidefinite matrix (and in particular Hermitian or real symmetric). If assume_psd=true, then A will be assumed to be positive semidefinite. Otherwise, Convex._is_psd will be used to check if A is positive semidefinite or negative semidefinite.\nor A is a matrix-valued variable and x is a vector.\n\nExamples\n\njulia> x = Variable(2);\n\njulia> A = [1 0; 0 1]\n2×2 Matrix{Int64}:\n 1  0\n 0  1\n\njulia> atom = quadform(x, A)\n* (convex; positive)\n├─ [1;;]\n└─ qol_elem (convex; positive)\n   ├─ norm2 (convex; positive)\n   │  └─ * (affine; real)\n   │     ├─ …\n   │     └─ …\n   └─ [1.0;;]\n\njulia> size(atom)\n(1, 1)\n\njulia> x = [1, 2]\n\njulia> A = Variable(2, 2);\n\njulia> atom = quadform(x, A)\n* (affine; real)\n├─ * (affine; real)\n│  ├─ [1 2]\n│  └─ 2×2 real variable (id: 111…794)\n└─ [1; 2;;]\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.quadoverlin-Tuple{Convex.AbstractExpr, Convex.AbstractExpr}","page":"Supported operations","title":"Convex.quadoverlin","text":"quadoverlin(x::AbstractExpr, y::AbstractExpr)\n\nThe epigraph of fracx_2^2y.\n\nExamples\n\njulia> x = Variable(3);\n\njulia> y = Variable(Positive());\n\njulia> atom = quadoverlin(x, y)\nqol (convex; positive)\n├─ 3-element real variable (id: 868…883)\n└─ positive variable (id: 991…712)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.quantum_entropy","page":"Supported operations","title":"Convex.quantum_entropy","text":"quantum_entropy(X::AbstractExpr, m::Integer, k::Integer)\n\nquantum_entropy returns -LinearAlgebra.tr(X*log(X)) where X is a positive semidefinite.\n\nNote this function uses logarithm base e, not base 2, so return value is in units of nats, not bits.\n\nQuantum entropy is concave. This function implements the semidefinite programming approximation given in the reference below.  Parameters m and k control the accuracy of this approximation: m is the number of quadrature nodes to use and k the number of square-roots to take. See reference for more details.\n\nThe implementation uses the expression\n\nH(X) = -tr(D_op(XI))\n\nwhere D_op is the operator relative entropy:\n\nD_op(XY) = X^12*logm(X^12 Y^-1 X^12)*X^12\n\nReference\n\nPorted from CVXQUAD which is based on the paper: \"Lieb's concavity theorem, matrix geometric means and semidefinite optimization\" by Hamza Fawzi and James Saunderson (arXiv:1512.03401)\n\nExamples\n\nApplies to a matrix:\n\njulia> X = Variable(2, 2);\n\njulia> atom = quantum_entropy(X)\nquantum_entropy (concave; positive)\n└─ 2×2 real variable (id: 700…694)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"function"},{"location":"reference/atoms/#Convex.quantum_relative_entropy","page":"Supported operations","title":"Convex.quantum_relative_entropy","text":"quantum_relative_entropy(\n    A::AbstractExpr,\n    B::AbstractExpr,\n    m::Integer,\n    k::Integer,\n)\n\nquantum_relative_entropy returns LinearAlgebra.tr(A*(log(A)-log(B))) where A and B are positive semidefinite matrices.\n\nNote this function uses logarithm base e, not base 2, so return value is in units of nats, not bits.\n\nQuantum relative entropy is convex (jointly) in (A, B). This function implements the semidefinite programming approximation given in the reference below. Parameters m and k control the accuracy of this approximation: m is the number of quadrature nodes to use and k the number of square-roots to take. See reference for more details.\n\nImplementation uses the expression\n\nD(AB) = e*D_op (A otimes I  I otimes B) )*e\n\nwhere D_op is the operator relative entropy and e = vec(Matrix(I, n, n)).\n\nReference\n\nPorted from CVXQUAD which is based on the paper: \"Lieb's concavity theorem, matrix geometric means and semidefinite optimization\" by Hamza Fawzi and James Saunderson (arXiv:1512.03401)\n\nExamples\n\njulia> A = Variable(2, 2);\n\njulia> B = Variable(2, 2);\n\njulia> atom = quantum_relative_entropy(A, B)\nquantum_relative_entropy (convex; positive)\n├─ 2×2 real variable (id: 144…849)\n└─ 2×2 real variable (id: 969…693)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"function"},{"location":"reference/atoms/#Convex.rationalnorm-Tuple{Convex.AbstractExpr, Rational{Int64}}","page":"Supported operations","title":"Convex.rationalnorm","text":"rationalnorm(x::AbstractExpr, k::Rational{Int})\n\nThe epigraph of ||x||_k.\n\nExamples\n\nApplies to a single matrix:\n\njulia> x = Variable(2);\n\njulia> atom = rationalnorm(x, 3 // 2)\nrationalnorm (convex; positive)\n└─ 2-element real variable (id: 182…293)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.real-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.real","text":"Base.real(x::Convex.AbstractExpr)\n\nReturn the real component of x.\n\nExamples\n\nApplies to a single expression:\n\njulia> x = ComplexVariable();\n\njulia> real(x)\nreal (affine; real)\n└─ complex variable (id: 407…692)\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = ComplexVariable(3);\n\njulia> atom = real(x)\nreal (affine; real)\n└─ 3-element complex variable (id: 435…057)\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.relative_entropy-Tuple{Convex.AbstractExpr, Convex.AbstractExpr}","page":"Supported operations","title":"Convex.relative_entropy","text":"relative_entropy(x::Convex.AbstractExpr, y::Convex.AbstractExpr)\n\nThe epigraph of sum x_i*log fracx_iy_i.\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> y = Variable();\n\njulia> relative_entropy(x, y)\nrelative_entropy (convex; real)\n├─ real variable (id: 124…372)\n└─ real variable (id: 409…346)\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> y = Variable(3);\n\njulia> atom = relative_entropy(x, y)\nrelative_entropy (convex; real)\n├─ 3-element real variable (id: 906…671)\n└─ 3-element real variable (id: 118…912)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.reshape-Tuple{Convex.AbstractExpr, Int64, Int64}","page":"Supported operations","title":"Base.reshape","text":"Base.reshape(x::AbstractExpr, m::Int, n::Int)\n\nReshapes the expression x into a matrix with m rows and n columns.\n\nExamples\n\nApplies to a matrix:\n\njulia> x = Variable(6, 1);\n\njulia> size(x)\n(6, 1)\n\njulia> atom = reshape(x, 2, 3)\nreshape (affine; real)\n└─ 6-element real variable (id: 103…813)\n\njulia> size(atom)\n(2, 3)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.rootdet-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Convex.rootdet","text":"Convex.rootdet(X::Convex.AbstractExpr)\n\nThe hypograph of det(X)^frac1n, where n is the side-dimension of the square matrix X.\n\nExamples\n\nApplies to a single matrix expression:\n\njulia> X = Variable(2, 2);\n\njulia> atom = rootdet(X)\nrootdet (concave; real)\n└─ 2×2 real variable (id: 159…883)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.sigmamax-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Convex.sigmamax","text":"sigmamax(x::Convex.AbstractExpr)\n\nThe epigraph of the spectral norm X_2, which is the maximum of the singular values of X.\n\nExamples\n\nApplies to a real- or complex-valued matrix:\n\njulia> x = Variable(2, 2);\n\njulia> atom = sigmamax(x)\nopnorm (convex; positive)\n└─ 2×2 real variable (id: 106…758)\n\njulia> size(atom)\n(1, 1)\n\njulia> y = ComplexVariable(2, 2);\n\njulia> atom = sigmamax(y)\nopnorm (convex; positive)\n└─ 2×2 complex variable (id: 577…313)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.sqrt-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.sqrt","text":"Base.sqrt(x::Convex.AbstractExpr)\n\nThe hypograph of sqrt x.\n\nExamples\n\nApplies to a single expression:\n\njulia> x = Variable();\n\njulia> sqrt(x)\ngeomean (concave; positive)\n├─ real variable (id: 576…546)\n└─ [1.0;;]\n\nAnd element-wise to a matrix of expressions:\n\njulia> x = Variable(3);\n\njulia> atom = sqrt(x)\ngeomean (concave; positive)\n├─ 3-element real variable (id: 181…583)\n└─ [1.0; 1.0; 1.0;;]\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.square-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Convex.square","text":"square(x::AbstractExpr)\n\nThe epigraph of x^2.\n\nExamples\n\nApplies elementwise to a matrix\n\njulia> x = Variable(3);\n\njulia> atom = square(x)\nqol_elem (convex; positive)\n├─ 3-element real variable (id: 438…681)\n└─ [1.0; 1.0; 1.0;;]\n\njulia> size(atom)\n(3, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.sum-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.sum","text":"Base.sum(x::Convex.AbstractExpr; dims = :)\n\nSum x, optionally along a dimension dims.\n\nExamples\n\nSum all elements in an expression:\n\njulia> x = Variable(2, 2);\n\njulia> atom = sum(x)\nsum (affine; real)\n└─ 2×2 real variable (id: 263…449)\n\njulia> size(atom)\n(1, 1)\n\nSum along the first dimension, creating a row vector:\n\njulia> x = Variable(2, 2);\n\njulia> atom = sum(x; dims = 1)\n* (affine; real)\n├─ [1.0 1.0]\n└─ 2×2 real variable (id: 143…826)\n\njulia> size(atom)\n(1, 2)\n\nSum along the second dimension, creating a columnn vector:\n\njulia> atom = sum(x; dims = 2)\n* (affine; real)\n├─ 2×2 real variable (id: 143…826)\n└─ [1.0; 1.0;;]\n\njulia> size(atom)\n(2, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.sumlargest-Tuple{Convex.AbstractExpr, Int64}","page":"Supported operations","title":"Convex.sumlargest","text":"sumlargest(x::Convex.AbstractExpr, k::Int)\n\nSum the k largest values of x.\n\nExamples\n\nApplies to a matrix:\n\njulia> x = Variable(3, 3);\n\njulia> atom = sumlargest(x, 2)\nsumlargest (convex; real)\n├─ 3×3 real variable (id: 833…482)\n└─ [2;;]\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.sumlargesteigs-Tuple{Convex.AbstractExpr, Int64}","page":"Supported operations","title":"Convex.sumlargesteigs","text":"sumlargesteigs(x::Convex.AbstractExpr, k::Int)\n\nSum the k largest eigen values of x.\n\nExamples\n\nApplies to a matrix:\n\njulia> x = Variable(3, 3);\n\njulia> atom = sumlargesteigs(x, 2)\nsumlargesteigs (convex; real)\n├─ 3×3 real variable (id: 833…482)\n└─ [2;;]\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.sumsmallest-Tuple{Convex.AbstractExpr, Int64}","page":"Supported operations","title":"Convex.sumsmallest","text":"sumsmallest(x::Convex.AbstractExpr, k::Int)\n\nSum the k smallest values of x.\n\nExamples\n\nApplies to a matrix:\n\njulia> x = Variable(3, 3);\n\njulia> atom = sumsmallest(x, 2)\nConvex.NegateAtom (concave; real)\n└─ sumlargest (convex; real)\n   └─ Convex.NegateAtom (affine; real)\n      └─ 3×3 real variable (id: 723…082)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.sumsquares-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Convex.sumsquares","text":"sumsquares(x::AbstractExpr)\n\nThe epigraph of x_2^2.\n\nExamples\n\nApplies to a single matrix\n\njulia> x = Variable(3);\n\njulia> atom = sumsquares(x)\nqol (convex; positive)\n├─ 3-element real variable (id: 125…181)\n└─ [1;;]\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#LinearAlgebra.tr-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"LinearAlgebra.tr","text":"LinearAlgebra.tr(x::AbstractExpr)\n\nThe trace of the matrix x.\n\nExamples\n\njulia> x = Variable(2, 2);\n\njulia> atom = tr(x)\nsum (affine; real)\n└─ diag (affine; real)\n   └─ 2×2 real variable (id: 844…180)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Convex.trace_logm","page":"Supported operations","title":"Convex.trace_logm","text":"trace_logm(\n    X::Convex.AbstractExpr,\n    C::AbstractMatrix,\n    m::Integer = 3,\n    k::Integer = 3,\n)\n\ntrace_logm(X, C) returns LinearAlgebra.tr(C*logm(X)) where X and C are positive definite matrices and C is constant.\n\ntrace_logm is concave in X.\n\nThis function implements the semidefinite programming approximation given in the reference below. Parameters m and k control the accuracy of the approximation: m is the number of quadrature nodes to use and k is the number of square-roots to take. See reference for more details.\n\nImplementation uses the expression\n\ntr(C times logm(X)) = -tr(C times D_op(IX))\n\nwhere D_{op} is the operator relative entropy:\n\nD_op(XY) = X^12*logm(X^12 Y^-1 X^12)*X^12\n\nReference\n\nPorted from CVXQUAD which is based on the paper: \"Lieb's concavity theorem, matrix geometric means and semidefinite optimization\" by Hamza Fawzi and James Saunderson (arXiv:1512.03401)\n\nExamples\n\nApplies to a matrix:\n\njulia> X = Variable(2, 2);\n\njulia> C = [1 0; 0 1];\n\njulia> atom = trace_logm(X, C)\ntrace_logm (concave; real)\n└─ 2×2 real variable (id: 608…362)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"function"},{"location":"reference/atoms/#Convex.trace_mpower-Tuple{Convex.AbstractExpr, Rational, Union{Convex.Constant, AbstractMatrix}}","page":"Supported operations","title":"Convex.trace_mpower","text":"trace_mpower(A::Convex.AbstractExpr, t::Rational, C::AbstractMatrix)\n\ntrace_mpower(A, t, C) returns LinearAlgebra.tr(C*A^t) where A and C are positive definite matrices, C is constant and t is a rational in [-1, 2].\n\nWhen t is in [0, 1], trace_mpower(A, t, C) is concave in A (for fixed positive semidefinite matrix C) and convex for t in [-1, 0) or (1, 2].\n\nReference\n\nPorted from CVXQUAD which is based on the paper: \"Lieb's concavity theorem, matrix geometric means and semidefinite optimization\" by Hamza Fawzi and James Saunderson (arXiv:1512.03401)\n\nExamples\n\nApplies to a matrix:\n\njulia> A = Variable(2, 2);\n\njulia> C = [1 0; 0 1];\n\njulia> atom = trace_mpower(A, 1 // 2, C)\ntrace_mpower (concave; real)\n└─ 2×2 real variable (id: 150…626)\n\njulia> size(atom)\n(1, 1)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.transpose-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.transpose","text":"LinearAlgebra.transpose(x::AbstractExpr)\n\nThe transpose of the matrix x.\n\nExamples\n\njulia> x = Variable(2, 2);\n\njulia> atom = transpose(x)\nreshape (affine; real)\n└─ * (affine; real)\n   ├─ 4×4 SparseArrays.SparseMatrixCSC{Int64, Int64} with 4 stored entries\n   └─ reshape (affine; real)\n      └─ 2×2 real variable (id: 151…193)\n\njulia> size(atom)\n(2, 2)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.vcat-Tuple{Vararg{Convex.AbstractExpr}}","page":"Supported operations","title":"Base.vcat","text":"Base.vcat(args::AbstractExpr...)\n\nVertically concatenate args.\n\nExamples\n\nApplies to a matrix:\n\njulia> x = Variable(2, 2);\n\njulia> atom = vcat(x, x)\nvcat (affine; real)\n├─ 2×2 real variable (id: 111…376)\n└─ 2×2 real variable (id: 111…376)\n\njulia> size(atom)\n(4, 2)\n\nYou can also use the Julia [x; x] syntax:\n\njulia> x = Variable(2, 2);\n\njulia> atom = [x; x]\nvcat (affine; real)\n├─ 2×2 real variable (id: 111…376)\n└─ 2×2 real variable (id: 111…376)\n\njulia> size(atom)\n(4, 2)\n\n\n\n\n\n","category":"method"},{"location":"reference/atoms/#Base.vec-Tuple{Convex.AbstractExpr}","page":"Supported operations","title":"Base.vec","text":"Base.vec(x::AbstractExpr)\n\nReshapes the expression x into a column vector.\n\nExamples\n\nApplies to a matrix:\n\njulia> x = Variable(2, 2);\n\njulia> atom = vec(x)\nreshape (affine; real)\n└─ 2×2 real variable (id: 115…295)\n\njulia> size(atom)\n(4, 1)\n\n\n\n\n\n","category":"method"},{"location":"manual/complex-domain_optimization/#Optimization-with-Complex-Variables","page":"Complex-domain Optimization","title":"Optimization with Complex Variables","text":"Convex.jl also supports optimization with complex variables. Below, we present a quick start guide on how to use Convex.jl for optimization with complex variables, and then list the operations supported on complex variables in Convex.jl. In general, any operation available in Convex.jl that is well defined and DCP compliant on complex variables should be available. We list these functions below. organized by the type of cone (linear, second-order, or semidefinite) used to represent that operation.\n\nInternally, Convex.jl transforms the complex-domain problem to a larger real-domain problem using a bijective mapping. It then solves the real-domain problem and transforms the solution back to the complex domain.","category":"section"},{"location":"manual/complex-domain_optimization/#Complex-Variables","page":"Complex-domain Optimization","title":"Complex Variables","text":"Complex Variables in Convex.jl are declared in the same way as the variables are declared but using the different keyword ComplexVariable.\n\n  # Scalar complex variable\n  z = ComplexVariable()\n\n  # Column vector variable\n  z = ComplexVariable(5)\n\n  # Matrix variable\n  z = ComplexVariable(4, 6)\n\n  # Complex Positive Semidefinite variable\n  z = HermitianSemidefinite(4)","category":"section"},{"location":"manual/complex-domain_optimization/#Linear-Program-Representable-Functions-(complex-variables)","page":"Complex-domain Optimization","title":"Linear Program Representable Functions (complex variables)","text":"All of the linear functions that are listed under Linear Program Representable Functions operate on complex variables as well. In addition, several specialized functions for complex variables are available:\n\noperation description vexity slope notes\nreal(z) real part of complex of variable affine increasing none\nimag(z) imaginary part of complex variable affine increasing none\nconj(x) element-wise complex conjugate affine increasing none\ninnerproduct(x,y) real(trace(x'*y)) affine increasing PR: one argument is constant","category":"section"},{"location":"manual/complex-domain_optimization/#Second-Order-Cone-Representable-Functions-(complex-variables)","page":"Complex-domain Optimization","title":"Second-Order Cone Representable Functions (complex variables)","text":"Most of the second order cone function listed under Second-Order Cone Representable Functions operate on complex variables as well. Notable exceptions include:\n\ninverse\nsquare\nsqrt\ngeomean\nhuber\n\nOne new function is available:\n\noperation description vexity slope notes\nabs2(z) square(abs(z)) convex increasing none","category":"section"},{"location":"manual/complex-domain_optimization/#Semidefinite-Program-Representable-Functions-(complex-variables)","page":"Complex-domain Optimization","title":"Semidefinite Program Representable Functions (complex variables)","text":"All SDP-representable functions listed under Semidefinite Program Representable Functions work for complex variables.","category":"section"},{"location":"manual/complex-domain_optimization/#Exponential-SDP-representable-Functions-(complex-variables)","page":"Complex-domain Optimization","title":"Exponential + SDP representable Functions (complex variables)","text":"Complex variables also support logdet function.","category":"section"},{"location":"manual/complex-domain_optimization/#Optimizing-over-quantum-states","page":"Complex-domain Optimization","title":"Optimizing over quantum states","text":"The complex and Hermitian matrix variables, along with the kron and partialtrace operations, enable the definition of a wide range of problems in quantum information theory. As a simple example, let us consider a state rho over a composite Hilbert space mathcalH_AotimesmathcalH_B, where both component spaces are isomorphic to mathbbC^2. Assume that rho is a product state, with its component in mathcalH_A given as A, a complex-valued matrix. We can optimize over the second component B to meet some requirement. Here we simply fix the second component too, but via the partialtrace operator:\n\nusing Convex, SCS\nA = [ 0.47213595 0.11469794+0.48586827im; 0.11469794-0.48586827im  0.52786405]\nB = ComplexVariable(2, 2)\nρ = kron(A, B)\nconstraints = [\n    partialtrace(ρ, 1, [2; 2]) == [1 0; 0 0],\n    tr(ρ) == 1,\n    isposdef(ρ),\n  ]\np = satisfy(constraints)\nsolve!(p, SCS.Optimizer; silent = true)\np.status\n\nSince we fix both components as trace-1 positive semidefinite matrices, the last two constraints are actually redundant in this case.","category":"section"},{"location":"examples/general_examples/logistic_regression/#Logistic-regression","page":"Logistic regression","title":"Logistic regression","text":"using DataFrames\nusing Plots\nusing RDatasets\nusing Convex\nusing SCS\n\nThis is an example logistic regression using RDatasets's iris data. Our goal is to predict whether the iris species is versicolor using the sepal length and width and petal length and width.\n\niris = dataset(\"datasets\", \"iris\");\niris[1:10, :]\n\nWe'll define Y as the outcome variable: +1 for versicolor, -1 otherwise.\n\nY = [species == \"versicolor\" ? 1.0 : -1.0 for species in iris.Species]\n\nWe'll create our data matrix with one column for each feature (first column corresponds to offset).\n\nX = hcat(\n    ones(size(iris, 1)),\n    iris.SepalLength,\n    iris.SepalWidth,\n    iris.PetalLength,\n    iris.PetalWidth,\n);\nnothing #hide\n\nNow to solve the logistic regression problem.\n\nn, p = size(X)\nbeta = Variable(p)\nproblem = minimize(logisticloss(-Y .* (X * beta)))\nsolve!(problem, SCS.Optimizer; silent = true)\n\nLet's see how well the model fits.\n\nusing Plots\nlogistic(x::Real) = inv(exp(-x) + one(x))\nperm = sortperm(vec(X * evaluate(beta)))\nplot(1:n, (Y[perm] .+ 1) / 2, st = :scatter)\nplot!(1:n, logistic.(X * evaluate(beta))[perm])\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"developer/contributing/#Contributing","page":"Contributing","title":"Contributing","text":"We'd welcome contributions to the Convex.jl package. Here are some short instructions on how to get started. If you don't know what you'd like to contribute, you could\n\ntake a look at the current   issues and pick   one. (Feature requests are probably the easiest to tackle.)\nadd a usage   example.\n\nThen submit a pull request (PR). (Let us know if it's a work in progress by putting [WIP] in the name of the PR.)","category":"section"},{"location":"developer/contributing/#Adding-examples","page":"Contributing","title":"Adding examples","text":"Take a look at our existing usage   examples   and add another in similar style.\nSubmit a PR. (Let us know if it's a work in progress by putting   [WIP] in the name of the PR.)\nWe'll look it over, fix up anything that doesn't work, and merge   it.","category":"section"},{"location":"developer/contributing/#Adding-atoms","page":"Contributing","title":"Adding atoms","text":"Here are the steps to add a new function or operation (atom) to Convex.jl. Let's say you're adding the new function f.\n\nTake a look at the   nuclear norm atom   for an example of how to construct atoms, and see the   norm atom   for an example of an atom that depends on a parameter.\nCopy paste (for example, the nuclear norm file), replace anything saying   nuclear norm with the name of the atom f, fill in monotonicity,   curvature, etc. Save it in the appropriate subdirectory of   src/atoms/.\nEnsure the atom is a mutable struct, so that objectid can be called.\nAdd as a comment a description of what the atom does and its   parameters.\nEnsure evaluate is only called during the definition of evaluate itself, from conic_form!, or   on constants or matrices. Specifically, evaluate must not be called on a potentially fix!'d variable   when the expression tree is being built (for example, when constructing an atom or reformulating),   since then any changes to the variable's value (or it being free!'d) will not be recognized.   See #653 and #585   for previous bugs caused by misuse here.\nDo not add constraints to variables directly during a reformulation.   Instead, create an atom to control the vexity, or return a partially   specified problem: minimize(var, constraints...), to ensure the vexity   of the result is correct.\nThe most mathematically interesting part is the new_conic_form!   function. Following the example in the nuclear norm atom, you'll   see that you can just construct the problem whose optimal value is   f(x), introducing any auxiliary variables you need, exactly as   you would normally in Convex.jl, and then call conic_form!   on that problem.\nAdd a test for the atom so we can verify it works in   src/problem_depot/problem/<cone>, where <cone> matches the subdirectory of   src/atoms. See How to write a ProblemDepot problem for details   on how to write the tests.\nFollowing the other examples, add a test to test/test_atoms.jl.\nSubmit a PR, including a description of what the atom does and its   parameters. (Let us know if it's a work in progress by putting   [WIP] in the name of the PR.)\nWe'll look it over, fix up anything that doesn't work, and merge   it.","category":"section"},{"location":"developer/contributing/#Fixing-the-guts","page":"Contributing","title":"Fixing the guts","text":"If you want to do a more major bug fix, you may need to understand how Convex.jl thinks about conic form.\n\nTo do this, start by reading the Convex.jl paper.\n\nYou may find our JuliaCon 2014 talk helpful as well; you can find the Jupyter notebook presented in the talk here.\n\nConvex has been updated several times over the years however, so older information may be out of date. Here is a brief summary of how the package works (as of July 2023).\n\nA Problem{T} struct is created by putting together an objective function and constraints. The problem is an expression graph, in which variables and constants are the leaf nodes, and atoms form the intermediate nodes. Here T refers to the numeric type of the problem that all data coefficients will be coerced to when we pass the data to a solver.\nWhen we go to solve! a problem, we first load it into a MathOptInterface (MOI) model. To do so, we traverse the Problem and apply our extended formulations. This occurs via conic_form!. We construct a Context{T} associated to the problem, which holds an MOI model, and progressively load it by applying conic_form! to each object's children and then itself. For variables, conic_form! returns SparseTape{T} or ComplexTape{T}, depending on the sign variable. Likewise for constants, conic_form! returns either Vector{T} or ComplexStructOfVec{T}. Here a Tape refers to a lazy sequence of sparse affine operators that will be applied to a vector of variables. The central computational task of Convex is to compose this sequence of operators (and thus enact it's extended formulations). For atoms, conic_form! generally either creates a new object using Convex' primitives (for example, another problem) and calls conic_form! on that, or, when that isn't possible, calls operate to manipulate the tape objects themselves (for example, to add a new operation to the composition). We try to minimize the amount of operate methods and defer to existing primitives when possible. conic_form! can also create new constraints and add them directly to the model. It is easy to create constraints of the form \"vector-affine-function-in-cone\" for any of MOI's many supported cones; these constraints do not need to be exposed at the level of Convex itself as Constraint objects, although they can be.\nOnce we have filled our Context{T}, we go to solve it with MOI. Then we recover the solution status and values of primal and dual variables, and populate them using dictionaries stored in the Context.\n\nYou're now armed and dangerous. Go ahead and open an issue (or comment on a previous one) if you can't figure something out, or submit a PR if you can figure it out. (Let us know if it's a work in progress by putting [WIP] in the name of the PR.)\n\nPRs that comment the code more thoroughly will also be welcomed.","category":"section"},{"location":"developer/contributing/#Developer-notes","page":"Contributing","title":"Developer notes","text":"conic_form! is allowed to mutate the context, but should never mutate the atoms or problems\nWe currently construct a fresh context on every solve. It may be possible to set things up to reuse contexts for efficiency.\nData flow: we take in user data that may be of any type.\nAt the level of problem formulation (when we construct atoms), we convert everything to an AbstractExpr (or Constraint); in particular, constants become Constant or ComplexConstant. At this time we don't know the numeric type that will be used to solve the problem.\nOnce we begin to solve! the problem, we recursively call conic_form!. The output is of type SparseTape{T}, ComplexTape{T}, Vector{T}, or ComplexStructOfVec{T}. We can call operate to manipulate these outputs.\nWe convert these to MOI.VectorAffineFunction before passing them to MOI.","category":"section"},{"location":"examples/general_examples/basic_usage/#Basic-Usage","page":"Basic Usage","title":"Basic Usage","text":"First we load Convex itself, LinearAlgebra to access the identity matrix I, and two solvers: SCS and GLPK.\n\nusing Convex\nusing LinearAlgebra\nusing SCS, GLPK","category":"section"},{"location":"examples/general_examples/basic_usage/#Linear-program","page":"Basic Usage","title":"Linear program","text":"beginarrayll\n  textmaximize  c^T x \n  textsubject to  A x leq b\n   x geq 1 \n   x leq 10 \n   x_2 leq 5 \n   x_1 + x_4 - x_2 leq 10 \nendarray\n\nx = Variable(4)\nc = [1; 2; 3; 4]\nA = I(4)\nb = [10; 10; 10; 10]\nconstraints = [A * x <= b, x >= 1, x <= 10, x[2] <= 5, x[1] + x[4] - x[2] <= 10]\np = minimize(dot(c, x), constraints) # or c' * x\nsolve!(p, SCS.Optimizer; silent = true)\n\nWe can also inspect the objective value and the values of the variables at the solution:\n\nprintln(round(p.optval, digits = 2))\nprintln(round.(evaluate(x), digits = 2))\nprintln(evaluate(x[1] + x[4] - x[2]))","category":"section"},{"location":"examples/general_examples/basic_usage/#Matrix-Variables-and-promotions","page":"Basic Usage","title":"Matrix Variables and promotions","text":"beginarrayll\n  textminimize   X _F + y \n  textsubject to  2 X leq 1\n   X + y geq 1 \n   X geq 0 \n   y geq 0 \nendarray\n\nX = Variable(2, 2)\ny = Variable()\n# X is a 2 x 2 variable, and y is scalar. X' + y promotes y to a 2 x 2 variable before adding them\np = minimize(norm(X) + y, 2 * X <= 1, X' + y >= 1, X >= 0, y >= 0)\nsolve!(p, SCS.Optimizer; silent = true)\n\nWe can also inspect the values of the variables at the solution:\n\nprintln(round.(evaluate(X), digits = 2))\nprintln(evaluate(y))\np.optval","category":"section"},{"location":"examples/general_examples/basic_usage/#Norm,-exponential-and-geometric-mean","page":"Basic Usage","title":"Norm, exponential and geometric mean","text":"beginarrayll\n  textsatisfy   x _2 leq 100 \n   e^x_1 leq 5 \n   x_2 geq 7 \n   sqrtx_3 x_4 geq x_2\nendarray\n\nx = Variable(4)\np = satisfy(\n    norm(x) <= 100,\n    exp(x[1]) <= 5,\n    x[2] >= 7,\n    geomean(x[3], x[4]) >= x[2],\n)\nsolve!(p, SCS.Optimizer; silent = true)","category":"section"},{"location":"examples/general_examples/basic_usage/#PSD-cone-and-Eigenvalues","page":"Basic Usage","title":"PSD cone and Eigenvalues","text":"y = Semidefinite(2)\np = maximize(eigmin(y), tr(y) <= 6)\nsolve!(p, SCS.Optimizer; silent = true)\n\nx = Variable()\ny = Variable((2, 2))\n\n# PSD constraints\np = minimize(x + y[1, 1], y ⪰ 0, x >= 1, y[2, 1] == 1)\nsolve!(p, SCS.Optimizer; silent = true)","category":"section"},{"location":"examples/general_examples/basic_usage/#Mixed-integer-program","page":"Basic Usage","title":"Mixed integer program","text":"beginarrayll\n  textminimize  sum_i=1^n x_i \n    textsubject to  x in mathbbZ^n \n   x geq 05 \nendarray\n\nx = Variable(4, IntVar)\np = minimize(sum(x), x >= 0.5)\nsolve!(p, GLPK.Optimizer; silent = true)\n\nAnd the value of x at the solution:\n\nevaluate(x)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"manual/operations/#Supported-Operations","page":"Supported Operations","title":"Supported Operations","text":"Convex.jl currently supports the following functions. These functions may be composed according to the DCP composition rules to form new convex, concave, or affine expressions.\n\nConvex.jl transforms each problem into an equivalent conic program in order to pass the problem to a specialized solver. Depending on the types of functions used in the problem, the conic constraints may include linear, second-order, exponential, or semidefinite constraints, as well as any binary or integer constraints placed on the variables.\n\nBelow, we list each function available in Convex.jl organized by the (most complex) type of cone used to represent that function, and indicate which solvers may be used to solve problems with those cones. Problems mixing many different conic constraints can be solved by any solver that supports every kind of cone present in the problem.\n\nIn the notes column in the tables below, we denote implicit constraints imposed on the arguments to the function by IC, and parameter restrictions that the arguments must obey by PR. Convex.jl will automatically impose ICs; the user must make sure to satisfy PRs.\n\nElementwise means that the function operates elementwise on vector arguments, returning a vector of the same size.","category":"section"},{"location":"manual/operations/#Linear-Program-Representable-Functions","page":"Supported Operations","title":"Linear Program Representable Functions","text":"An optimization problem using only these functions can be solved by any LP solver.\n\noperation description vexity slope notes\nx+y or x.+y addition affine increasing \nx-y or x.-y subtraction affine increasing in x decreasing in y \nx*y multiplication affine increasing if constant term ge 0 decreasing if constant term le 0 not monotonic otherwise PR: one argument is constant\nx/y division affine increasing PR: y is scalar constant\nx .* y elementwise multiplication affine increasing PR: one argument is constant\nx ./ y elementwise division affine increasing PR: one argument is constant\nx[1:4, 2:3] indexing and slicing affine increasing \ndiag(x, k) k-th diagonal of a matrix affine increasing \ndiagm(x) construct diagonal matrix affine increasing PR: x is a vector\nx' transpose affine increasing \nvec(x) vector representation affine increasing \ndot(x,y) sum_i x_i y_i affine increasing PR: one argument is constant\nkron(x,y) Kronecker product affine increasing PR: one argument is constant\nsum(x) sum_ij x_ij affine increasing \nsum(x, k) sum elements across dimension k affine increasing \nsumlargest(x, k) sum of k largest elements of x convex increasing \nsumsmallest(x, k) sum of k smallest elements of x concave increasing \ndotsort(a, b) dot(sort(a),sort(b)) convex increasing PR: one argument is constant\nreshape(x, m, n) reshape into m times n affine increasing \nminimum(x) min(x) concave increasing \nmaximum(x) max(x) convex increasing \n[x y] or [x; y] hcat(x, y) or vcat(x, y) stacking affine increasing \ntr(x) computes the trace mathrmtr left(X right) affine increasing \npartialtrace(x,sys,dims) Partial trace affine increasing \npartialtranspose(x,sys,dims) Partial transpose affine increasing \nconv(h,x) h in mathbbR^m, x in mathbbR^n, hstar x in mathbbR^m+n-1; entry i is given by sum_j=1^m h_jx_i-j+1 with x_k=0 for k out of bounds affine increasing if hge 0 decreasing if hle 0 not monotonic otherwise PR: h is constant\nmin(x,y) min(xy) concave increasing \nmax(x,y) max(xy) convex increasing \npos(x) max(x0) convex increasing \nneg(x) max(-x0) convex decreasing \ninvpos(x) 1x convex decreasing IC: x0\nabs(x) leftxright convex increasing on x ge 0 decreasing on x le 0 \nopnorm(x, 1) maximum absolute column sum: max_1  j  n sum_i=1^m leftx_ijright convex increasing on x ge 0 decreasing on x le 0 \nopnorm(x, Inf) maximum absolute row sum: max_1  i  m sum_j=1^n leftx_ijright convex increasing on x ge 0 decreasing on x le 0 ","category":"section"},{"location":"manual/operations/#Second-Order-Cone-Representable-Functions","page":"Supported Operations","title":"Second-Order Cone Representable Functions","text":"An optimization problem using these functions can be solved by any SOCP solver (including ECOS, SCS, Mosek, Gurobi, and CPLEX). Of course, if an optimization problem has both LP and SOCP representable functions, then any solver that can solve both LPs and SOCPs can solve the problem.\n\noperation description vexity slope notes\nnorm(x, p) (sum x_i^p)^1p convex increasing on x ge 0 decreasing on x le 0 PR: p >= 1\nquadform(x, P; assume_psd=false) x^T P x convex in x affine in P increasing on x ge 0 decreasing on x le 0 increasing in P PR: either x or P must be constant; if x is not constant, then P must be symmetric and positive semidefinite. Pass assume_psd=true to skip checking if P is positive semidefinite.\nquadoverlin(x, y) x^T xy convex increasing on x ge 0 decreasing on x le 0 decreasing in y IC: y  0\nsumsquares(x) sum x_i^2 convex increasing on x ge 0 decreasing on x le 0 \nsqrt(x) sqrtx concave decreasing IC: x0\nsquare(x), x^2 x^2 convex increasing on x ge 0 decreasing on x le 0 PR : x is scalar\nx .^ 2 x^2 convex increasing on x ge 0 decreasing on x le 0 elementwise\ngeomean(x, y) sqrtxy concave increasing IC: xge0, yge0\nhuber(x, M=1) begincases x^2 x leq M  2Mx - M^2 x  M endcases convex increasing on x ge 0 decreasing on x le 0 PR: M  0\n\nNote that for p=1 and p=Inf, the function norm(x,p) is a linear-program representable, and does not need a SOCP solver, and for a matrix x, norm(x,p) is defined as norm(vec(x), p).","category":"section"},{"location":"manual/operations/#Exponential-Cone-Representable-Functions","page":"Supported Operations","title":"Exponential Cone Representable Functions","text":"An optimization problem using these functions can be solved by any exponential cone solver (SCS).\n\noperation description vexity slope notes\nlogsumexp(x) log(sum_i exp(x_i)) convex increasing \nexp(x) exp(x) convex increasing \nlog(x) log(x) concave increasing IC: x0\nentropy(x) sum_ij -x_ij log(x_ij) concave not monotonic IC: x0\nentropy_elementwise(x) -x log(x) concave not monotonic IC: x0\nlogisticloss(x) log(1 + exp(x_i)) convex increasing \nrelative_entropy(x, y) sum_i x_i log(x_i  y_i) convex not monotonic IC: x0 y0","category":"section"},{"location":"manual/operations/#Semidefinite-Program-Representable-Functions","page":"Supported Operations","title":"Semidefinite Program Representable Functions","text":"An optimization problem using these functions can be solved by any SDP solver (including SCS and Mosek).\n\noperation description vexity slope notes\nnuclearnorm(x) sum of singular values of x convex not monotonic \nopnorm(x, 2) (operatornorm(x)) max of singular values of x convex not monotonic \neigmax(x) max eigenvalue of x convex not monotonic \neigmin(x) min eigenvalue of x concave not monotonic \nrootdet(X) n-th root-determinant of the n-by-n matrix X, that is det(X)^1n concave not monotonic \nmatrixfrac(x, P) x^TP^-1x convex not monotonic IC: P is positive semidefinite\nsumlargesteigs(x, k) sum of top k eigenvalues of x convex not monotonic IC: P symmetric\nConvex.Constraint((T, A, B), GeometricMeanHypoConeSquare(t, size(T, 1))) T preceq A _t B = A^12 (A^-12 B A^-12)^t A^12 concave increasing IC: A succeq 0, B succeq 0, t in 01\nConvex.Constraint((T, A, B), GeometricMeanEpiConeSquare(t, size(T, 1))) T succeq A _t B = A^12 (A^-12 B A^-12)^t A^12 convex not monotonic IC: A succeq 0, B succeq 0, t in -1 0 cup 1 2\nquantum_entropy(X) -textrmTr(X log X) concave not monotonic IC: X succeq 0; uses natural log\nquantum_relative_entropy(A, B) textrmTr(A log A - A log B) convex not monotonic IC: A succeq 0, B succeq 0; uses natural log\ntrace_logm(X, C) textrmTr(C log X) concave in X not monotonic IC: X succeq 0, C succeq 0, C constant; uses natural log\ntrace_mpower(A, t, C) textrmTr(C A^t) concave in A for t in 01, convex for t in -10 cup 12 not monotonic IC: X succeq 0, C succeq 0, C constant, t in -1 2\nlieb_ando(A, B, K, t) textrmTr(K A^1-t K B^t) concave in A,B for t in 01, convex for t in -10 cup 12 not monotonic IC: A succeq 0, B succeq 0, K constant, t in -1 2\nConvex.Constraint((T, X, Y), RelativeEntropyEpiConeSquare(size(X, 1), m, k, e)) T succeq e X^12 log(X^12 Y^-1 X^12) X^12 e convex not monotonic IC: e constant; uses natural log","category":"section"},{"location":"manual/operations/#Exponential-SDP-representable-Functions","page":"Supported Operations","title":"Exponential + SDP representable Functions","text":"An optimization problem using these functions can be solved by any solver that supports exponential constraints and semidefinite constraints simultaneously (SCS).\n\noperation description vexity slope notes\nlogdet(x) log of determinant of x concave increasing IC: x is positive semidefinite","category":"section"},{"location":"manual/operations/#Promotions","page":"Supported Operations","title":"Promotions","text":"When an atom or constraint is applied to a scalar and a higher dimensional variable, the scalars are promoted. For example, we can do max(x, 0) gives an expression with the shape of x whose elements are the maximum of the corresponding element of x and 0.","category":"section"},{"location":"manual/types/#Basic-Types","page":"Basic Types","title":"Basic Types","text":"The basic building block of Convex.jl is called an expression, which can represent a variable, a constant, or a function of another expression. We discuss each kind of expression in turn.","category":"section"},{"location":"manual/types/#Variables","page":"Basic Types","title":"Variables","text":"The simplest kind of expression in Convex.jl is a variable. Variables in Convex.jl are declared using the Variable keyword, along with the dimensions of the variable.\n\n# Scalar variable\nx = Variable()\n\n# Column vector variable\nx = Variable(5)\n\n# Matrix variable\nx = Variable(4, 6)\n\nVariables may also be declared as having special properties, such as being\n\n(elementwise) positive: x = Variable(4, Positive())\n(elementwise) negative: x = Variable(4, Negative())\nintegral: x = Variable(4, IntVar)\nbinary: x = Variable(4, BinVar)\n(for a matrix) being symmetric, with nonnegative eigenvalues (that is,    positive semidefinite): z = Semidefinite(4)\n\nThe order of the arguments is the size, the sign, and then the Convex.VarType (that is, integer, binary, or continuous), and any may be omitted to use the default. The current value of a variable x can be accessed with evaluate(x). After solve!ing a problem, the value of each variable used in the problem is set to its optimal value.\n\nSee also Custom Variable Types for how to implement your own variable types.","category":"section"},{"location":"manual/types/#Constants","page":"Basic Types","title":"Constants","text":"Numbers, vectors, and matrices present in the Julia environment are wrapped automatically into a Constant expression when used in a Convex.jl expression.","category":"section"},{"location":"manual/types/#Expressions","page":"Basic Types","title":"Expressions","text":"Expressions in Convex.jl are formed by applying any atom (mathematical function defined in Convex.jl) to variables, constants, and other expressions. For a list of these functions, see Supported operations. Atoms are applied to expressions using operator overloading. For example, 2+2 calls Julia's built-in addition operator, while 2+x calls the Convex.jl addition method and returns a Convex.jl expression. Many of the useful language features in Julia, such as arithmetic, array indexing, and matrix transpose are overloaded in Convex.jl so they may be used with variables and expressions just as they are used with native Julia types.\n\nExpressions that are created must be DCP-compliant. More information on DCP can be found here. :\n\nx = Variable(5)\n# The following are all expressions\ny = sum(x)\nz = 4 * x + y\nz_1 = z[1]\n\nConvex.jl allows the values of the expressions to be evaluated directly.\n\nx = Variable()\ny = Variable()\nz = Variable()\nexpr = x + y + z\nproblem = minimize(expr, x >= 1, y >= x, 4 * z >= y)\nsolve!(problem, SCS.Optimizer)\n\n# Once the problem is solved, we can call evaluate() on expr:\nevaluate(expr)\n\nwarning: Warning\nAvoid using Julia's broadcasting (the . operator). As an example, the expression -log(1.0 .+ A * x) will fail while -log(1.0 + A * x) will succeed.","category":"section"},{"location":"manual/types/#Constraints","page":"Basic Types","title":"Constraints","text":"Constraints in Convex.jl are declared using the standard comparison operators <=, >=, and ==. They specify relations that must hold between two expressions. Convex.jl does not distinguish between strict and non-strict inequality constraints.\n\nx = Variable(5, 5)\n# Equality constraint\nconstraint = x == 0\n# Inequality constraint\nconstraint = x >= 1\n\nNote that constraints apply elementwise automatically; that is, x >= 1 means that x[i, j] >= 1 for i in 1:5 and j in 1:5. Consequently, broadcasting should not be used to constrain arrays, that is, use x >= y instead of x .>= y.\n\nMatrices can also be constrained to be positive semidefinite.\n\nx = Variable(3, 3)\ny = Variable(3, 1)\nz = Variable()\n# constrain [x y; y' z] to be positive semidefinite\nconstraint = isposdef([x y; y' z])\n# or equivalently,\nconstraint = ([x y; y' z] ⪰ 0)\n\nConstraints can also be added to variables after their construction, to automatically apply constraints to any problem which uses the variable. For example,\n\nx = Variable(3)\nadd_constraint!(x, sum(x) == 1)\n\nNow, in any problem in which x is used, the constraint sum(x) == 1 will be added.","category":"section"},{"location":"manual/types/#Objective","page":"Basic Types","title":"Objective","text":"The objective of the problem is a scalar expression to be maximized or minimized by using maximize or minimize respectively. Feasibility problems can be expressed by either giving a constant as the objective, or using problem = satisfy(constraints).","category":"section"},{"location":"manual/types/#Problem","page":"Basic Types","title":"Problem","text":"A problem in Convex.jl consists of a sense (minimize, maximize, or satisfy), an objective (an expression to which the sense verb is to be applied), and zero or more constraints that must be satisfied at the solution. Problems may be constructed as\n\nproblem = minimize(objective, constraints)\n# or\nproblem = maximize(objective, constraints)\n# or\nproblem = satisfy(constraints)\n\nConstraints can be added at any time before the problem is solved.\n\n# No constraints given\nproblem = minimize(objective)\n# Add some constraint\nproblem.constraints += constraint\n# Add many more constraints\nproblem.constraints += [constraint1, constraint2, ...]\n\nA problem can be solved by calling solve!\n\nsolve!(problem, solver)\n\npassing a solver such as SCS.Optimizer() from the package SCS as the second argument.\n\nAfter the problem is solved, problem.status records the status returned by the optimization solver, and problem.optval will record the optimum value of the problem. The optimal value for each variable x participating in the problem can be found in evaluate(x). The optimal value of an expression can be found by calling the evaluate() function on the expression as follows: evaluate(expr).","category":"section"},{"location":"examples/mixed_integer/n_queens/#N-queens","page":"N queens","title":"N queens","text":"using Convex, GLPK, LinearAlgebra, SparseArrays, Test\naux(str) = joinpath(@__DIR__, \"aux_files\", str) # path to auxiliary files\ninclude(aux(\"antidiag.jl\"))\n\nn = 8\n\nWe encode the locations of the queens with a matrix of binary random variables.\n\nx = Variable((n, n), BinVar)\n\nNow we impose the constraints: at most one queen on any anti-diagonal, at most one queen on any diagonal, and we must have exactly one queen per row and per column.\n\n# At most one queen on any anti-diagonal\nconstraints = Constraint[sum(antidiag(x, k)) <= 1 for k in -n+2:n-2]\n# At most one queen on any diagonal\nappend!(constraints, [sum(diag(x, k)) <= 1 for k in -n+2:n-2])\n# Exactly one queen per row and one queen per column\nappend!(constraints, [sum(x, dims = 1) == 1, sum(x, dims = 2) == 1])\np = satisfy(constraints)\nsolve!(p, GLPK.Optimizer)\n\nLet us test the results:\n\nfor k in -n+2:n-2\n    @test evaluate(sum(antidiag(x, k))) <= 1\n    @test evaluate(sum(diag(x, k))) <= 1\nend\n@test all(evaluate(sum(x, dims = 1)) .≈ 1)\n@test all(evaluate(sum(x, dims = 2)) .≈ 1)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/tomography/tomography/#Tomography","page":"Tomography","title":"Tomography","text":"Tomography is the process of reconstructing a density distribution from given integrals over sections of the distribution. In our example, we will work with tomography on black and white images. Suppose x be the vector of n pixel densities, with x_j denoting how white pixel j is. Let y be the vector of m line integrals over the image, with y_i denoting the integral for line i. We can define a matrix A to describe the geometry of the lines. Entry A_ij describes how much of pixel j is intersected by line i. Assuming our measurements of the line integrals are perfect, we have the relationship that\n\n  y = Ax\n\nHowever, anytime we have measurements, there are usually small errors that occur. Therefore it makes sense to try to minimize\n\n y - Ax_2^2\n\nThis is simply an unconstrained least squares problem; something we can readily solve.\n\nusing Convex, ECOS, DelimitedFiles, SparseArrays\naux(str) = joinpath(@__DIR__, \"aux_files\", str) # path to auxiliary files\nline_mat_x = readdlm(aux(\"tux_sparse_x.txt\"))\nsummary(line_mat_x)\n\nline_mat_y = readdlm(aux(\"tux_sparse_y.txt\"))\nsummary(line_mat_y)\n\nline_mat_val = readdlm(aux(\"tux_sparse_val.txt\"))\nsummary(line_mat_val)\n\nline_vals = readdlm(aux(\"tux_sparse_lines.txt\"))\nsummary(line_vals)\n\nForm the sparse matrix from the data Image is 50 x 50\n\nimg_size = 50\n\nThe number of pixels in the image\n\nnum_pixels = img_size * img_size\n\nline_mat = spzeros(length(line_vals), num_pixels)\n\nnum_vals = length(line_mat_val)\n\nfor i in 1:num_vals\n    x = Int(line_mat_x[i])\n    y = Int(line_mat_y[i])\n    line_mat[x+1, y+1] = line_mat_val[i]\nend\n\npixel_colors = Variable(num_pixels)\n# line_mat * pixel_colors should be close to the line_integral_values\n# to reflect that, we minimize a norm\nobjective = sumsquares(line_mat * pixel_colors - line_vals)\nproblem = minimize(objective)\nsolve!(problem, ECOS.Optimizer; silent = true)\n\nrows = zeros(img_size * img_size)\ncols = zeros(img_size * img_size)\nfor i in 1:img_size\n    for j in 1:img_size\n        rows[(i-1)*img_size+j] = i\n        cols[(i-1)*img_size+j] = img_size + 1 - j\n    end\nend\n\nPlot the image using the pixel values obtained:\n\nusing Plots\nimage = reshape(evaluate(pixel_colors), img_size, img_size)\nheatmap(\n    image,\n    yflip = true,\n    aspect_ratio = 1,\n    colorbar = nothing,\n    color = :grays,\n)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/mixed_integer/binary_knapsack/#Binary-(or-0-1)-knapsack-problem","page":"Binary (or 0-1) knapsack problem","title":"Binary (or 0-1) knapsack problem","text":"Given a knapsack of some capacity C and n objects with object i having weight w_i and profit p_i, the goal is to choose some subset of the objects that can fit in the knapsack (that is, the sum of their weights is no more than C) while maximizing profit.\n\nThis can be formulated as a mixed-integer program as:\n\nbeginarrayll\n  textmaximize  x p \n    textsubject to  x in 0 1 \n   w x leq C \nendarray\n\nwhere x is a vector is size n where x_i is one if we chose to keep the object in the knapsack, 0 otherwise.\n\n# Data taken from http://people.sc.fsu.edu/~jburkardt/datasets/knapsack_01/knapsack_01.html\nw = [23; 31; 29; 44; 53; 38; 63; 85; 89; 82]\nC = 165\np = [92; 57; 49; 68; 60; 43; 67; 84; 87; 72];\nn = length(w)\n\nusing Convex, GLPK\nx = Variable(n, BinVar)\nproblem = maximize(dot(p, x), dot(w, x) <= C)\nsolve!(problem, GLPK.Optimizer)\n\nevaluate(x)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/supplemental_material/paper_examples/#Paper-examples","page":"Paper examples","title":"Paper examples","text":"using Convex, ECOS\n\nSummation.\n\nprintln(\"Summation example\")\nx = Variable();\ne = 0;\n@time begin\n    for i in 1:1000\n        global e\n        e += x\n    end\n    p = minimize(e, x >= 1)\nend\n@time solve!(p, ECOS.Optimizer; silent = true)\n\nIndexing.\n\nprintln(\"Indexing example\")\nx = Variable(1000, 1);\ne = 0;\n@time begin\n    for i in 1:1000\n        global e\n        e += x[i]\n    end\n    p = minimize(e, x >= ones(1000, 1))\nend\n@time solve!(p, ECOS.Optimizer; silent = true)\n\nMatrix constraints.\n\nprintln(\"Matrix constraint example\")\nn, m, p = 100, 100, 100\nX = Variable(m, n);\nA = randn(p, m);\nb = randn(p, n);\n@time begin\n    p = minimize(norm(X), A * X == b)\nend\n@time solve!(p, ECOS.Optimizer; silent = true)\n\nTranspose.\n\nprintln(\"Transpose example\")\nX = Variable(5, 5);\nA = randn(5, 5);\n@time begin\n    p = minimize(norm2(X - A), X' == X)\nend\n@time solve!(p, ECOS.Optimizer; silent = true)\n\nn = 3\nA = randn(n, n);\n#@time begin\nX = Variable(n, n);\np = minimize(norm(X' - A), X[1, 1] == 1);\nsolve!(p, ECOS.Optimizer; silent = true)\n#end\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/#Phase-recovery-using-MaxCut","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"In this example, we relax the phase retrieval problem similar to the classical MaxCut semidefinite program and recover the phase of the signal given the magnitude of the linear measurements.\n\nPhase recovery has wide applications such as  in X-ray and crystallography imaging, diffraction imaging or microscopy and audio signal processing. In all these applications, the detectors cannot measure the phase of the incoming wave and only record its amplitude i.e complex measurements of a signal x in mathbbC^p are obtained from a linear injective operator A, but we can only measure the magnitude vector Ax, not the phase of Ax.\n\nRecovering the phase of Ax from Ax is a nonconvex optimization problem. Using results from this paper, the problem can be relaxed to a (complex) semidefinite program (complex SDP).\n\nThe original representation of the problem is as follows:\n\nbeginarrayll\n  textfind  x in mathbbC^p \n    textsubject to  Ax = b\nendarray\n\nwhere A in mathbbC^n times p and b in mathbbR^n.\n\nIn this example, the problem is to find the phase of Ax given the value Ax. Given a linear operator A and a vector b= Ax of measured amplitudes, in the noiseless case, we can write Ax = textdiag(b)u where u in mathbbC^n  is a phase vector, satisfying mathbbu_i = 1 for i = 1ldots n.\n\nWe relax this problem as Complex Semidefinite Programming.","category":"section"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/#Relaxed-Problem-similar-to-[MaxCut](http://www-math.mit.edu/goemans/PAPERS/maxcut-jacm.pdf)","page":"Phase recovery using MaxCut","title":"Relaxed Problem similar to MaxCut","text":"Define the positive semidefinite hermitian matrix M = textdiag(b) (I - A A^*) textdiag(b). The problem is:\n\nbeginarrayll\n  textminimize  langle U M rangle \n    textsubject to  textdiag(U) = 1\n     U succeq 0\nendarray\n\nHere the variable U must be hermitian (U in mathbbH_n ) and we have a solution to the phase recovery problem if U = u u^* has rank one. Otherwise, the leading singular vector of U can be used to approximate the solution.\n\nusing Convex, SCS, LinearAlgebra\n\nn = 20\np = 2\nA = rand(n, p) + im * randn(n, p)\nx = rand(p) + im * randn(p)\nb = abs.(A * x) + rand(n)\n\nM = diagm(b) * (I(n) - A * A') * diagm(b)\nU = ComplexVariable(n, n)\nobjective = inner_product(U, M)\nc1 = diag(U) == 1\nc2 = isposdef(U)\np = minimize(objective, c1, c2)\nsolve!(p, SCS.Optimizer; silent = true)\n\nevaluate(U)\n\nVerify if the rank of U is 1:\n\nB, C = eigen(evaluate(U));\nlength([e for e in B if (abs(real(e)) > 1e-4)])\n\nDecompose U = uu^* where u is the phase of Ax\n\nu = C[:, 1];\nfor i in 1:n\n    u[i] = u[i] / abs(u[i])\nend\nu\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"examples/general_examples/robust_approx_fitting/#Robust-approximate-fitting","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"Section 6.4.2 Boyd & Vandenberghe \"Convex Optimization\" Original by Lieven Vandenberghe Adapted for Convex by Joelle Skaf - 10/03/05\n\nAdapted for Convex.jl by Karanveer Mohan and David Zeng - 26/05/14 Original CVX code and plots here: http://web.cvxr.com/cvx/examples/cvxbook/Ch06_approx_fitting/html/fig6_15.html\n\nConsider the least-squares problem:       minimize (A + tB)x - b_2 where t is an uncertain parameter in [-1,1] Three approximate solutions are found:\n\nnominal optimal (that is, letting t=0)\nstochastic robust approximation:      minimize mathbbE(A+tB)x - b_2 assuming u is uniformly distributed on [-1,1]. (reduces to minimizing mathbbE (A+tB)x-b^2 = A*x-b^2  + x^TPx   where P = mathbbE(t^2) B^TB = (13) B^TB )\nworst-case robust approximation:      minimize mathrmsup_-1leq uleq 1 (A+tB)x - b_2 (reduces to minimizing max(A-B)x - b_2 (A+B)x - b_2 ).\n\nusing Convex, LinearAlgebra, SCS\n\nInput Data\n\nm = 20;\nn = 10;\nA = randn(m, n);\n(U, S, V) = svd(A);\nS = diagm(exp10.(range(-1, stop = 1, length = n)));\nA = U[:, 1:n] * S * V';\n\nB = randn(m, n);\nB = B / norm(B);\n\nb = randn(m, 1);\nx = Variable(n)","category":"section"},{"location":"examples/general_examples/robust_approx_fitting/#Case-1:-nominal-optimal-solution","page":"Robust approximate fitting","title":"Case 1: nominal optimal solution","text":"p = minimize(norm(A * x - b, 2))\nsolve!(p, SCS.Optimizer; silent = true)\n\nx_nom = evaluate(x)","category":"section"},{"location":"examples/general_examples/robust_approx_fitting/#Case-2:-stochastic-robust-approximation","page":"Robust approximate fitting","title":"Case 2: stochastic robust approximation","text":"P = 1 / 3 * B' * B;\np = minimize(square(pos(norm(A * x - b))) + quadform(x, Symmetric(P)))\nsolve!(p, SCS.Optimizer; silent = true)\n\nx_stoch = evaluate(x)","category":"section"},{"location":"examples/general_examples/robust_approx_fitting/#Case-3:-worst-case-robust-approximation","page":"Robust approximate fitting","title":"Case 3: worst-case robust approximation","text":"p = minimize(max(norm((A - B) * x - b), norm((A + B) * x - b)))\nsolve!(p, SCS.Optimizer; silent = true)\n\nx_wc = evaluate(x)","category":"section"},{"location":"examples/general_examples/robust_approx_fitting/#Plots","page":"Robust approximate fitting","title":"Plots","text":"Here we plot the residuals.\n\nparvals = range(-2, stop = 2, length = 100);\n\nerrvals(x) = [norm((A + parvals[k] * B) * x - b) for k in eachindex(parvals)]\nerrvals_ls = errvals(x_nom)\nerrvals_stoch = errvals(x_stoch)\nerrvals_wc = errvals(x_wc)\n\nusing Plots\nplot(parvals, errvals_ls, label = \"Nominal problem\")\nplot!(parvals, errvals_stoch, label = \"Stochastic Robust Approximation\")\nplot!(parvals, errvals_wc, label = \"Worst-Case Robust Approximation\")\nplot!(\n    title = \"Residual r(u) vs a parameter u for three approximate solutions\",\n    xlabel = \"u\",\n    ylabel = \"r(u) = ||A(u)x-b||_2\",\n)\n\n\n\nThis page was generated using Literate.jl.","category":"section"}]
}
