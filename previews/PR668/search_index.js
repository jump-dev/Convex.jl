var documenterSearchIndex = {"docs":
[{"location":"introduction/quick_tutorial/#Quick-Tutorial","page":"Quick Tutorial","title":"Quick Tutorial","text":"","category":"section"},{"location":"introduction/quick_tutorial/","page":"Quick Tutorial","title":"Quick Tutorial","text":"Consider a constrained least squares problem","category":"page"},{"location":"introduction/quick_tutorial/","page":"Quick Tutorial","title":"Quick Tutorial","text":"beginaligned\nbeginarrayll\ntextminimize  Ax - b_2^2 \ntextsubject to  x geq 0\nendarray\nendaligned","category":"page"},{"location":"introduction/quick_tutorial/","page":"Quick Tutorial","title":"Quick Tutorial","text":"with variable xin mathbfR^n, and problem data A in mathbfR^m times n, b in mathbfR^m.","category":"page"},{"location":"introduction/quick_tutorial/","page":"Quick Tutorial","title":"Quick Tutorial","text":"This problem can be solved in Convex.jl as follows:","category":"page"},{"location":"introduction/quick_tutorial/","page":"Quick Tutorial","title":"Quick Tutorial","text":"# Make the Convex.jl module available\nusing Convex, SCS\n\n# Generate random problem data\nm = 4;  n = 5\nA = randn(m, n); b = randn(m)\n\n# Create a (column vector) variable of size n x 1.\nx = Variable(n)\n\n# The problem is to minimize ||Ax - b||^2 subject to x >= 0\n# This can be done by: minimize(objective, constraints)\nproblem = minimize(sumsquares(A * x - b), [x >= 0])\n\n# Solve the problem by calling solve!\nsolve!(problem, SCS.Optimizer; silent_solver = true)\n\n# Check the status of the problem\nproblem.status # :Optimal, :Infeasible, :Unbounded etc.\n\n# Get the optimum value\nproblem.optval","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"EditURL = \"Convex.jl_intro_ISMP2015.jl\"","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Convex-Optimization-in-Julia","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Madeleine-Udell-ISMP-2015","page":"Convex Optimization in Julia","title":"Madeleine Udell | ISMP 2015","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Convex.jl-team","page":"Convex Optimization in Julia","title":"Convex.jl team","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Convex.jl: Madeleine Udell, Karanveer Mohan, David Zeng, Jenny Hong","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Collaborators/Inspiration:","page":"Convex Optimization in Julia","title":"Collaborators/Inspiration:","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"CVX: Michael Grant, Stephen Boyd\nCVXPY: Steven Diamond, Eric Chu, Stephen Boyd\nJuliaOpt: Miles Lubin, Iain Dunning, Joey Huchette","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"initial package installation","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Make the Convex.jl module available","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"using Convex, SparseArrays, LinearAlgebra\nusing SCS # first order splitting conic solver [O'Donoghue et al., 2014]","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Generate random problem data","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"m = 50;\nn = 100;\nA = randn(m, n)\nx♮ = sprand(n, 1, 0.5) # true (sparse nonnegative) parameter vector\nnoise = 0.1 * randn(m)    # gaussian noise\nb = A * x♮ + noise      # noisy linear observations","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Create a (column vector) variable of size n.","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"x = Variable(n)","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"nonnegative elastic net with regularization","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"λ = 1\nμ = 1\nproblem = minimize(\n    square(norm(A * x - b)) + λ * square(norm(x)) + μ * norm(x, 1),\n    x >= 0,\n)","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Solve the problem by calling solve!","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"solve!(problem, SCS.Optimizer; silent_solver = true)\n\nprintln(\"problem status is \", problem.status) # :Optimal, :Infeasible, :Unbounded etc.\nprintln(\"optimal value is \", problem.optval)","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"using Interact, Plots\n# Interact.WebIO.install_jupyter_nbextension() # might be helpful if you see `WebIO` warnings in Jupyter\n@manipulate throttle = 0.1 for λ in 0:0.1:5, μ in 0:0.1:5\n    global A\n    problem = minimize(\n        square(norm(A * x - b)) + λ * square(norm(x)) + μ * norm(x, 1),\n        x >= 0,\n    )\n    solve!(problem, SCS.Optimizer; silent_solver = true)\n    histogram(evaluate(x), xlims = (0, 3.5), label = \"x\")\nend\nnothing # hide","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Quick-convex-prototyping","page":"Convex Optimization in Julia","title":"Quick convex prototyping","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Variables","page":"Convex Optimization in Julia","title":"Variables","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Scalar variable","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"x = Variable()","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"(Column) vector variable","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"y = Variable(4)","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Matrix variable","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Z = Variable(4, 4)","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Expressions","page":"Convex Optimization in Julia","title":"Expressions","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Convex.jl allows you to use a wide variety of functions on variables and on expressions to form new expressions.","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"x + 2x","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"e = y[1] + logdet(Z) + sqrt(x) + minimum(y)","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Examine-the-expression-tree","page":"Convex Optimization in Julia","title":"Examine the expression tree","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"e.children[2]","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Constraints","page":"Convex Optimization in Julia","title":"Constraints","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"A constraint is convex if convex combinations of feasible points are also feasible. Equivalently, feasible sets are convex sets.","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"In other words, convex constraints are of the form","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"convexExpr <= 0\nconcaveExpr >= 0\naffineExpr == 0","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"x <= 0","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"square(x) <= sum(y)","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"M = Z\nfor i in 1:length(y)\n    global M += rand(size(Z)...) * y[i]\nend\nM ⪰ 0","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Problems","page":"Convex Optimization in Julia","title":"Problems","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"x = Variable()\ny = Variable(4)\nobjective = 2 * x + 1 - sqrt(sum(y))\nconstraint = x >= maximum(y)\np = minimize(objective, constraint)","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Solve the problem:","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"solve!(p, SCS.Optimizer; silent_solver = true)\np.status","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"evaluate(x)","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Can evaluate expressions directly:","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"evaluate(objective)","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Pass-to-solver","page":"Convex Optimization in Julia","title":"Pass to solver","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"call a MathProgBase solver suited for your problem class","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"see the list of Convex.jl operations to find which cones you're using\nsee the list of solvers for an up-to-date list of solvers and which cones they support","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"to solve problem using a different solver, just import the solver package and pass the solver to the solve! method:","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"using Mosek\nsolve!(p, Mosek.Optimizer)","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#Warmstart","page":"Convex Optimization in Julia","title":"Warmstart","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Generate random problem data:","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"m = 50;\nn = 100;\nA = randn(m, n)\nx♮ = sprand(n, 1, 0.5) # true (sparse nonnegative) parameter vector\nnoise = 0.1 * randn(m)    # gaussian noise\nb = A * x♮ + noise      # noisy linear observations","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Create a (column vector) variable of size n.","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"x = Variable(n)","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"nonnegative elastic net with regularization","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"λ = 1\nμ = 1\nproblem = minimize(\n    square(norm(A * x - b)) + λ * square(norm(x)) + μ * norm(x, 1),\n    x >= 0,\n)\n@time solve!(problem, SCS.Optimizer; silent_solver = true)\nλ = 1.5\n@time solve!(problem, SCS.Optimizer; silent_solver = true)#, warmstart = true) # FIXME","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/#DCP-examples","page":"Convex Optimization in Julia","title":"DCP examples","text":"","category":"section"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"affine","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"x = Variable(4)\ny = Variable(2)\nsum(x) + y[2]","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"2 * maximum(x) + 4 * sum(y) - sqrt(y[1] + x[1]) - 7 * minimum(x[2:4])","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"not DCP compliant","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"log(x) + square(x)","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Composition fcirc g where f is convex increasing and g is convex","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"square(pos(x))","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Composition fcirc g where f is convex decreasing and g is concave","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"invpos(sqrt(x))","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"Composition fcirc g where f is concave increasing and g is concave","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"sqrt(sqrt(x))","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"","category":"page"},{"location":"examples/supplemental_material/Convex.jl_intro_ISMP2015/","page":"Convex Optimization in Julia","title":"Convex Optimization in Julia","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/general_examples/huber_regression/","page":"Huber regression","title":"Huber regression","text":"EditURL = \"huber_regression.jl\"","category":"page"},{"location":"examples/general_examples/huber_regression/#Huber-regression","page":"Huber regression","title":"Huber regression","text":"","category":"section"},{"location":"examples/general_examples/huber_regression/","page":"Huber regression","title":"Huber regression","text":"This example can be found here: https://web.stanford.edu/~boyd/papers/pdf/cvx_applications.pdf. Here we set big_example = false to only generate a small example which takes less time to run.","category":"page"},{"location":"examples/general_examples/huber_regression/","page":"Huber regression","title":"Huber regression","text":"big_example = false\nif big_example\n    n = 300\n    number_tests = 50\nelse\n    n = 50\n    number_tests = 10\nend","category":"page"},{"location":"examples/general_examples/huber_regression/","page":"Huber regression","title":"Huber regression","text":"Generate data for Huber regression.","category":"page"},{"location":"examples/general_examples/huber_regression/","page":"Huber regression","title":"Huber regression","text":"using Random\nRandom.seed!(1);\nnumber_samples = round(Int, 1.5 * n);\nbeta_true = 5 * randn(n);\nX = randn(n, number_samples);\nY = zeros(number_samples);\nv = randn(number_samples);\nnothing #hide","category":"page"},{"location":"examples/general_examples/huber_regression/","page":"Huber regression","title":"Huber regression","text":"Generate data for different values of p. Solve the resulting problems.","category":"page"},{"location":"examples/general_examples/huber_regression/","page":"Huber regression","title":"Huber regression","text":"using Convex, SCS, Distributions\nlsq_data = zeros(number_tests);\nhuber_data = zeros(number_tests);\nprescient_data = zeros(number_tests);\np_vals = range(0, stop = 0.15, length = number_tests);\nfor i in 1:length(p_vals)\n    p = p_vals[i]\n    # Generate the sign changes.\n    factor = 2 * rand(Binomial(1, 1 - p), number_samples) .- 1\n    Y = factor .* X' * beta_true + v\n\n    # Form and solve a standard regression problem.\n    beta = Variable(n)\n    fit = norm(beta - beta_true) / norm(beta_true)\n    cost = norm(X' * beta - Y)\n    prob = minimize(cost)\n    solve!(prob, SCS.Optimizer; silent_solver = true)\n    lsq_data[i] = evaluate(fit)\n\n    # Form and solve a prescient regression problem,\n    # that is, where the sign changes are known.\n    cost = norm(factor .* (X' * beta) - Y)\n    solve!(minimize(cost), SCS.Optimizer; silent_solver = true)\n    prescient_data[i] = evaluate(fit)\n\n    # Form and solve the Huber regression problem.\n    cost = sum(huber(X' * beta - Y, 1))\n    solve!(minimize(cost), SCS.Optimizer; silent_solver = true)\n    huber_data[i] = evaluate(fit)\nend","category":"page"},{"location":"examples/general_examples/huber_regression/","page":"Huber regression","title":"Huber regression","text":"using Plots\n\nplot(p_vals, huber_data, label = \"Huber\", xlabel = \"p\", ylabel = \"Fit\")\nplot!(p_vals, lsq_data, label = \"Least squares\")\nplot!(p_vals, prescient_data, label = \"Prescient\")","category":"page"},{"location":"examples/general_examples/huber_regression/","page":"Huber regression","title":"Huber regression","text":"# Plot the relative reconstruction error for Huber and prescient regression,\n# zooming in on smaller values of p.\nindices = findall(p_vals .<= 0.08);\nplot(p_vals[indices], huber_data[indices], label = \"Huber\")\nplot!(p_vals[indices], prescient_data[indices], label = \"Prescient\")","category":"page"},{"location":"examples/general_examples/huber_regression/","page":"Huber regression","title":"Huber regression","text":"","category":"page"},{"location":"examples/general_examples/huber_regression/","page":"Huber regression","title":"Huber regression","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/general_examples/dualization/","page":"Dualization","title":"Dualization","text":"EditURL = \"dualization.jl\"","category":"page"},{"location":"examples/general_examples/dualization/#Dualization","page":"Dualization","title":"Dualization","text":"","category":"section"},{"location":"examples/general_examples/dualization/","page":"Dualization","title":"Dualization","text":"Sometimes it can be much faster to solve the dual problem than the primal problem. Some solvers will automatically dualize the problem when heuristics deem it beneficial, and alternative DCP modeling languages like CVX will also automatically dualize the problem in some cases. Convex.jl does not automatically dualize any problem, but it is easy to manually do so with Dualization.jl. Here, we will solve a simple semidefinite program (from issue #492) to show how easy it is to dualize the problem, and that it can potentially provide speed ups.","category":"page"},{"location":"examples/general_examples/dualization/","page":"Dualization","title":"Dualization","text":"First we load our packages:","category":"page"},{"location":"examples/general_examples/dualization/","page":"Dualization","title":"Dualization","text":"using LinearAlgebra\nusing Convex\nusing SCS\nusing Random\nusing Dualization","category":"page"},{"location":"examples/general_examples/dualization/","page":"Dualization","title":"Dualization","text":"Then we setup some test data.","category":"page"},{"location":"examples/general_examples/dualization/","page":"Dualization","title":"Dualization","text":"Random.seed!(2022)\np = 50\nΣ = Symmetric(randn(p, p))\nΣ = Σ * Σ'","category":"page"},{"location":"examples/general_examples/dualization/","page":"Dualization","title":"Dualization","text":"Now we formulate and solve our primal problem:","category":"page"},{"location":"examples/general_examples/dualization/","page":"Dualization","title":"Dualization","text":"d = Variable(p)\nproblem = maximize(sum(d), 0 ≤ d, d ≤ 1, Σ ⪰ Diagonal(d))\n@time solve!(problem, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/dualization/","page":"Dualization","title":"Dualization","text":"To solve the dual problem instead, we simply call dual_optimizer on our optimizer function:","category":"page"},{"location":"examples/general_examples/dualization/","page":"Dualization","title":"Dualization","text":"@time solve!(problem, dual_optimizer(SCS.Optimizer); silent_solver = true)","category":"page"},{"location":"examples/general_examples/dualization/","page":"Dualization","title":"Dualization","text":"","category":"page"},{"location":"examples/general_examples/dualization/","page":"Dualization","title":"Dualization","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"EditURL = \"lasso_regression.jl\"","category":"page"},{"location":"examples/general_examples/lasso_regression/#Lasso,-Ridge-and-Elastic-Net-Regressions","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"","category":"section"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"This notebook presents a simple implementation of Lasso and elastic net regressions.","category":"page"},{"location":"examples/general_examples/lasso_regression/#Load-Packages-and-Extra-Functions","page":"Lasso, Ridge and Elastic Net Regressions","title":"Load Packages and Extra Functions","text":"","category":"section"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"using DelimitedFiles, LinearAlgebra, Statistics, Plots, Convex, SCS","category":"page"},{"location":"examples/general_examples/lasso_regression/#Loading-Data","page":"Lasso, Ridge and Elastic Net Regressions","title":"Loading Data","text":"","category":"section"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"We use the diabetes data from Efron et al, downloaded from https://web.stanford.edu/~hastie/StatLearnSparsity_files/DATA/diabetes.html and then converted from a tab to a comma delimited file.","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"All data series are standardised (see below) to have zero means and unit standard deviation, which improves the numerical stability. (Efron et al do not standardise the scale of the response variable.)","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"x, header =\n    readdlm(joinpath(@__DIR__, \"aux_files/diabetes.csv\"), ',', header = true)\nx = (x .- mean(x, dims = 1)) ./ std(x, dims = 1) # standardise\n(Y, X) = (x[:, end], x[:, 1:end-1]); # to get traditional names\nxNames = header[1:end-1]","category":"page"},{"location":"examples/general_examples/lasso_regression/#Lasso,-Ridge-and-Elastic-Net-Regressions-2","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"","category":"section"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"(a)  The regression is Y = Xb + u, where Y and u are T times 1, X is T times K, and b is the K-vector of regression coefficients.","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"(b) We want to minimize (Y-Xb)(Y-Xb)T + gamma sum b_i + lambda sum b_i^2.","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"(c) We can equally well minimise bQb - 2cb + gamma sum b_i + lambda sum b_i^2, where Q = XXT and c=XYT.","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"(d) Lasso: gamma0lambda=0; Ridge: gamma=0lambda0; elastic net: gamma0lambda0.","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"\"\"\"\n    LassoEN(Y,X,γ,λ)\n\nDo Lasso (set γ>0,λ=0), ridge (set γ=0,λ>0) or elastic net regression (set γ>0,λ>0).\n\n\n# Input\n- `Y::Vector`:     T-vector with the response (dependent) variable\n- `X::VecOrMat`:   TxK matrix of covariates (regressors)\n- `γ::Number`:     penalty on sum(abs.(b))\n- `λ::Number`:     penalty on sum(b.^2)\n\n\"\"\"\nfunction LassoEN(Y, X, γ, λ = 0)\n    (T, K) = (size(X, 1), size(X, 2))\n\n    b_ls = X \\ Y                    #LS estimate of weights, no restrictions\n\n    Q = X'X / T\n    c = X'Y / T                      #c'b = Y'X*b\n\n    b = Variable(K)              #define variables to optimize over\n    L1 = quadform(b, Q)            #b'Q*b\n    L2 = dot(c, b)                 #c'b\n    L3 = norm(b, 1)                #sum(|b|)\n    L4 = sumsquares(b)            #sum(b^2)\n\n    if λ > 0\n        # u'u/T + γ*sum(|b|) + λ*sum(b^2), where u = Y-Xb\n        problem = minimize(L1 - 2 * L2 + γ * L3 + λ * L4)\n    else\n        # u'u/T + γ*sum(|b|) where u = Y-Xb\n        problem = minimize(L1 - 2 * L2 + γ * L3)\n    end\n    solve!(problem, SCS.Optimizer; silent_solver = true)\n    problem.status == Convex.MOI.OPTIMAL ? b_i = vec(evaluate(b)) : b_i = NaN\n\n    return b_i, b_ls\nend","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"The next cell makes a Lasso regression for a single value of γ.","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"K = size(X, 2)\nγ = 0.25\n\n(b, b_ls) = LassoEN(Y, X, γ)\n\nprintln(\"OLS and Lasso coeffs (with γ=$γ)\")\nprintln([[\"\" \"OLS\" \"Lasso\"]; xNames b_ls b])","category":"page"},{"location":"examples/general_examples/lasso_regression/#Redo-the-Lasso-Regression-with-Different-Gamma-Values","page":"Lasso, Ridge and Elastic Net Regressions","title":"Redo the Lasso Regression with Different Gamma Values","text":"","category":"section"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"We now loop over gamma values.","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"Remark: it would be quicker to put this loop inside the LassoEN() function so as to not recreate L1-L4.","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"nγ = 101\nγM = range(0; stop = 1.5, length = nγ)             #different γ values\n\nbLasso = fill(NaN, size(X, 2), nγ)       #results for γM[i] are in bLasso[:,i]\nfor i in 1:nγ\n    sol, _ = LassoEN(Y, X, γM[i])\n    bLasso[:, i] .= sol\nend","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"plot(\n    log10.(γM),\n    bLasso',\n    title = \"Lasso regression coefficients\",\n    xlabel = \"log10(γ)\",\n    label = permutedims(xNames),\n    size = (600, 400),\n)","category":"page"},{"location":"examples/general_examples/lasso_regression/#Ridge-Regression","page":"Lasso, Ridge and Elastic Net Regressions","title":"Ridge Regression","text":"","category":"section"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"We use the same function to do a ridge regression. Alternatively, do b = inv(X'X/T + λ*I)*X'Y/T.","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"nλ = 101\nλM = range(0; stop = 7.5, length = nλ)\n\nbRidge = fill(NaN, size(X, 2), nλ)\nfor i in 1:nλ\n    sol, _ = LassoEN(Y, X, 0, λM[i])\n    bRidge[:, i] .= sol\nend","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"plot(\n    log10.(λM),\n    bRidge',\n    title = \"Ridge regression coefficients\",\n    xlabel = \"log10(λ)\",\n    label = permutedims(xNames),\n    size = (600, 400),\n)","category":"page"},{"location":"examples/general_examples/lasso_regression/#Elastic-Net-Regression","page":"Lasso, Ridge and Elastic Net Regressions","title":"Elastic Net Regression","text":"","category":"section"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"λ = 0.5\nprintln(\"redo the Lasso regression, but with λ=$λ: an elastic net regression\")\n\nbEN = fill(NaN, size(X, 2), nγ)\nfor i in 1:nγ\n    sol, _ = LassoEN(Y, X, γM[i], λ)\n    bEN[:, i] .= sol\nend","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"plot(\n    log10.(γM),\n    bEN',\n    title = \"Elastic Net regression coefficients\",\n    xlabel = \"log10(γ)\",\n    label = permutedims(xNames),\n    size = (600, 400),\n)","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"","category":"page"},{"location":"examples/general_examples/lasso_regression/","page":"Lasso, Ridge and Elastic Net Regressions","title":"Lasso, Ridge and Elastic Net Regressions","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"EditURL = \"optimal_advertising.jl\"","category":"page"},{"location":"examples/general_examples/optimal_advertising/#Optimal-advertising","page":"Optimal advertising","title":"Optimal advertising","text":"","category":"section"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"This example is taken from https://web.stanford.edu/~boyd/papers/pdf/cvx_applications.pdf.","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"Setup:","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"We have m adverts and n time slots\nThe total traffic in time slot t is T_t\nThe number of ad i displayed in period t is D_it geq 0\nWe require sum_i=1^m D_it leq T_t since we cannot show more than T_t ads during time slot t.\nWe require sum_t=1^n D_it geq c_i to fulfill a contract to show advertisement i at least c_i times.","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"Goal: choose D_it.","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"For some empirical P_it with 0 leq P_it leq 1, we obtain C_it = P_itD_it clicks for ad i, which pays us some number R_i  0 up to a budget B_i. The ad revenue for ad i is S_i = min( R_i sum_t C_it B_i ) which is concave in D. We aim to maximize sum_i S_i.","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"using Random\nusing Distributions: LogNormal\nRandom.seed!(1);\n\nm = 5; # number of adverts\nn = 24; # number of time slots\nSCALE = 10000;\nB = rand(LogNormal(8), m) .+ 10000;\nB = round.(B, digits = 3); # Budget\n\nP_ad = rand(m);\nP_time = rand(1, n);\nP = P_ad * P_time;\n\nT = sin.(range(-2 * pi / 2, stop = 2 * pi - 2 * pi / 2, length = n)) * SCALE;\nT .+= -minimum(T) + SCALE; # traffic\nc = rand(m); # contractual minimum\nc *= 0.6 * sum(T) / sum(c);\nc = round.(c, digits = 3);\nR = [rand(LogNormal(minimum(c) / c[i]), 1) for i in 1:m]; # revenue\nnothing #hide","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"# Form and solve the optimal advertising problem.\nusing Convex, SCS;\nD = Variable(m, n);\nSi = [min(R[i] * dot(P[i, :], D[i, :]'), B[i]) for i in 1:m];\nproblem =\n    maximize(sum(Si), [D >= 0, sum(D, dims = 1)' <= T, sum(D, dims = 2) >= c]);\nsolve!(problem, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"Plot traffic.","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"using Plots\nplot(1:length(T), T, xlabel = \"hour\", ylabel = \"Traffic\")","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"Plot P.","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"heatmap(P)","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"Plot optimal D.","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"heatmap(evaluate(D))","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"","category":"page"},{"location":"examples/general_examples/optimal_advertising/","page":"Optimal advertising","title":"Optimal advertising","text":"This page was generated using Literate.jl.","category":"page"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"CurrentModule = Convex","category":"page"},{"location":"changelog/#Release-notes","page":"Release notes","title":"Release notes","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.","category":"page"},{"location":"changelog/#v0.16.0-(unreleased)","page":"Release notes","title":"v0.16.0 (unreleased)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"This release contains a large number of changes, including some breaking changes.","category":"page"},{"location":"changelog/#Breaking","page":"Release notes","title":"Breaking","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"This release involved a substantial rewrite of Convex.jl to integrate better with MathOptInterface. (#504), (#551), (#584), (#588), (#637)\nx + A will error if x is a scalar variable and A is an array. Instead, use x * ones(size(A)) + A.\nThe RelativeEntropyAtom now returns a scalar value instead o elementwise values. This does not affect the result of relative_entropy.\nThe function constant should be used instead of the type Constant (which now refers to exclusively real constants).\nThe constraint a <= b now produces a - b in Nonpositives() instead of b - a in Nonnegatives(). The primal solutions are equivalent, but the dual variable associated with such constraints is now reversed in sign. (Following the convention in MathOptInterface, the dual of a <= b is always negative, regardless of optimization sense.) (#593)\nThe structs LtConstraint, GtConstraint, EqConstraint SOCConstraint, ExpConstraint, GeoMeanEpiConeConstraint, GeoMeanHypoConeConstraint, and SDPConstraint have been replaced by Constraint{S} where S<:MOI.AbstractSet (#590), (#597), (#598), (#599), (#601), (#602), (#604), (#623), (#632), (#648)\nThe set GeomMeanEpiCone has been renamed to GeometricMeanEpiConeSquare and GeomMeanHypoCone has been renamed to GeometricMeanHypoConeSquare (#638)\nSubtle breaking change: scalar row indexing like x[i, :] now produces a column vector instead of a row vector. This better aligns with Julia Base, but it can result in subtle differences, particularly for code like x[i, :] * y[i, :]': this used to be equivalent to the inner product, but it is now the outer product. In Base Julia, this is the outer product, so the previous code may be been silently broken (#624)\nThe syntaxes dot(*), dot(/) and dot(^) have been removed in favor of explicit broadcasting (x .* y, x ./ y, and x .^ y). These were (mild) type piracy. In addition, vecdot(x,y) has been removed. Call dot(vec(x), vec(y)) instead. (#524)\nThe function constraints, used to get constraints associated to an individual variable, has been renamed get_constraints (#527)\nDCP violations now throw a DCPViolationError exception, rather than a warning. Relatedly, Convex.emit_dcp_warnings has been removed (#523)\nThe strict inequalities > and < have been deprecated. They will be removed in the next breaking release. Note that these never enforced strict inequalities, but instead were equivalent to >= and <= respectively (#555)\nThe functions norm_inf, norm_1, and norm_fro have been deprecated. They will be removed in the next breaking release (#567)\nThe syntax x in :PSD to create a semidefinite constraint is deprecated and will be removed in the next breaking release (#578)\nFixed setting a Constant objective function. This is breaking because it now has an objective sense instead of ignoring the objective. (#581)\nquadform now errors when fixed variables are used instead of silently giving incorrect answers if the value of the fixed variable is modified between solves (#586)\nThe Context struct has been refactored and various fields have been changed. The internal details are now considered private. (#645)","category":"page"},{"location":"changelog/#Added","page":"Release notes","title":"Added","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"SDP, SOC, and exponential cone constraints now have dual values populated (#504)\ngeomean supports more than 2 arguments (#504)\nAdded Convex.Optimizer (#511), (#530), (#534)\nAdded write_to_file (#531), (#591)\nAdded entropy_elementwise (#570)\nnorm on AbstractExpr objects now supports matrices (treating them like vectors), matching Base's behavior (#528)\nAdded root_det (#605)\nAdded VcatAtom which is a more efficient implementation of vcat (#607)\nAdded support for SparseArrays.SparseMatrixCSC in Constant. This fixed performance problems with some atoms (#631)\nsolve! now reports the time and memory allocation during compilation from the DCP expression graph to MathOptInterface (#633)\nAdded support for using Problem as an atom (#646)\nshow(::IO, ::Problem) now includes some problem statistics (#650)","category":"page"},{"location":"changelog/#Fixed","page":"Release notes","title":"Fixed","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"sumlargesteigs now enforces that it's argument is hermitian. (#504)\nType piracy of imag and real has been removed. This should not affect use of Convex. (#504)\nFix dot to now correctly complex-conjugates its first argument (#524)\nFixed ambiguities identified by Aqua.jl (#642), (#647)\nAdd tests and fix  a number of bugs in various atoms (#546), (#547), (#550), (#554), (#556), (#558), (#559), (#561), (#562), (#563), (#565), (#566), (#567), (#568), (#608), (#609), (#617), (#626), (#654), (#655)\nFixed performance issues in a number of issues related to scalar indexing (#618), (#619), (#620), (#621), (#625), (#634)\nFixed show for Problem (#649)","category":"page"},{"location":"changelog/#Other","page":"Release notes","title":"Other","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Improved the documentation (#506), (#517), (#529), (#571), (#573), (#574), (#576), (#579), (#587), (#594), (#628), (#652), (#656)\nRefactored the tests into a functional form (#532)\nUpdated Project.toml (#535)\nAdded test/Project.toml (#536)\nRefactored imports to explicitly overload methods (#537)\nTidied and renamed various atoms and files clarity. This should be non-breaking as no public API was changed. (#538), (#539), (#540), (#541), (#543), (#545), (#549), (#553), (#582), (#583)\nRemoved the unused file src/problem_depot/problems/benchmark.jl (#560)\nAdded various tests to improve code coverage (#522), (#572), (#575), (#577), (#580)\nUpdated versions in GitHub actions (#596), (#612), (#629)\nAdded license headers (#606)","category":"page"},{"location":"changelog/#v0.15.4-(October-24,-2023)","page":"Release notes","title":"v0.15.4 (October 24, 2023)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Convex's piracy of hcat and vcat was made less severe, allowing precompilation of Convex.jl on Julia 1.10.","category":"page"},{"location":"changelog/#v0.15.3-(February-11,-2023)","page":"Release notes","title":"v0.15.3 (February 11, 2023)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Add support for LDLFactorizations v0.10 #496.\nReplace randn(m, 1) with randn(m) to be more Julian #498.\nAdd support for indexing expressions with CartesianIndex #500.","category":"page"},{"location":"changelog/#v0.15.2-(August-10,-2022)","page":"Release notes","title":"v0.15.2 (August 10, 2022)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Add support for LDLFactorizations v0.9 #493.\nFix use of deprecated functions from AbstractTrees #494.","category":"page"},{"location":"changelog/#v0.15.1-(March-28,-2022)","page":"Release notes","title":"v0.15.1 (March 28, 2022)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Use OrderedDict internally for reproducible results, issue: #488, fix: #489.","category":"page"},{"location":"changelog/#v0.15.0-(March-2,-2022)","page":"Release notes","title":"v0.15.0 (March 2, 2022)","text":"","category":"section"},{"location":"changelog/#Breaking-changes","page":"Release notes","title":"Breaking changes","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Minimum required version of Julia is now v1.6\nUpdated to MathOptInterface v1.0\nAs a consequence, many previously deprecated solver calls may stop working. For example, instead of () -> SCS.Optimizer(verbose = 0), use MOI.OptimizerWithAttributes(SCS.Optimizer, \"verbose\" => 0).","category":"page"},{"location":"changelog/#v0.14.18-(November-14,-2021)","page":"Release notes","title":"v0.14.18 (November 14, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Fix typo in logisticloss for length-1 expressions which caused errors (reported in #458, fixed in #469).","category":"page"},{"location":"changelog/#v0.14.17-(November-14,-2021)","page":"Release notes","title":"v0.14.17 (November 14, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Updated to become compatible with MathOptInterface v0.10, which enables compatibility with the latest version of many solvers (#467, #468).","category":"page"},{"location":"changelog/#v0.14.16-(September-25,-2021)","page":"Release notes","title":"v0.14.16 (September 25, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Improve numerical stability when evaluating logsumexp (#457). Thanks @JinraeKim!","category":"page"},{"location":"changelog/#v0.14.15-(September-15,-2021)","page":"Release notes","title":"v0.14.15 (September 15, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Use sparse factorization for checking for positive semi-definiteness in quadform when possible (#457). Thanks @mtanneau!\nAdd assume_psd=false argument to skip checking for positive semi-definiteness in quadform (#456).","category":"page"},{"location":"changelog/#v0.14.14-(September-8,-2021)","page":"Release notes","title":"v0.14.14 (September 8, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Increase the tolerance used in checking if a matrix is positive-semi definite in quadform (#453). Thanks @numbermaniac!","category":"page"},{"location":"changelog/#v0.14.13-(July-25,-2021)","page":"Release notes","title":"v0.14.13 (July 25, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"fix quadform for positive semi-definite matrices (fixes a regression introduced in v0.14.11 that required strictly positive semi-definite inputs) #450.","category":"page"},{"location":"changelog/#v0.14.12-(July-19,-2021)","page":"Release notes","title":"v0.14.12 (July 19, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"fix size of result of evaluate on IndexAtoms #448. Thanks @hurak!","category":"page"},{"location":"changelog/#v0.14.11-(July-5,-2021)","page":"Release notes","title":"v0.14.11 (July 5, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"fix quadform in the complex case #444. Thanks @lrnv!","category":"page"},{"location":"changelog/#v0.14.10-(May-20,-2021)","page":"Release notes","title":"v0.14.10 (May 20, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"declare compatibility with BenchmarkTools v1.0 #441","category":"page"},{"location":"changelog/#v0.14.9-(May-18,-2021)","page":"Release notes","title":"v0.14.9 (May 18, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"fix some tests in lp_dual_abs_atom #439. Thanks @moehle!","category":"page"},{"location":"changelog/#v0.14.8-(May-4,-2021)","page":"Release notes","title":"v0.14.8 (May 4, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"a complete port of cvxquad thanks to @dstahlke, yielding new functions quantum_relative_entropy, quantum_entropy, trace_logm, trace_mpower, and lieb_ando, and cones GeomMeanHypoCone, GeomMeanEpiCone, and RelativeEntropyEpiCone (#418). Thanks a ton for the awesome contribution @dstahlke!","category":"page"},{"location":"changelog/#v0.14.7-(April-22,-2021)","page":"Release notes","title":"v0.14.7 (April 22, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"declare compatibility with BenchmarkTools v0.7 #434","category":"page"},{"location":"changelog/#v0.14.6-(March-28,-2021)","page":"Release notes","title":"v0.14.6 (March 28, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Use MOI.instantiate to create the optimizer, which allows users to pass an MOI.OptimizerWithAttributes to configure solver settings #431. Thanks @odow!","category":"page"},{"location":"changelog/#v0.14.5-(March-14,-2021)","page":"Release notes","title":"v0.14.5 (March 14, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"allow sumlargest(x,k), sumsmallest(x,k), and sumlargesteigs(x,k) for k=0 (simply returns Constant(0)). (#429).","category":"page"},{"location":"changelog/#v0.14.4-(March-14,-2021)","page":"Release notes","title":"v0.14.4 (March 14, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"fixed a bug where the values of variables were being converted to Float64 even if the problem was solved in high precision. (#427).","category":"page"},{"location":"changelog/#v0.14.3-(March-10,-2021)","page":"Release notes","title":"v0.14.3 (March 10, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"update compatibility bounds for BenchmarkTools 0.6","category":"page"},{"location":"changelog/#v0.14.2-(February-15,-2021)","page":"Release notes","title":"v0.14.2 (February 15, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"added lasso, ridge, and elastic net regression examples (#420). Thanks to @PaulSoderlind!","category":"page"},{"location":"changelog/#v0.14.1-(January-24,-2021)","page":"Release notes","title":"v0.14.1 (January 24, 2021)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"there was a bug causing conj to act in-place (reported in #416), which has been fixed (#417). This bug appears to have existed since the introduction of conj in Convex.jl v0.5.0.","category":"page"},{"location":"changelog/#v0.14.0-(January-17,-2021)","page":"Release notes","title":"v0.14.0 (January 17, 2021)","text":"","category":"section"},{"location":"changelog/#Breaking-changes-2","page":"Release notes","title":"Breaking changes","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Changes to the sign of atoms:\nThe sign of sumlargesteigs has been changed from Positive() to  NoSign(), to allow non-positive-semidefinite inputs (#409). This has the potential to break code that required that sign to be positive. If you run into this problem, please file an issue so we can figure out a workaround.\nThe sign of eigmin and eigmax has been changed from Positive() to  NoSign() (#413). This is a bugfix because in general eigmin and eigmax do not need to return a positive quantity (for non-positive-semidefinite inputs). Again, this has the potential to break code that required that sign to be positive. If you run into this problem, please file an issue so we can figure out a workaround.\nRemoval of deprecations:\nlambdamin and lambdamax has been deprecated to eigmin and eigmax since Convex v0.13.0. This deprecation has been removed, so your code must be updated to call eigmin or eigmax instead (#412).\nnorm(x, p) where x is a matrix expression has been deprecated to opnorm(x,p) since Convex v0.8.0. This deprecation has been removed, so your code must be updated to call opnorm(x, p) instead (#412). Currently, norm(x,p) for a matrix\nexpression x will error, but in Convex.jl v0.15.0 it will return norm(vec(x), p).\nConvex.clearmemory() has been deprecated and unnecessary since Convex v0.12.5. This deprecation has been removed, so if this function is in your code, just delete it (#412).\nvecnorm(x, p) has been deprecated to norm(vec(x), p) since Convex v0.8.0. This deprecation has been removed, so your code must be updated to call norm(vec(x),p) instead (#412).\nOther changes:\nConvex.DCP_WARNINGS was introduced in Convex v0.13.1 to allow turning off Convex.jl's DCP warnings. This has been removed in favor of the function Convex.emit_dcp_warnings() (Commit 481fa02).","category":"page"},{"location":"changelog/#Other-changes","page":"Release notes","title":"Other changes","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"updated nuclearnorm and sumlargesteigs to allow complex variables, and allow the argument of sumlargesteigs to be non-positive-semi-definite (#409). Thanks to @dstahlke!","category":"page"},{"location":"changelog/#v0.13.8-(December-2,-2020)","page":"Release notes","title":"v0.13.8 (December 2, 2020)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"add unary + for Sign and ComplexSign to allow single-argument hcat and vcat to work (#405). Thanks to @dstahlke!","category":"page"},{"location":"changelog/#v0.13.7-(September-11,-2020)","page":"Release notes","title":"v0.13.7 (September 11, 2020)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"fix #403 by adding the keyword argument silent_solver to solve!.","category":"page"},{"location":"changelog/#v0.13.6-(September-8,-2020)","page":"Release notes","title":"v0.13.6 (September 8, 2020)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"fix #401 by allowing diagm(x).","category":"page"},{"location":"changelog/#v0.13.5-(August-25,-2020)","page":"Release notes","title":"v0.13.5 (August 25, 2020)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"fix #398 by allowing fix!'d variables in quadform.","category":"page"},{"location":"changelog/#v0.13.4-(July-27,-2020)","page":"Release notes","title":"v0.13.4 (July 27, 2020)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"You can now create your own variable types by subtyping AbstractVariable. See the docs for more information. You can also add constraints directly to a variable using add_constraint! (#358).\nFunctions vexity(x::Variable), sign(x::Variable), and evaluate(x::Variable) should now be the preferred way to access properties of a variable; likewise use set_value! to set the initial value of a variable (#358).\nTo create integer or binary constraints, use the VarType enum (for example, Variable(BinVar)). Access or set this via vartype and vartype! (#358).","category":"page"},{"location":"changelog/#v0.13.3-(March-22,-2020)","page":"Release notes","title":"v0.13.3 (March 22, 2020)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Make add_constraint! actually add the constraint to the problem.","category":"page"},{"location":"changelog/#v0.13.2-(March-14,-2020)","page":"Release notes","title":"v0.13.2 (March 14, 2020)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Add Convex.MAXDIGITS. Thanks to @riccardomurri!","category":"page"},{"location":"changelog/#v0.13.1-(March-6,-2020)","page":"Release notes","title":"v0.13.1 (March 6, 2020)","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"Allow disabling DCP warnings (#372)\nRestore export of Constraint (#371)","category":"page"},{"location":"changelog/#v0.13.0-(February-28,-2020)","page":"Release notes","title":"v0.13.0 (February 28, 2020)","text":"","category":"section"},{"location":"changelog/#Major-changes","page":"Release notes","title":"Major changes","text":"","category":"section"},{"location":"changelog/","page":"Release notes","title":"Release notes","text":"The intermediate layer has changed from MathProgBase.jl to MathOptInterface.jl (#330). To solve problems, one should pass a MathOptInterface optimizer constructor, such as SCS.Optimizer, or MOI.OptimizerWithAttributes(SCS.Optimizer, \"verbose\" => 0).\nlambdamin and lambdamax have been deprecated in favor of eigmin and eigmax (#357).\nMany \"internal\" functions and types are no longer exported, such as the atoms, types corresponding to constraints and vexities, etc. (#357).\nevaluate(x::Variable) and evaluate(c::Constant) now return scalars and vectors as appropriate, instead of (1,1)- and (d,1)-matrices (#359). This affects functions which used to return (1,1)-matrices; for example, now evaluate(quadform(...)) yields a scalar.","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"EditURL = \"svm.jl\"","category":"page"},{"location":"examples/general_examples/svm/#Support-vector-machine","page":"Support vector machine","title":"Support vector machine","text":"","category":"section"},{"location":"examples/general_examples/svm/#Support-Vector-Machine-(SVM)","page":"Support vector machine","title":"Support Vector Machine (SVM)","text":"","category":"section"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"We are given two sets of points in bf R^n, x_1 ldots x_N and y_1 ldots y_M, and wish to find a function f(x) = w^T x - b that linearly separates the points, that is, f(x_i) geq 1 for i = 1 ldots N and f(y_i) leq -1 for i = 1 ldots M. That is, the points are separated by two hyperplanes, w^T x - b = 1 and w^T x - b = -1.","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"Perfect linear separation is not always possible, so we seek to minimize the amount that these inequalities are violated. The violation of point x_i is textmax 1 + b - w^T x_i 0, and the violation of point y_i is textmax 1 - b + w^T y_i 0. We tradeoff the error sum_i=1^N textmax 1 + b - w^T x_i 0 + sum_i=1^M textmax 1 - b + w^T y_i 0 with the distance between the two hyperplanes, which we want to be large, via minimizing w^2.","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"We can write this problem as","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"beginarrayll\n    textminimize    w^2 + C * (sum_i=1^N textmax 1 + b - w^T x_i 0 + sum_i=1^M textmax 1 - b + w^T y_i 0)\nendarray","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"where w in bf R^n and b in bf R are our optimization variables.","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"We can solve the problem as follows.","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"using Convex, SCS","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"# Generate data.\nn = 2; # dimensionality of data\nC = 10; # inverse regularization parameter in the objective\nN = 10; # number of positive examples\nM = 10; # number of negative examples\n\nusing Distributions: MvNormal\n# positive data points\npos_data = rand(MvNormal([1.0, 2.0], 1.0), N);\n# negative data points\nneg_data = rand(MvNormal([-1.0, 2.0], 1.0), M);\nnothing #hide","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"function svm(pos_data, neg_data)\n    # Create variables for the separating hyperplane w'*x = b.\n    w = Variable(n)\n    b = Variable()\n    # Form the objective.\n    obj =\n        sumsquares(w) +\n        C * sum(max(1 + b - w' * pos_data, 0)) +\n        C * sum(max(1 - b + w' * neg_data, 0))\n    # Form and solve problem.\n    problem = minimize(obj)\n    solve!(problem, SCS.Optimizer; silent_solver = true)\n    return evaluate(w), evaluate(b)\nend;\nnothing #hide","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"w, b = svm(pos_data, neg_data);\nnothing #hide","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"# Plot our results.\nusing Plots","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"Generate the separating hyperplane","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"line_x = -2:0.1:2;\nline_y = (-w[1] * line_x .+ b) / w[2];\nnothing #hide","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"Plot the positive points, negative points, and separating hyperplane.","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"plot(pos_data[1, :], pos_data[2, :], st = :scatter, label = \"Positive points\")\nplot!(neg_data[1, :], neg_data[2, :], st = :scatter, label = \"Negative points\")\nplot!(line_x, line_y, label = \"Separating hyperplane\")","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"","category":"page"},{"location":"examples/general_examples/svm/","page":"Support vector machine","title":"Support vector machine","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Convex.AbstractVariable\nConvex._value\nConvex.set_value!\nConvex.get_constraints\nConvex.add_constraint!\nConvex.vexity\nConvex.vexity!\nConvex.sign\nConvex.sign!\nConvex.VarType\nConvex.vartype\nConvex.vartype!\nConvex.fix!\nConvex.free!\nConvex.evaluate\nConvex.solve!\nConvex.MAXDEPTH\nConvex.MAXWIDTH\nConvex.MAXDIGITS\nConvex.ProblemDepot.run_tests\nConvex.ProblemDepot.benchmark_suite\nConvex.ProblemDepot.foreach_problem\nConvex.ProblemDepot.PROBLEMS\nConvex.conic_form!\nConvex.new_conic_form!","category":"page"},{"location":"reference/#Convex.AbstractVariable","page":"Reference","title":"Convex.AbstractVariable","text":"abstract type AbstractVariable <: AbstractExpr end\n\nAn AbstractVariable should have head field, and a size field to conform to the AbstractExpr interface, and implement methods (or use the field-access fallbacks) for\n\n_value, set_value!: get or set the numeric value of the variable.   _value should return nothing when no numeric value is set. Note:   evaluate is the user-facing method to access the value of x.\nvexity, vexity!: get or set the vexity of the variable. The   vexity should be AffineVexity() unless the variable has been   fix!'d, in which case it is ConstVexity().\nsign, vartype, and get_constraints: get the Sign, VarType,   numeric type, and a (possibly empty) vector of constraints which are   to be applied to any problem in which the variable is used.\n\nOptionally, also implement sign!, vartype!, and add_constraint! to allow users to modify those values or add a constraint.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Convex._value","page":"Reference","title":"Convex._value","text":"_value(x::AbstractVariable)\n\nRaw access to the current value of x; used internally by Convex.jl.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.set_value!","page":"Reference","title":"Convex.set_value!","text":"set_value!(x::AbstractVariable, v)\n\nSets the current value of x to v.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.get_constraints","page":"Reference","title":"Convex.get_constraints","text":"get_constraints(x::AbstractVariable)\n\nReturns the current constraints carried by x.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.add_constraint!","page":"Reference","title":"Convex.add_constraint!","text":"add_constraint!(x::AbstractVariable, C::Constraint)\n\nAdds an constraint to those carried by x.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.vexity","page":"Reference","title":"Convex.vexity","text":"vexity(x::AbstractVariable)\n\nReturns the current vexity of x.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.vexity!","page":"Reference","title":"Convex.vexity!","text":"vexity!(x::AbstractVariable, v::Vexity)\n\nSets the current vexity of x to v. Should only be called by fix! and free!.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Base.sign","page":"Reference","title":"Base.sign","text":"sign(x)\n\nReturn zero if x==0 and xx otherwise (i.e., ±1 for real x).\n\n\n\n\n\nBase.sign(x::AbstractVariable)\n\nReturns the current sign of x.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.sign!","page":"Reference","title":"Convex.sign!","text":"sign!(x::AbstractVariable, s::Sign)\n\nSets the current sign of x to s.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.VarType","page":"Reference","title":"Convex.VarType","text":"VarType\n\nDescribe the type of a AbstractVariable: either continuous (ContVar), integer-valued (IntVar), or binary (BinVar).\n\n\n\n\n\n","category":"type"},{"location":"reference/#Convex.vartype","page":"Reference","title":"Convex.vartype","text":"vartype(x::AbstractVariable)\n\nReturns the current VarType of x.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.vartype!","page":"Reference","title":"Convex.vartype!","text":"vartype!(x::AbstractVariable, vt::VarType)\n\nSets the current VarType of x to vt.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.fix!","page":"Reference","title":"Convex.fix!","text":"fix!(x::AbstractVariable, v = value(x))\n\nFixes x to v. It is subsequently treated as a constant in future optimization problems. See also free!.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.free!","page":"Reference","title":"Convex.free!","text":"free!(x::AbstractVariable)\n\nFrees a previously fix!'d variable x, to treat it once again as a variable to optimize over.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.evaluate","page":"Reference","title":"Convex.evaluate","text":"evaluate(x::AbstractVariable)\n\nReturns the current value of x if assigned; errors otherwise.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.solve!","page":"Reference","title":"Convex.solve!","text":"solve!(\n    problem::Problem,\n    optimizer_factory;\n    silent_solver = false,\n    warmstart::Bool = true,\n)\n\nSolves the problem, populating problem.optval with the optimal value, as well as the values of the variables (accessed by evaluate) and constraint duals (accessed by cons.dual), where applicable. Returns the input problem.\n\nOptional keyword arguments:\n\nsilent_solver: whether the solver should be silent (and not emit output or logs) during the solution process.\nwarmstart (default: false): whether the solver should start the optimization from a previous optimal value (according to the current primal value of the variables in the problem, which can be set by set_value!.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.MAXDEPTH","page":"Reference","title":"Convex.MAXDEPTH","text":"MAXDEPTH\n\nControls depth of tree printing globally for Convex.jl; defaults to 3. Set via\n\nConvex.MAXDEPTH[] = 5\n\n\n\n\n\n","category":"constant"},{"location":"reference/#Convex.MAXWIDTH","page":"Reference","title":"Convex.MAXWIDTH","text":"MAXWIDTH\n\nControls width of tree printing globally for Convex.jl; defaults to 3. Set via\n\nConvex.MAXWIDTH[] = 10\n\n\n\n\n\n","category":"constant"},{"location":"reference/#Convex.MAXDIGITS","page":"Reference","title":"Convex.MAXDIGITS","text":"MAXDIGITS\n\nWhen priting IDs of variables, only show the initial and final digits if the full ID has more than double the number of digits specified here.  So, with the default setting MAXDIGITS=3, any ID longer than 7 digits would be shortened; for example, ID 14656210999710729289 would be printed as 146…289.\n\nThis setting controls tree printing globally for Convex.jl; defaults to 3.\n\nSet via:\n\nConvex.MAXDIGITS[] = 3\n\n\n\n\n\n","category":"constant"},{"location":"reference/#Convex.ProblemDepot.run_tests","page":"Reference","title":"Convex.ProblemDepot.run_tests","text":"run_tests(\n    handle_problem!::Function;\n    problems::Union{Nothing, Vector{String}, Vector{Regex}} = nothing;\n    exclude::Vector{Regex} = Regex[],\n    T=Float64, atol=1e-3, rtol=0.0,\n)\n\nRun a set of tests. handle_problem! should be a function that takes one argument, a Convex.jl Problem and processes it (e.g. solve! the problem with a specific solver).\n\nUse exclude to exclude a subset of sets; automatically excludes r\"benchmark\". Optionally, pass a second argument problems to only allow certain problems (specified by exact names or regex). The test tolerances specified by atol and rtol. Set T to choose a numeric type for the problem. Currently this is only used for choosing the type parameter of the underlying MathOptInterface model, but not for the actual problem data.\n\nExamples\n\nrun_tests(exclude=[r\"mip\"]) do p\n    solve!(p, SCS.Optimizer; silent_solver=true)\nend\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.ProblemDepot.benchmark_suite","page":"Reference","title":"Convex.ProblemDepot.benchmark_suite","text":"benchmark_suite(\n    handle_problem!::Function,\n    problems::Union{Nothing, Vector{String}, Vector{Regex}} = nothing;\n    exclude::Vector{Regex} = Regex[],\n    test = Val(false),\n    T=Float64, atol=1e-3, rtol=0.0,\n)\n\nCreate a benchmarksuite of benchmarks. `handleproblem!should be a function that takes one argument, a Convex.jlProblemand processes it (e.g.solve!the problem with a specific solver). Pass a second argumentproblems` to specify run benchmarks only with certain problems (specified by exact names or regex).\n\nUse exclude to exclude a subset of benchmarks. Optionally, pass a second argument problems to only allow certain problems (specified by exact names or regex). Set test=true to also check the answers, with tolerances specified by atol and rtol. Set T to choose a numeric type for the problem. Currently this is only used for choosing the type parameter of the underlying MathOptInterface model, but not for the actual problem data.\n\nExamples\n\nbenchmark_suite(exclude=[r\"mip\"]) do p\n    solve!(p, SCS.Optimizer; silent_solver=true)\nend\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.ProblemDepot.foreach_problem","page":"Reference","title":"Convex.ProblemDepot.foreach_problem","text":"foreach_problem(apply::Function, [class::String],\n    problems::Union{Nothing, Vector{String}, Vector{Regex}} = nothing;\n    exclude::Vector{Regex} = Regex[])\n\nProvides a convience method for iterating over problems in PROBLEMS. For each problem in PROBLEMS, apply the function apply, which takes two arguments: the name of the function associated to the problem, and the function associated to the problem itself.\n\nOptionally, pass a second argument class to only iterate over a class of problems (class should satsify class ∈ keys(PROBLEMS)), and pass third argument problems to only allow certain problems (specified by exact names or regex). Use the exclude keyword argument to exclude problems by regex.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.ProblemDepot.PROBLEMS","page":"Reference","title":"Convex.ProblemDepot.PROBLEMS","text":"const PROBLEMS = Dict{String, Dict{String, Function}}()\n\nA \"depot\" of Convex.jl problems, subdivided into categories. Each problem is stored as a function with the signature\n\nf(handle_problem!, ::Val{test}, atol, rtol, ::Type{T}) where {T, test}\n\nwhere handle_problem! specifies what to do with the Problem instance (e.g., solve! it with a chosen solver), an option test to choose whether or not to test the values (assuming it has been solved), tolerances for the tests, and a numeric type in which the problem should be specified (currently, this is not respected and all problems are specified in Float64 precision).\n\nSee also run_tests and benchmark_suite for helpers to use these problems in testing or benchmarking.\n\nExamples\n\njulia> PROBLEMS[\"affine\"][\"affine_diag_atom\"]\naffine_diag_atom (generic function with 1 method)\n\n\n\n\n\n","category":"constant"},{"location":"reference/#Convex.conic_form!","page":"Reference","title":"Convex.conic_form!","text":"conic_form!(context::Context, a::AbstractExpr)\n\nReturn the conic form for a. If it as already been created, it is directly accessed in context[a], otherwise, it is created by calling Convex.new_conic_form! and then cached in context so that the next call with the same expression does not create a duplicate one.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Convex.new_conic_form!","page":"Reference","title":"Convex.new_conic_form!","text":"new_conic_form!(context::Context, a::AbstractExpr)\n\nCreate a new conic form for a and return it, assuming that no conic form for a has already been created, that is !haskey(context, a) as this is already checked in conic_form! which calls this function.\n\n\n\n\n\n","category":"function"},{"location":"examples/general_examples/worst_case_analysis/","page":"Worst case risk analysis","title":"Worst case risk analysis","text":"EditURL = \"worst_case_analysis.jl\"","category":"page"},{"location":"examples/general_examples/worst_case_analysis/#Worst-case-risk-analysis","page":"Worst case risk analysis","title":"Worst case risk analysis","text":"","category":"section"},{"location":"examples/general_examples/worst_case_analysis/","page":"Worst case risk analysis","title":"Worst case risk analysis","text":"Generate data for worst-case risk analysis.","category":"page"},{"location":"examples/general_examples/worst_case_analysis/","page":"Worst case risk analysis","title":"Worst case risk analysis","text":"using Random\nusing LinearAlgebra\nRandom.seed!(2);\nn = 5;\nr = abs.(randn(n, 1)) / 15;\nSigma = 0.9 * rand(n, n) .- 0.15;\nSigma_nom = Sigma' * Sigma;\nSigma_nom .-= (maximum(Sigma_nom) - 0.9)\nSigma_nom .+= (1e-4 - eigmin(Sigma_nom)) * I(n) # ensure positive-definite","category":"page"},{"location":"examples/general_examples/worst_case_analysis/","page":"Worst case risk analysis","title":"Worst case risk analysis","text":"Form and solve portfolio optimization problem. Here we minimize risk while requiring a 0.1 return.","category":"page"},{"location":"examples/general_examples/worst_case_analysis/","page":"Worst case risk analysis","title":"Worst case risk analysis","text":"using Convex, SCS\nw = Variable(n);\nret = dot(r, w);\nrisk = sum(quadform(w, Sigma_nom));\nproblem = minimize(risk, [sum(w) == 1, ret >= 0.1, norm(w, 1) <= 2])\nsolve!(problem, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/worst_case_analysis/","page":"Worst case risk analysis","title":"Worst case risk analysis","text":"wval = vec(evaluate(w))","category":"page"},{"location":"examples/general_examples/worst_case_analysis/","page":"Worst case risk analysis","title":"Worst case risk analysis","text":"Form and solve worst-case risk analysis problem.","category":"page"},{"location":"examples/general_examples/worst_case_analysis/","page":"Worst case risk analysis","title":"Worst case risk analysis","text":"Sigma = Semidefinite(n);\nDelta = Variable(n, n);\nrisk = sum(quadform(wval, Sigma));\nproblem = maximize(\n    risk,\n    [\n        Sigma == Sigma_nom + Delta,\n        diag(Delta) == 0,\n        abs(Delta) <= 0.2,\n        Delta == Delta',\n    ],\n);\nsolve!(problem, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/worst_case_analysis/","page":"Worst case risk analysis","title":"Worst case risk analysis","text":"println(\n    \"standard deviation = \",\n    round(sqrt(wval' * Sigma_nom * wval), sigdigits = 2),\n);\nprintln(\n    \"worst-case standard deviation = \",\n    round(sqrt(evaluate(risk)), sigdigits = 2),\n);\nprintln(\"worst-case Delta = \");\nprintln(round.(evaluate(Delta), sigdigits = 2));\nnothing #hide","category":"page"},{"location":"examples/general_examples/worst_case_analysis/","page":"Worst case risk analysis","title":"Worst case risk analysis","text":"","category":"page"},{"location":"examples/general_examples/worst_case_analysis/","page":"Worst case risk analysis","title":"Worst case risk analysis","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/general_examples/max_entropy/","page":"Entropy Maximization","title":"Entropy Maximization","text":"EditURL = \"max_entropy.jl\"","category":"page"},{"location":"examples/general_examples/max_entropy/#Entropy-Maximization","page":"Entropy Maximization","title":"Entropy Maximization","text":"","category":"section"},{"location":"examples/general_examples/max_entropy/","page":"Entropy Maximization","title":"Entropy Maximization","text":"Here is a constrained entropy maximization problem:","category":"page"},{"location":"examples/general_examples/max_entropy/","page":"Entropy Maximization","title":"Entropy Maximization","text":"beginarrayll\n    textmaximize    -sum_i=1^n x_i log x_i \n    textsubject to  mathbf1 x = 1 \n                   Ax leq b\nendarray","category":"page"},{"location":"examples/general_examples/max_entropy/","page":"Entropy Maximization","title":"Entropy Maximization","text":"where x in mathbfR^n is our optimization variable and A in mathbfR^m times n b in mathbfR^m.","category":"page"},{"location":"examples/general_examples/max_entropy/","page":"Entropy Maximization","title":"Entropy Maximization","text":"To solve this, we can simply use the entropy operation Convex.jl provides.","category":"page"},{"location":"examples/general_examples/max_entropy/","page":"Entropy Maximization","title":"Entropy Maximization","text":"using Convex, SCS\n\nn = 25;\nm = 15;\nA = randn(m, n);\nb = rand(m, 1);\n\nx = Variable(n);\nproblem = maximize(entropy(x), sum(x) == 1, A * x <= b)\nsolve!(problem, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/max_entropy/","page":"Entropy Maximization","title":"Entropy Maximization","text":"evaluate(x)","category":"page"},{"location":"examples/general_examples/max_entropy/","page":"Entropy Maximization","title":"Entropy Maximization","text":"","category":"page"},{"location":"examples/general_examples/max_entropy/","page":"Entropy Maximization","title":"Entropy Maximization","text":"This page was generated using Literate.jl.","category":"page"},{"location":"introduction/faq/#FAQ","page":"FAQ","title":"FAQ","text":"","category":"section"},{"location":"introduction/faq/#Where-can-I-get-help?","page":"FAQ","title":"Where can I get help?","text":"","category":"section"},{"location":"introduction/faq/","page":"FAQ","title":"FAQ","text":"For usage questions, please contact us via the Julia Discourse. If you're running into bugs or have feature requests, please use the GitHub Issue Tracker.","category":"page"},{"location":"introduction/faq/#How-does-Convex.jl-differ-from-JuMP?","page":"FAQ","title":"How does Convex.jl differ from JuMP?","text":"","category":"section"},{"location":"introduction/faq/","page":"FAQ","title":"FAQ","text":"Convex.jl and JuMP are both modelling languages for mathematical programming embedded in Julia, and both interface with solvers via MathOptInterface, so many of the same solvers are available in both.","category":"page"},{"location":"introduction/faq/","page":"FAQ","title":"FAQ","text":"Convex.jl converts problems to a standard conic form. This approach requires (and certifies) that the problem is convex and DCP compliant, and guarantees global optimality of the resulting solution (if the solver succeeds. For some models, the solver may experience numerical difficulty).","category":"page"},{"location":"introduction/faq/","page":"FAQ","title":"FAQ","text":"JuMP allows nonlinear programming through an interface that learns about functions via their derivatives. This approach is more flexible (for example, you can optimize non-convex functions), but can't guarantee global optimality if your function is not convex, or warn you if you've entered a non-convex formulation.","category":"page"},{"location":"introduction/faq/","page":"FAQ","title":"FAQ","text":"For linear programming, the difference is more stylistic. JuMP's syntax is scalar-based and similar to AMPL and GAMS making it easy and fast to create constraints by indexing and summation (like sum(x[i] for i in 1:numLocation)).","category":"page"},{"location":"introduction/faq/","page":"FAQ","title":"FAQ","text":"Convex.jl allows (and prioritizes) linear algebraic and functional constructions (like max(x,y) <= A*z); indexing and summation are also supported in Convex.jl, but are somewhat slower than in JuMP.","category":"page"},{"location":"introduction/faq/","page":"FAQ","title":"FAQ","text":"JuMP also lets you efficiently solve a sequence of problems when new constraints are added or when coefficients are modified, whereas Convex.jl parses the problem again whenever the [solve!]{.title-ref} method is called.","category":"page"},{"location":"introduction/faq/#Where-can-I-learn-more-about-Convex-Optimization?","page":"FAQ","title":"Where can I learn more about Convex Optimization?","text":"","category":"section"},{"location":"introduction/faq/","page":"FAQ","title":"FAQ","text":"See the freely available book Convex Optimization by Boyd and Vandenberghe for general background on convex optimization. For help understanding the rules of Disciplined Convex Programming, we recommend this DCP tutorial website.","category":"page"},{"location":"introduction/faq/#Are-there-similar-packages-available-in-other-languages?","page":"FAQ","title":"Are there similar packages available in other languages?","text":"","category":"section"},{"location":"introduction/faq/","page":"FAQ","title":"FAQ","text":"You might use CVXPY in Python, or CVX in Matlab.","category":"page"},{"location":"introduction/faq/#How-does-Convex.jl-work?","page":"FAQ","title":"How does Convex.jl work?","text":"","category":"section"},{"location":"introduction/faq/","page":"FAQ","title":"FAQ","text":"For a detailed discussion of how Convex.jl works, see our paper.","category":"page"},{"location":"introduction/faq/#How-do-I-cite-this-package?","page":"FAQ","title":"How do I cite this package?","text":"","category":"section"},{"location":"introduction/faq/","page":"FAQ","title":"FAQ","text":"If you use Convex.jl for published work, we encourage you to cite the software using the following BibTeX citation: :","category":"page"},{"location":"introduction/faq/","page":"FAQ","title":"FAQ","text":"@article{convexjl,\n     title = {Convex Optimization in {J}ulia},\n     author ={Udell, Madeleine and Mohan, Karanveer and Zeng, David and Hong, Jenny and Diamond, Steven and Boyd, Stephen},\n     year = {2014},\n     journal = {SC14 Workshop on High Performance Technical Computing in Dynamic Languages},\n     archivePrefix = \"arXiv\",\n     eprint = {1410.4821},\n     primaryClass = \"math-oc\",\n    }","category":"page"},{"location":"developer/problem_depot/#Problem-Depot","page":"Problem Depot","title":"Problem Depot","text":"","category":"section"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"Convex.jl has a submodule, ProblemDepot which holds a collection of convex optimization problems. The problems are used by Convex itself to test and benchmark its code, but can also be used by solvers to test and benchmark their code. These tests have been used with many solvers at ConvexTests.jl.","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"ProblemDepot has two main methods for accessing these problems: Convex.ProblemDepot.run_tests and Convex.ProblemDepot.benchmark_suite.","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"For example, to test the solver SCS on all the problems of the depot except the mixed-integer problems (which it cannot handle), run","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"using Convex, SCS, Test\nconst MOI = Convex.MOI\n@testset \"SCS\" begin\n    Convex.ProblemDepot.run_tests(; exclude=[r\"mip\"]) do p\n        solve!(p, MOI.OptimizerWithAttributes(SCS.Optimizer, \"verbose\" => 0, \"eps_abs\" => 1e-6))\n    end\nend","category":"page"},{"location":"developer/problem_depot/#How-to-write-a-ProblemDepot-problem","page":"Problem Depot","title":"How to write a ProblemDepot problem","text":"","category":"section"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"The problems are organized into folders in src/problem_depot/problems. Each is written as a function, annotated by @add_problem, and a name, which is used to group the problems. For example, here is a simple problem:","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"@add_problem affine function affine_negate_atom(handle_problem!, ::Val{test}, atol, rtol, ::Type{T}) where {T, test}\n    x = Variable()\n    p = minimize(-x, [x <= 0])\n    if test\n        @test vexity(p) == AffineVexity()\n    end\n    handle_problem!(p)\n    if test\n        @test p.optval ≈ 0 atol=atol rtol=rtol\n        @test evaluate(-x) ≈ 0 atol=atol rtol=rtol\n    end\nend","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"The @add_problem call adds the problem to the registry of problems in Convex.ProblemDepot.PROBLEMS, which in turn is used by Convex.ProblemDepot.run_tests and Convex.ProblemDepot.benchmark_suite. Next, affine is the grouping of the problem; this problem came from one of the affine tests, and in particular is testing the negation atom. Next is the function signature:","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"function affine_negate_atom(handle_problem!, ::Val{test}, atol, rtol, ::Type{T}) where {T, test}","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"this should be the same for every problem, except for the name, which is a description of the problem. It should include what kind of atoms it uses (affine in this case), so that certain kinds of atoms can be ruled out by the exclude keyword to Convex.ProblemDepot.run_tests and Convex.ProblemDepot.benchmark_suite; for example, many solvers cannot solve mixed-integer problems, so mip is included in the name of such problems.","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"Then begins the body of the problem. It is setup like any other Convex.jl problem, only handle_problem! is called instead of solve!. This allows particular solvers to be used (via for example, choosing handle_problem! = p -> solve!(p, solver)), or for any other function of the problem. Tests should be included and gated behind if test blocks, so that tests can be skipped for benchmarking, or in the case that the problem is not in fact solved during handle_problem!.","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"The fact that the problems may not be solved during handle_problem! brings with it a small complication: any command that assumes the problem has been solved should be behind an if test check. For example, in some of the problems, real(evaluate(x)) is used, for a variable x; perhaps as","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"x_re = real(evaluate(x))\nif test\n    @test x_re = ...\nend","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"However, if the problem x is used in has not been solved, then evaluate(x) === nothing, and real(nothing) throws an error. So instead, this should be rewritten as","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"if test\n    x_re = real(evaluate(x))\n    @test x_re = ...\nend","category":"page"},{"location":"developer/problem_depot/#Benchmark-only-problems","page":"Problem Depot","title":"Benchmark-only problems","text":"","category":"section"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"To add problems for benchmarking without tests, place problems in src/problem_depot/problems/benchmark, and include benchmark in the name. These problems will be automatically skipped during run_tests calls. For example, to benchmark the time it takes to add an SDP constraint, we have the problem","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"@add_problem constraints_benchmark function sdp_constraint(handle_problem!, args...)\n    p = satisfy()\n    x = Variable(44, 44) # 990 vectorized entries\n    push!(p.constraints, x ⪰ 0)\n    handle_problem!(p)\n    nothing\nend","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"However, this \"problem\" has no tests or interesting content for testing, so we skip it during testing. Note, we use args... in the function signature so that it may be called with the standard function signature","category":"page"},{"location":"developer/problem_depot/","page":"Problem Depot","title":"Problem Depot","text":"f(handle_problem!, ::Val{test}, atol, rtol, ::Type{T}) where {T, test}","category":"page"},{"location":"manual/advanced/#Advanced-Features","page":"Advanced Features","title":"Advanced Features","text":"","category":"section"},{"location":"manual/advanced/#DCP-Errors","page":"Advanced Features","title":"DCP Errors","text":"","category":"section"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"When a problem is solved that involves an expression which is not of DCP form, an error is emitted. For example,","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"using SCS\nx = Variable()\ny = Variable()\np = minimize(log(x) + square(y), x >= 0, y >= 0)\nsolve!(p, SCS.Optimizer)","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"See Extended formulations and the DCP ruleset for more discussion on why these errors occur.","category":"page"},{"location":"manual/advanced/#Dual-Variables","page":"Advanced Features","title":"Dual Variables","text":"","category":"section"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"Convex.jl also returns the optimal dual variables for a problem. These are stored in the dual field associated with each constraint.","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"using Convex, SCS\n\nx = Variable()\nconstraint = x >= 0\np = minimize(x, constraint)\nsolve!(p, SCS.Optimizer)\n\n# Get the dual value for the constraint\np.constraints[1].dual\n# or\nconstraint.dual","category":"page"},{"location":"manual/advanced/#Warmstarting","page":"Advanced Features","title":"Warmstarting","text":"","category":"section"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"If you're solving the same problem many times with different values of a parameter, Convex.jl can initialize many solvers with the solution to the previous problem, which sometimes speeds up the solution time. This is called a warm start.","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"To use this feature, pass the optional argument warmstart=true to the solve! method.","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"using Convex, SCS\nn = 1_000\ny = rand(n)\nx = Variable(n)\nlambda = Variable(Positive())\nfix!(lambda, 100)\nproblem = minimize(sumsquares(y - x) + lambda * sumsquares(x - 10))\n@time solve!(problem, SCS.Optimizer)\n# Now warmstart. If the solver takes advantage of warmstarts, this run will be\n# faster\nfix!(lambda, 105)\n@time solve!(problem, SCS.Optimizer; warmstart = true)","category":"page"},{"location":"manual/advanced/#Fixing-and-freeing-variables","page":"Advanced Features","title":"Fixing and freeing variables","text":"","category":"section"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"Convex.jl allows you to fix a variable x to a value by calling the fix! method. Fixing the variable essentially turns it into a constant. Fixed variables are sometimes also called parameters.","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"fix!(x, v) fixes the variable x to the value v.","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"fix!(x) fixes x to its current value, which might be the value obtained by solving another problem involving the variable x.","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"To allow the variable x to vary again, call free!(x).","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"Fixing and freeing variables can be particularly useful as a tool for performing alternating minimization on nonconvex problems. For example, we can find an approximate solution to a nonnegative matrix factorization problem with alternating minimization as follows. We use warmstarts to speed up the solution.","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"# initialize nonconvex problem\nn, k = 10, 1\nA = rand(n, k) * rand(k, n)\nx = Variable(n, k)\ny = Variable(k, n)\nproblem = minimize(sum_squares(A - x*y), x>=0, y>=0)\n\n# initialize value of y\nset_value!(y, rand(k, n))\n# we'll do 10 iterations of alternating minimization\nfor i=1:10\n    # first solve for x\n    # with y fixed, the problem is convex\n    fix!(y)\n    solve!(problem, SCS.Optimizer, warmstart = i > 1 ? true : false)\n    free!(y)\n\n    # now solve for y with x fixed at the previous solution\n    fix!(x)\n    solve!(problem, SCS.Optimizer, warmstart = true)\n    free!(x)\nend","category":"page"},{"location":"manual/advanced/#Custom-Variable-Types","page":"Advanced Features","title":"Custom Variable Types","text":"","category":"section"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"By making subtypes of Convex.AbstractVariable that conform to the appropriate interface (see the Convex.AbstractVariable docstring for details), one can easily provide custom variable types for specific constructions. These aren't always necessary though; for example, one can define the following function probabilityvector:","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"using Convex\n\nfunction probabilityvector(d::Int)\n    x = Variable(d, Positive())\n    add_constraint!(x, sum(x) == 1)\n    return x\nend","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"and then use, say, p = probabilityvector(3) in any Convex.jl problem. The constraints that the entries of p are non-negative and sum to 1 will be automatically added to any problem p is used in.","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"Custom types are necessary when one wants to dispatch on custom variables, use them as callable types, or provide a different implementation. Continuing with the probability vector example, let's say we often use probability vectors variables in taking expectation values, and we want to use function notation for this. To do so, we define","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"using Convex\n\n# Must be mutable! Otherwise variables with the same size/value would be treated as the same object.\nmutable struct ProbabilityVector <: Convex.AbstractVariable\n    head::Symbol\n    size::Tuple{Int,Int}\n    value::Union{Convex.Value,Nothing}\n    vexity::Convex.Vexity\n    function ProbabilityVector(d)\n        return new(:ProbabilityVector, (d, 1), nothing, Convex.AffineVexity())\n    end\nend\n\nConvex.get_constraints(p::ProbabilityVector) = [ sum(p) == 1 ]\nConvex.sign(::ProbabilityVector) = Convex.Positive()\nConvex.vartype(::ProbabilityVector) = Convex.ContVar\n\n(p::ProbabilityVector)(x) = dot(p, x)","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"Then one can call p = ProbabilityVector(3) to construct a our custom variable which can be used in Convex, which already encodes the appropriate constraints (non-negative and sums to 1), and which can act on constants via p(x). For example,","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"using SCS\np = ProbabilityVector(3)\nx = [1.0, 2.0, 3.0]\nprob = minimize( p(x) )\nsolve!(prob, SCS.Optimizer)\nevaluate(p) # [1.0, 0.0, 0.0]","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"Subtypes of AbstractVariable must have the fields head and size. Then they must also","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"either have a field value, or implement Convex._value and Convex.set_value!\neither have a field vexity, or implement Convex.vexity and Convex.vexity! (though the latter is only necessary if you wish to support Convex.fix! and Convex.free!\nhave a field constraints or implement Convex.get_constraints (optionally, implement Convex.add_constraint! to be able to add constraints to your variable after its creation),\neither have a field sign or implement Convex.sign, and\neither have a field vartype, or implement Convex.vartype (optionally, implement Convex.vartype! to be able to change a variables' vartype after construction.)","category":"page"},{"location":"manual/advanced/#Printing-and-the-tree-structure","page":"Advanced Features","title":"Printing and the tree structure","text":"","category":"section"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"A Convex problem is structured as a tree, with the root being the problem object, with branches to the objective and the set of constraints. The objective is an AbstractExpr which itself is a tree, with each atom being a node and having children which are other atoms, variables, or constants. Convex provides children methods from AbstractTrees.jl so that the tree-traversal functions of that package can be used with Convex.jl problems and structures. This is what allows powers the printing of problems, expressions, and constraints. The depth to which the tree corresponding to a problem, expression, or constraint is printed is controlled by the global variable Convex.MAXDEPTH, which defaults to 3. This can be changed by for example, setting","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"Convex.MAXDEPTH[] = 5","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"Likewise, Convex.MAXWIDTH, which defaults to 15, controls the \"width\" of the printed tree. For example, when printing a problem with 20 constraints, only the first MAXWIDTH of the constraints will be printed. Vertical dots, ⋮, will be printed indicating that some constraints were omitted in the printing.","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"A related setting is Convex.MAXDIGITS, which controls printing the internal IDs of atoms: if the string representation of an ID is longer than double the value of MAXDIGITS, then it is shortened by printing only the first and last MAXDIGITS characters.","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"The AbstractTrees methods can also be used to analyze the structure of a Convex.jl problem. For example,","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"using Convex, AbstractTrees\nx = Variable()\np = maximize( log(x), x >= 1, x <= 3 )\nfor leaf in AbstractTrees.Leaves(p)\n    println(\"Here's a leaf: $(summary(leaf))\")\nend","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"We can also iterate over the problem in various orders. The following descriptions are taken from the AbstractTrees.jl docstrings, which have more information.","category":"page"},{"location":"manual/advanced/#PostOrderDFS","page":"Advanced Features","title":"PostOrderDFS","text":"","category":"section"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"Iterator to visit the nodes of a tree, guaranteeing that children will be visited before their parents.","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"for (i, node) in enumerate(AbstractTrees.PostOrderDFS(p))\n    println(\"Here's node $i via PostOrderDFS: $(summary(node))\")\nend","category":"page"},{"location":"manual/advanced/#PreOrderDFS","page":"Advanced Features","title":"PreOrderDFS","text":"","category":"section"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"Iterator to visit the nodes of a tree, guaranteeing that parents will be visited before their children.","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"for (i, node) in enumerate(AbstractTrees.PreOrderDFS(p))\n    println(\"Here's node $i via PreOrderDFS: $(summary(node))\")\nend","category":"page"},{"location":"manual/advanced/#StatelessBFS","page":"Advanced Features","title":"StatelessBFS","text":"","category":"section"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"Iterator to visit the nodes of a tree, guaranteeing that all nodes of a level will be visited before their children.","category":"page"},{"location":"manual/advanced/","page":"Advanced Features","title":"Advanced Features","text":"for (i, node) in enumerate(AbstractTrees.StatelessBFS(p))\n    println(\"Here's node $i via StatelessBFS: $(summary(node))\")\nend","category":"page"},{"location":"manual/solvers/#Solvers","page":"Solvers","title":"Solvers","text":"","category":"section"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"Convex.jl transforms each problem into an equivalent cone program in order to pass the problem to a specialized solver. Depending on the types of functions used in the problem, the conic constraints may include linear, second-order, exponential, or semidefinite constraints, as well as any binary or integer constraints placed on the variables.","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"By default, Convex.jl does not install any solvers. Many users use the solver SCS, which is able to solve problems with linear, second-order cone constraints (SOCPs), exponential constraints and semidefinite constraints (SDPs). Likewise, COSMO is a pure-Julia solver which can handle every cone that Convex.jl itself supports. Any other solver in JuliaOpt may also be used, so long as it supports the conic constraints used to represent the problem. Many other solvers in the JuliaOpt ecosystem can be used to solve (mixed integer) linear programs (LPs and MILPs). Mosek and Gurobi can be used to solve SOCPs (even with binary or integer constraints), and Mosek can also solve SDPs. For up-to-date information about solver capabilities, please see the table here describing which solvers can solve which kind of problems. See also ConvexTests.jl to see the results of running test problems with Convex.jl for many solvers.","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"Installing these solvers is very simple. Just follow the instructions in the documentation for that solver.","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"To use a specific solver, you can use the following syntax","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"solve!(p, Gurobi.Optimizer)\nsolve!(p, Mosek.Optimizer)\nsolve!(p, GLPK.Optimizer)\nsolve!(p, ECOS.Optimizer)\nsolve!(p, SCS.Optimizer)","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"(Of course, the solver must be installed first.) For example, we can use GLPK to solve a MILP:","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"using GLPK\nsolve!(p, GLPK.Optimizer)","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"Many of the solvers also allow options to be passed using MOI.OptimizerWithAttributes. For example, to set a time limit (in milliseconds) with GLPK, use:","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"using Convex, GLPK\nconst MOI = Convex.MOI\n\nsolve!(\n    p,\n    MOI.OptimizerWithAttributes(GLPK.Optimizer, \"tm_lim\" => 60_000.0)\n)","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"As another example, if we wish to turn off printing for the SCS solver (that is, run in quiet mode), we can do so as follows:","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"using Convex, SCS\nconst MOI = Convex.MOI\n\nopt = MOI.OptimizerWithAttributes(SCS.Optimizer, MOI.Silent() => false)\nsolve!(p, opt)","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"Another option is to use the solver-independent silent_solver keyword argument to solve!:","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"solve!(p, SCS.Optimizer; silent_solver=true)","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"See each solver's documentation for more information on solver-dependent options.","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"To turn off the problem status warning issued by Convex when a solver is not able to solve a problem to optimality, use the keyword argument verbose=false of the solve method:","category":"page"},{"location":"manual/solvers/","page":"Solvers","title":"Solvers","text":"solve!(p, SCS.Optimizer, verbose=false)","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"EditURL = \"portfolio_optimization.jl\"","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/#Portfolio-Optimization","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"","category":"section"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"In this problem, we will find the portfolio allocation that minimizes risk while achieving a given expected return R_texttarget.","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"Suppose that we know the mean returns mu in mathbfR^n and the covariance Sigma in mathbfR^n times n of the n assets. We would like to find a portfolio allocation w in mathbfR^n, sum_i w_i = 1, minimizing the risk of the portfolio, which we measure as the variance w^T Sigma w of the portfolio. The requirement that the portfolio allocation achieve the target expected return can be expressed as w^T mu = R_texttarget. We suppose further that our portfolio allocation must comply with some lower and upper bounds on the allocation, w_textlower leq w leq w_textupper.","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"This problem can be written as","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"beginarrayll\n    textminimize    w^T Sigma w \n    textsubject to  w^T mu = R_texttarget \n                       sum_i w_i = 1 \n                       w_textlower leq w leq w_textupper\nendarray","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"where w in mathbfR^n is our optimization variable.","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"using Convex, SCS\n\n# generate problem data\nμ = [11.5; 9.5; 6] / 100          #expected returns\nΣ = [\n    166 34 58              #covariance matrix\n    34 64 4\n    58 4 100\n] / 100^2\n\nn = length(μ)                   #number of assets\n\nR_target = 0.1\nw_lower = 0\nw_upper = 0.5;\nnothing #hide","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"If you want to try the optimization with more assets, uncomment and run the next cell. It creates a vector or average returns and a variance-covariance matrix that have scales similar to the numbers above.","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"using Random Random.seed!(123)","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"n = 15                                      #number of assets, CHANGE IT?","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"μ = (6 .+ (11.5-6)*rand(n))/100             #mean A = randn(n,n) Σ = (A * A' + diagm(0=>rand(n)))/500;       #covariance matrix","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"w = Variable(n)\nret = dot(w, μ)\nrisk = quadform(w, Σ)\n\np = minimize(risk, ret >= R_target, sum(w) == 1, w_lower <= w, w <= w_upper)\n\nsolve!(p, SCS.Optimizer)","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"Optimal portfolio weights:","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"evaluate(w)","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"sum(evaluate(w))","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization/","page":"Portfolio Optimization","title":"Portfolio Optimization","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/general_examples/DCP_analysis/","page":"DCP analysis","title":"DCP analysis","text":"EditURL = \"DCP_analysis.jl\"","category":"page"},{"location":"examples/general_examples/DCP_analysis/#DCP-analysis","page":"DCP analysis","title":"DCP analysis","text":"","category":"section"},{"location":"examples/general_examples/DCP_analysis/","page":"DCP analysis","title":"DCP analysis","text":"using Convex\nx = Variable();\ny = Variable();\nexpr = quadoverlin(x - y, 1 - max(x, y))","category":"page"},{"location":"examples/general_examples/DCP_analysis/","page":"DCP analysis","title":"DCP analysis","text":"We can see from the printing of the expression that this quadoverlin (qol) atom is convex with positive sign. We can query these programmatically using the vexity and sign functions:","category":"page"},{"location":"examples/general_examples/DCP_analysis/","page":"DCP analysis","title":"DCP analysis","text":"println(\"expression convexity = \", vexity(expr));\nprintln(\"expression sign = \", sign(expr));\nnothing #hide","category":"page"},{"location":"examples/general_examples/DCP_analysis/","page":"DCP analysis","title":"DCP analysis","text":"","category":"page"},{"location":"examples/general_examples/DCP_analysis/","page":"DCP analysis","title":"DCP analysis","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"EditURL = \"control.jl\"","category":"page"},{"location":"examples/general_examples/control/#Control","page":"Control","title":"Control","text":"","category":"section"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"A simple control problem on a system usually involves a variable x(t) that denotes the state of the system over time, and a variable u(t) that denotes the input into the system over time. Linear constraints are used to capture the evolution of the system over time:","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"x(t) = Ax(t - 1) + Bu(t)  textfor  t = 1ldots T","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"where the numerical matrices A and B are called the dynamics and input matrices, respectively.","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"The goal of the control problem is to find a sequence of inputs u(t) that will allow the state x(t) to achieve specified values at certain times. For example, we can specify initial and final states of the system:","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"beginaligned\n  x(0) = x_i \n  x(T) = x_f\nendaligned","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"Additional states between the initial and final states can also be specified. These are known as waypoint constraints. Often, the input and state of the system will have physical meaning, so we often want to find a sequence inputs that also minimizes a least squares objective like the following:","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"sum_t = 0^T Fx(t)^2_2 + sum_t = 1^TGu(t)^2_2","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"where F and G are numerical matrices.","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"We'll now apply the basic format of the control problem to an example of controlling the motion of an object in a fluid over T intervals, each of h seconds. The state of the system at time interval t will be given by the position and the velocity of the object, denoted p(t) and v(t), while the input will be forces applied to the object, denoted by f(t). By the basic laws of physics, the relationship between force, velocity, and position must satisfy:","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"  beginaligned\n    p(t+1) = p(t) + h v(t) \n    v(t+1) = v(t) + h a(t)\n  endaligned","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"Here, a(t) denotes the acceleration at time t, for which we use a(t) = f(t)  m + g - d v(t), where m, d, g are constants for the mass of the object, the drag coefficient of the fluid, and the acceleration from gravity, respectively.","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"Additionally, we have our initial/final position/velocity conditions:","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"  beginaligned\n    p(1) = p_i\n    v(1) = v_i\n    p(T+1) = p_f\n    v(T+1) = 0\n  endaligned","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"One reasonable objective to minimize would be","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"  textobjective = mu sum_t = 1^T+1 (v(t))^2 + sum_t = 1^T (f(t))^2","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"We would like to keep both the forces small to perhaps save fuel, and keep the velocities small for safety concerns. Here mu serves as a parameter to control which part of the objective we deem more important, keeping the velocity small or keeping the force small.","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"The following code builds and solves our control example:","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"using Convex, SCS, Plots\n\n# Some constraints on our motion\n# The object should start from the origin, and end at rest\ninitial_velocity = [-20; 100]\nfinal_position = [100; 100]\n\nT = 100 # The number of timesteps\nh = 0.1 # The time between time intervals\nmass = 1 # Mass of object\ndrag = 0.1 # Drag on object\ng = [0, -9.8] # Gravity on object\n\n# Declare the variables we need\nposition = Variable(2, T)\nvelocity = Variable(2, T)\nforce = Variable(2, T - 1)\n\n# Create a problem instance\nmu = 1\n\n# Add constraints on our variables\nconstraints = Constraint[\n    position[:, i+1] == position[:, i] + h * velocity[:, i] for i in 1:T-1\n]\n\nfor i in 1:T-1\n    acceleration = force[:, i] / mass + g - drag * velocity[:, i]\n    push!(constraints, velocity[:, i+1] == velocity[:, i] + h * acceleration)\nend\n\n# Add position constraints\npush!(constraints, position[:, 1] == 0)\npush!(constraints, position[:, T] == final_position)\n\n# Add velocity constraints\npush!(constraints, velocity[:, 1] == initial_velocity)\npush!(constraints, velocity[:, T] == 0)\n\n# Solve the problem\nproblem = minimize(sumsquares(force), constraints)\nsolve!(problem, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"We can plot the trajectory taken by the object.","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"pos = evaluate(position)\nplot([pos[1, 1]], [pos[2, 1]], st = :scatter, label = \"initial point\")\nplot!([pos[1, T]], [pos[2, T]], st = :scatter, label = \"final point\")\nplot!(pos[1, :], pos[2, :], label = \"trajectory\")","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"We can also see how the magnitude of the force changes over time.","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"plot(vec(sum(evaluate(force) .^ 2, dims = 1)), label = \"force (magnitude)\")","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"","category":"page"},{"location":"examples/general_examples/control/","page":"Control","title":"Control","text":"This page was generated using Literate.jl.","category":"page"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"CurrentModule = Convex","category":"page"},{"location":"release_notes/#Release-notes","page":"Release notes","title":"Release notes","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.","category":"page"},{"location":"release_notes/#[v0.16.0](https://github.com/jump-dev/Convex.jl/releases/tag/v0.16.0)-(unreleased)","page":"Release notes","title":"v0.16.0 (unreleased)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"This release contains a large number of changes, including some breaking changes.","category":"page"},{"location":"release_notes/#Breaking","page":"Release notes","title":"Breaking","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"This release involved a substantial rewrite of Convex.jl to integrate better with MathOptInterface. (#504), (#551), (#584), (#588), (#637)\nx + A will error if x is a scalar variable and A is an array. Instead, use x * ones(size(A)) + A.\nThe RelativeEntropyAtom now returns a scalar value instead o elementwise values. This does not affect the result of relative_entropy.\nThe function constant should be used instead of the type Constant (which now refers to exclusively real constants).\nThe constraint a <= b now produces a - b in Nonpositives() instead of b - a in Nonnegatives(). The primal solutions are equivalent, but the dual variable associated with such constraints is now reversed in sign. (Following the convention in MathOptInterface, the dual of a <= b is always negative, regardless of optimization sense.) (#593)\nThe structs LtConstraint, GtConstraint, EqConstraint SOCConstraint, ExpConstraint, GeoMeanEpiConeConstraint, GeoMeanHypoConeConstraint, and SDPConstraint have been replaced by Constraint{S} where S<:MOI.AbstractSet (#590), (#597), (#598), (#599), (#601), (#602), (#604), (#623), (#632), (#648)\nThe set GeomMeanEpiCone has been renamed to GeometricMeanEpiConeSquare and GeomMeanHypoCone has been renamed to GeometricMeanHypoConeSquare (#638)\nSubtle breaking change: scalar row indexing like x[i, :] now produces a column vector instead of a row vector. This better aligns with Julia Base, but it can result in subtle differences, particularly for code like x[i, :] * y[i, :]': this used to be equivalent to the inner product, but it is now the outer product. In Base Julia, this is the outer product, so the previous code may be been silently broken (#624)\nThe syntaxes dot(*), dot(/) and dot(^) have been removed in favor of explicit broadcasting (x .* y, x ./ y, and x .^ y). These were (mild) type piracy. In addition, vecdot(x,y) has been removed. Call dot(vec(x), vec(y)) instead. (#524)\nThe function constraints, used to get constraints associated to an individual variable, has been renamed get_constraints (#527)\nDCP violations now throw a DCPViolationError exception, rather than a warning. Relatedly, Convex.emit_dcp_warnings has been removed (#523)\nThe strict inequalities > and < have been deprecated. They will be removed in the next breaking release. Note that these never enforced strict inequalities, but instead were equivalent to >= and <= respectively (#555)\nThe functions norm_inf, norm_1, and norm_fro have been deprecated. They will be removed in the next breaking release (#567)\nThe syntax x in :PSD to create a semidefinite constraint is deprecated and will be removed in the next breaking release (#578)\nFixed setting a Constant objective function. This is breaking because it now has an objective sense instead of ignoring the objective. (#581)\nquadform now errors when fixed variables are used instead of silently giving incorrect answers if the value of the fixed variable is modified between solves (#586)\nThe Context struct has been refactored and various fields have been changed. The internal details are now considered private. (#645)","category":"page"},{"location":"release_notes/#Added","page":"Release notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"SDP, SOC, and exponential cone constraints now have dual values populated (#504)\ngeomean supports more than 2 arguments (#504)\nAdded Convex.Optimizer (#511), (#530), (#534)\nAdded write_to_file (#531), (#591)\nAdded entropy_elementwise (#570)\nnorm on AbstractExpr objects now supports matrices (treating them like vectors), matching Base's behavior (#528)\nAdded root_det (#605)\nAdded VcatAtom which is a more efficient implementation of vcat (#607)\nAdded support for SparseArrays.SparseMatrixCSC in Constant. This fixed performance problems with some atoms (#631)\nsolve! now reports the time and memory allocation during compilation from the DCP expression graph to MathOptInterface (#633)\nAdded support for using Problem as an atom (#646)\nshow(::IO, ::Problem) now includes some problem statistics (#650)","category":"page"},{"location":"release_notes/#Fixed","page":"Release notes","title":"Fixed","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"sumlargesteigs now enforces that it's argument is hermitian. (#504)\nType piracy of imag and real has been removed. This should not affect use of Convex. (#504)\nFix dot to now correctly complex-conjugates its first argument (#524)\nFixed ambiguities identified by Aqua.jl (#642), (#647)\nAdd tests and fix  a number of bugs in various atoms (#546), (#547), (#550), (#554), (#556), (#558), (#559), (#561), (#562), (#563), (#565), (#566), (#567), (#568), (#608), (#609), (#617), (#626), (#654), (#655)\nFixed performance issues in a number of issues related to scalar indexing (#618), (#619), (#620), (#621), (#625), (#634)\nFixed show for Problem (#649)","category":"page"},{"location":"release_notes/#Other","page":"Release notes","title":"Other","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Improved the documentation (#506), (#517), (#529), (#571), (#573), (#574), (#576), (#579), (#587), (#594), (#628), (#652), (#656)\nRefactored the tests into a functional form (#532)\nUpdated Project.toml (#535)\nAdded test/Project.toml (#536)\nRefactored imports to explicitly overload methods (#537)\nTidied and renamed various atoms and files clarity. This should be non-breaking as no public API was changed. (#538), (#539), (#540), (#541), (#543), (#545), (#549), (#553), (#582), (#583)\nRemoved the unused file src/problem_depot/problems/benchmark.jl (#560)\nAdded various tests to improve code coverage (#522), (#572), (#575), (#577), (#580)\nUpdated versions in GitHub actions (#596), (#612), (#629)\nAdded license headers (#606)","category":"page"},{"location":"release_notes/#[v0.15.4](https://github.com/jump-dev/Convex.jl/releases/tag/v0.15.4)-(October-24,-2023)","page":"Release notes","title":"v0.15.4 (October 24, 2023)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Convex's piracy of hcat and vcat was made less severe, allowing precompilation of Convex.jl on Julia 1.10.","category":"page"},{"location":"release_notes/#[v0.15.3](https://github.com/jump-dev/Convex.jl/releases/tag/v0.15.3)-(February-11,-2023)","page":"Release notes","title":"v0.15.3 (February 11, 2023)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Add support for LDLFactorizations v0.10 #496.\nReplace randn(m, 1) with randn(m) to be more Julian #498.\nAdd support for indexing expressions with CartesianIndex #500.","category":"page"},{"location":"release_notes/#[v0.15.2](https://github.com/jump-dev/Convex.jl/releases/tag/v0.15.2)-(August-10,-2022)","page":"Release notes","title":"v0.15.2 (August 10, 2022)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Add support for LDLFactorizations v0.9 #493.\nFix use of deprecated functions from AbstractTrees #494.","category":"page"},{"location":"release_notes/#[v0.15.1](https://github.com/jump-dev/Convex.jl/releases/tag/v0.15.1)-(March-28,-2022)","page":"Release notes","title":"v0.15.1 (March 28, 2022)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Use OrderedDict internally for reproducible results, issue: #488, fix: #489.","category":"page"},{"location":"release_notes/#[v0.15.0](https://github.com/jump-dev/Convex.jl/releases/tag/v0.15.0)-(March-2,-2022)","page":"Release notes","title":"v0.15.0 (March 2, 2022)","text":"","category":"section"},{"location":"release_notes/#Breaking-changes","page":"Release notes","title":"Breaking changes","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Minimum required version of Julia is now v1.6\nUpdated to MathOptInterface v1.0\nAs a consequence, many previously deprecated solver calls may stop working. For example, instead of () -> SCS.Optimizer(verbose = 0), use MOI.OptimizerWithAttributes(SCS.Optimizer, \"verbose\" => 0).","category":"page"},{"location":"release_notes/#[v0.14.18](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.18)-(November-14,-2021)","page":"Release notes","title":"v0.14.18 (November 14, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Fix typo in logisticloss for length-1 expressions which caused errors (reported in #458, fixed in #469).","category":"page"},{"location":"release_notes/#[v0.14.17](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.17)-(November-14,-2021)","page":"Release notes","title":"v0.14.17 (November 14, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Updated to become compatible with MathOptInterface v0.10, which enables compatibility with the latest version of many solvers (#467, #468).","category":"page"},{"location":"release_notes/#[v0.14.16](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.16)-(September-25,-2021)","page":"Release notes","title":"v0.14.16 (September 25, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Improve numerical stability when evaluating logsumexp (#457). Thanks @JinraeKim!","category":"page"},{"location":"release_notes/#[v0.14.15](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.15)-(September-15,-2021)","page":"Release notes","title":"v0.14.15 (September 15, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Use sparse factorization for checking for positive semi-definiteness in quadform when possible (#457). Thanks @mtanneau!\nAdd assume_psd=false argument to skip checking for positive semi-definiteness in quadform (#456).","category":"page"},{"location":"release_notes/#[v0.14.14](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.14)-(September-8,-2021)","page":"Release notes","title":"v0.14.14 (September 8, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Increase the tolerance used in checking if a matrix is positive-semi definite in quadform (#453). Thanks @numbermaniac!","category":"page"},{"location":"release_notes/#[v0.14.13](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.13)-(July-25,-2021)","page":"Release notes","title":"v0.14.13 (July 25, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"fix quadform for positive semi-definite matrices (fixes a regression introduced in v0.14.11 that required strictly positive semi-definite inputs) #450.","category":"page"},{"location":"release_notes/#[v0.14.12](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.12)-(July-19,-2021)","page":"Release notes","title":"v0.14.12 (July 19, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"fix size of result of evaluate on IndexAtoms #448. Thanks @hurak!","category":"page"},{"location":"release_notes/#[v0.14.11](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.11)-(July-5,-2021)","page":"Release notes","title":"v0.14.11 (July 5, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"fix quadform in the complex case #444. Thanks @lrnv!","category":"page"},{"location":"release_notes/#[v0.14.10](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.10)-(May-20,-2021)","page":"Release notes","title":"v0.14.10 (May 20, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"declare compatibility with BenchmarkTools v1.0 #441","category":"page"},{"location":"release_notes/#[v0.14.9](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.9)-(May-18,-2021)","page":"Release notes","title":"v0.14.9 (May 18, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"fix some tests in lp_dual_abs_atom #439. Thanks @moehle!","category":"page"},{"location":"release_notes/#[v0.14.8](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.8)-(May-4,-2021)","page":"Release notes","title":"v0.14.8 (May 4, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"a complete port of cvxquad thanks to @dstahlke, yielding new functions quantum_relative_entropy, quantum_entropy, trace_logm, trace_mpower, and lieb_ando, and cones GeomMeanHypoCone, GeomMeanEpiCone, and RelativeEntropyEpiCone (#418). Thanks a ton for the awesome contribution @dstahlke!","category":"page"},{"location":"release_notes/#[v0.14.7](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.7)-(April-22,-2021)","page":"Release notes","title":"v0.14.7 (April 22, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"declare compatibility with BenchmarkTools v0.7 #434","category":"page"},{"location":"release_notes/#[v0.14.6](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.6)-(March-28,-2021)","page":"Release notes","title":"v0.14.6 (March 28, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Use MOI.instantiate to create the optimizer, which allows users to pass an MOI.OptimizerWithAttributes to configure solver settings #431. Thanks @odow!","category":"page"},{"location":"release_notes/#[v0.14.5](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.5)-(March-14,-2021)","page":"Release notes","title":"v0.14.5 (March 14, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"allow sumlargest(x,k), sumsmallest(x,k), and sumlargesteigs(x,k) for k=0 (simply returns Constant(0)). (#429).","category":"page"},{"location":"release_notes/#[v0.14.4](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.4)-(March-14,-2021)","page":"Release notes","title":"v0.14.4 (March 14, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"fixed a bug where the values of variables were being converted to Float64 even if the problem was solved in high precision. (#427).","category":"page"},{"location":"release_notes/#[v0.14.3](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.3)-(March-10,-2021)","page":"Release notes","title":"v0.14.3 (March 10, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"update compatibility bounds for BenchmarkTools 0.6","category":"page"},{"location":"release_notes/#[v0.14.2](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.2)-(February-15,-2021)","page":"Release notes","title":"v0.14.2 (February 15, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"added lasso, ridge, and elastic net regression examples (#420). Thanks to @PaulSoderlind!","category":"page"},{"location":"release_notes/#[v0.14.1](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.1)-(January-24,-2021)","page":"Release notes","title":"v0.14.1 (January 24, 2021)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"there was a bug causing conj to act in-place (reported in #416), which has been fixed (#417). This bug appears to have existed since the introduction of conj in Convex.jl v0.5.0.","category":"page"},{"location":"release_notes/#[v0.14.0](https://github.com/jump-dev/Convex.jl/releases/tag/v0.14.0)-(January-17,-2021)","page":"Release notes","title":"v0.14.0 (January 17, 2021)","text":"","category":"section"},{"location":"release_notes/#Breaking-changes-2","page":"Release notes","title":"Breaking changes","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Changes to the sign of atoms:\nThe sign of sumlargesteigs has been changed from Positive() to  NoSign(), to allow non-positive-semidefinite inputs (#409). This has the potential to break code that required that sign to be positive. If you run into this problem, please file an issue so we can figure out a workaround.\nThe sign of eigmin and eigmax has been changed from Positive() to  NoSign() (#413). This is a bugfix because in general eigmin and eigmax do not need to return a positive quantity (for non-positive-semidefinite inputs). Again, this has the potential to break code that required that sign to be positive. If you run into this problem, please file an issue so we can figure out a workaround.\nRemoval of deprecations:\nlambdamin and lambdamax has been deprecated to eigmin and eigmax since Convex v0.13.0. This deprecation has been removed, so your code must be updated to call eigmin or eigmax instead (#412).\nnorm(x, p) where x is a matrix expression has been deprecated to opnorm(x,p) since Convex v0.8.0. This deprecation has been removed, so your code must be updated to call opnorm(x, p) instead (#412). Currently, norm(x,p) for a matrix\nexpression x will error, but in Convex.jl v0.15.0 it will return norm(vec(x), p).\nConvex.clearmemory() has been deprecated and unnecessary since Convex v0.12.5. This deprecation has been removed, so if this function is in your code, just delete it (#412).\nvecnorm(x, p) has been deprecated to norm(vec(x), p) since Convex v0.8.0. This deprecation has been removed, so your code must be updated to call norm(vec(x),p) instead (#412).\nOther changes:\nConvex.DCP_WARNINGS was introduced in Convex v0.13.1 to allow turning off Convex.jl's DCP warnings. This has been removed in favor of the function Convex.emit_dcp_warnings() (Commit 481fa02).","category":"page"},{"location":"release_notes/#Other-changes","page":"Release notes","title":"Other changes","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"updated nuclearnorm and sumlargesteigs to allow complex variables, and allow the argument of sumlargesteigs to be non-positive-semi-definite (#409). Thanks to @dstahlke!","category":"page"},{"location":"release_notes/#[v0.13.8](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.8)-(December-2,-2020)","page":"Release notes","title":"v0.13.8 (December 2, 2020)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"add unary + for Sign and ComplexSign to allow single-argument hcat and vcat to work (#405). Thanks to @dstahlke!","category":"page"},{"location":"release_notes/#[v0.13.7](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.7)-(September-11,-2020)","page":"Release notes","title":"v0.13.7 (September 11, 2020)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"fix #403 by adding the keyword argument silent_solver to solve!.","category":"page"},{"location":"release_notes/#[v0.13.6](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.6)-(September-8,-2020)","page":"Release notes","title":"v0.13.6 (September 8, 2020)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"fix #401 by allowing diagm(x).","category":"page"},{"location":"release_notes/#[v0.13.5](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.5)-(August-25,-2020)","page":"Release notes","title":"v0.13.5 (August 25, 2020)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"fix #398 by allowing fix!'d variables in quadform.","category":"page"},{"location":"release_notes/#[v0.13.4](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.4)-(July-27,-2020)","page":"Release notes","title":"v0.13.4 (July 27, 2020)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"You can now create your own variable types by subtyping AbstractVariable. See the docs for more information. You can also add constraints directly to a variable using add_constraint! (#358).\nFunctions vexity(x::Variable), sign(x::Variable), and evaluate(x::Variable) should now be the preferred way to access properties of a variable; likewise use set_value! to set the initial value of a variable (#358).\nTo create integer or binary constraints, use the VarType enum (for example, Variable(BinVar)). Access or set this via vartype and vartype! (#358).","category":"page"},{"location":"release_notes/#[v0.13.3](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.3)-(March-22,-2020)","page":"Release notes","title":"v0.13.3 (March 22, 2020)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Make add_constraint! actually add the constraint to the problem.","category":"page"},{"location":"release_notes/#[v0.13.2](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.2)-(March-14,-2020)","page":"Release notes","title":"v0.13.2 (March 14, 2020)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Add Convex.MAXDIGITS. Thanks to @riccardomurri!","category":"page"},{"location":"release_notes/#[v0.13.1](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.1)-(March-6,-2020)","page":"Release notes","title":"v0.13.1 (March 6, 2020)","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"Allow disabling DCP warnings (#372)\nRestore export of Constraint (#371)","category":"page"},{"location":"release_notes/#[v0.13.0](https://github.com/jump-dev/Convex.jl/releases/tag/v0.13.0)-(February-28,-2020)","page":"Release notes","title":"v0.13.0 (February 28, 2020)","text":"","category":"section"},{"location":"release_notes/#Major-changes","page":"Release notes","title":"Major changes","text":"","category":"section"},{"location":"release_notes/","page":"Release notes","title":"Release notes","text":"The intermediate layer has changed from MathProgBase.jl to MathOptInterface.jl (#330). To solve problems, one should pass a MathOptInterface optimizer constructor, such as SCS.Optimizer, or MOI.OptimizerWithAttributes(SCS.Optimizer, \"verbose\" => 0).\nlambdamin and lambdamax have been deprecated in favor of eigmin and eigmax (#357).\nMany \"internal\" functions and types are no longer exported, such as the atoms, types corresponding to constraints and vexities, etc. (#357).\nevaluate(x::Variable) and evaluate(c::Constant) now return scalars and vectors as appropriate, instead of (1,1)- and (d,1)-matrices (#359). This affects functions which used to return (1,1)-matrices; for example, now evaluate(quadform(...)) yields a scalar.","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"EditURL = \"portfolio_optimization2.jl\"","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/#Portfolio-Optimization-Markowitz-Efficient-Frontier","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"","category":"section"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"In this problem, we will find the unconstrained portfolio allocation where we introduce the weighting parameter lambda (0 leq lambda leq 1) and minimize lambda * textrisk - (1-lambda)* textexpected return. By varying the values of lambda, we trace out the efficient frontier.","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"Suppose that we know the mean returns mu in mathbfR^n of each asset and the covariance Sigma in mathbfR^n times n between the assets. Our objective is to find a portfolio allocation that minimizes the risk (which we measure as the variance w^T Sigma w) and maximizes the expected return (w^T mu) of the portfolio of the simultaneously. We require w in mathbfR^n and sum_i w_i = 1.","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"This problem can be written as","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"beginarrayll\n    textminimize    lambda*w^T Sigma w - (1-lambda)*w^T mu \n    textsubject to  sum_i w_i = 1\nendarray","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"where w in mathbfR^n is the vector containing weights allocated to each asset.","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"using Convex, SCS    #We are using SCS solver. Install using Pkg.add(\"SCS\")\n\n# generate problem data\nμ = [11.5; 9.5; 6] / 100          #expected returns\nΣ = [\n    166 34 58              #covariance matrix\n    34 64 4\n    58 4 100\n] / 100^2\n\nn = length(μ)                   #number of assets","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"If you want to try the optimization with more assets, uncomment and run the next cell. It creates a vector or average returns and a variance-covariance matrix that have scales similar to the numbers above.","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"using Random Random.seed!(123)","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"n = 15                                      #number of assets, CHANGE IT?","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"μ = (6 .+ (11.5-6)*rand(n))/100             #mean A = randn(n,n) Σ = (A * A' + diagm(0=>rand(n)))/500;       #covariance matrix","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"First we solve without any bounds on w","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"N = 101\nλ_vals = range(0.01, stop = 0.99, length = N)\n\nw = Variable(n)\nret = dot(w, μ)\nrisk = quadform(w, Σ)\n\nMeanVarA = zeros(N, 2)\nfor i in 1:N\n    λ = λ_vals[i]\n    p = minimize(λ * risk - (1 - λ) * ret, sum(w) == 1)\n    solve!(p, SCS.Optimizer; silent_solver = true)\n    MeanVarA[i, :] = [evaluate(ret), evaluate(risk)]\nend","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"Now we solve with the bounds 0le w_i le 1","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"w_lower = 0                     #bounds on w\nw_upper = 1\n\nMeanVarB = zeros(N, 2)   #repeat, but with 0<w[i]<1\nfor i in 1:N\n    λ = λ_vals[i]\n    p = minimize(\n        λ * risk - (1 - λ) * ret,\n        sum(w) == 1,\n        w_lower <= w,     #w[i] is bounded\n        w <= w_upper,\n    )\n    solve!(p, SCS.Optimizer; silent_solver = true)\n    MeanVarB[i, :] = [evaluate(ret), evaluate(risk)]\nend","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"using Plots\nplot(\n    sqrt.([MeanVarA[:, 2] MeanVarB[:, 2]]),\n    [MeanVarA[:, 1] MeanVarB[:, 1]],\n    xlim = (0, 0.25),\n    ylim = (0, 0.15),\n    title = \"Markowitz Efficient Frontier\",\n    xlabel = \"Standard deviation\",\n    ylabel = \"Expected return\",\n    label = [\"no bounds on w\" \"with 0<w<1\"],\n)\nscatter!(sqrt.(diag(Σ)), μ, color = :red, label = \"assets\")","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"We now instead impose a restriction on  sum_i w_i - 1, allowing for varying degrees of leverage.","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"Lmax = 0.5\n\nMeanVarC = zeros(N, 2)   #repeat, but with restriction on Sum(|w[i]|)\nfor i in 1:N\n    λ = λ_vals[i]\n    p = minimize(\n        λ * risk - (1 - λ) * ret,\n        sum(w) == 1,\n        (norm(w, 1) - 1) <= Lmax,\n    )\n    solve!(p, SCS.Optimizer; silent_solver = true)\n    MeanVarC[i, :] = [evaluate(ret), evaluate(risk)]\nend","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"plot(\n    sqrt.([MeanVarA[:, 2] MeanVarB[:, 2] MeanVarC[:, 2]]),\n    [MeanVarA[:, 1] MeanVarB[:, 1] MeanVarC[:, 1]],\n    xlim = (0, 0.25),\n    ylim = (0, 0.15),\n    title = \"Markowitz Efficient Frontier\",\n    xlabel = \"Standard deviation\",\n    ylabel = \"Expected return\",\n    label = [\"no bounds on w\" \"with 0<w<1\" \"restriction on sum(|w|)\"],\n)\nscatter!(sqrt.(diag(Σ)), μ, color = :red, label = \"assets\")","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"","category":"page"},{"location":"examples/portfolio_optimization/portfolio_optimization2/","page":"Portfolio Optimization - Markowitz Efficient Frontier","title":"Portfolio Optimization - Markowitz Efficient Frontier","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/optimization_with_complex_variables/power_flow_optimization/","page":"Power flow optimization","title":"Power flow optimization","text":"EditURL = \"power_flow_optimization.jl\"","category":"page"},{"location":"examples/optimization_with_complex_variables/power_flow_optimization/#Power-flow-optimization","page":"Power flow optimization","title":"Power flow optimization","text":"","category":"section"},{"location":"examples/optimization_with_complex_variables/power_flow_optimization/","page":"Power flow optimization","title":"Power flow optimization","text":"The data for example is taken from MATPOWER website. MATPOWER is Matlab package for solving power flow and optimal power flow problems.","category":"page"},{"location":"examples/optimization_with_complex_variables/power_flow_optimization/","page":"Power flow optimization","title":"Power flow optimization","text":"using Convex, SCS\nusing Test\nusing MAT   #Pkg.add(\"MAT\")\naux(str) = joinpath(@__DIR__, \"aux_files\", str) # path to auxiliary files\n\nTOL = 1e-2;\ninput = matopen(aux(\"Data.mat\"))\nvarnames = names(input)\nData = read(input, \"inj\", \"Y\");\n\nn = size(Data[2], 1);\nY = Data[2];\ninj = Data[1];\nW = ComplexVariable(n, n);\nobjective = real(sum(diag(W)));\nc1 = Constraint[];\nfor i in 2:n\n    push!(c1, dot(Y[i, :], W[i, :]) == inj[i])\nend\nc2 = isposdef(W)\nc3 = real(W[1, 1]) == 1.06^2;\npush!(c1, c2)\npush!(c1, c3)\np = maximize(objective, c1);\nsolve!(p, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/optimization_with_complex_variables/power_flow_optimization/","page":"Power flow optimization","title":"Power flow optimization","text":"p.optval","category":"page"},{"location":"examples/optimization_with_complex_variables/power_flow_optimization/","page":"Power flow optimization","title":"Power flow optimization","text":"evaluate(objective)","category":"page"},{"location":"examples/optimization_with_complex_variables/power_flow_optimization/","page":"Power flow optimization","title":"Power flow optimization","text":"output = matopen(joinpath(@__DIR__, \"Res.mat\"))\nnames(output)\noutputData = read(output, \"Wres\");\nWres = outputData\nreal_diff = real(evaluate(W)) - real(Wres);\nimag_diff = imag(evaluate(W)) - imag(Wres);\n@test real_diff ≈ zeros(n, n) atol = TOL\n@test imag_diff ≈ zeros(n, n) atol = TOL\n\nreal_diff = real(evaluate(W)) - (real(evaluate(W)))';\nimag_sum = imag(evaluate(W)) + (imag(evaluate(W)))';\n@test real_diff ≈ zeros(n, n) atol = TOL\n@test imag_diff ≈ zeros(n, n) atol = TOL","category":"page"},{"location":"examples/optimization_with_complex_variables/power_flow_optimization/","page":"Power flow optimization","title":"Power flow optimization","text":"","category":"page"},{"location":"examples/optimization_with_complex_variables/power_flow_optimization/","page":"Power flow optimization","title":"Power flow optimization","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"EditURL = \"time_series.jl\"","category":"page"},{"location":"examples/time_series/time_series/#Time-Series-Analysis","page":"Time Series Analysis","title":"Time Series Analysis","text":"","category":"section"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"A time series is a sequence of data points, each associated with a time. In our example, we will work with a time series of daily temperatures in the city of Melbourne, Australia over a period of a few years. Let x be the vector of the time series, and x_i denote the temperature in Melbourne on day i. Here is a picture of the time series:","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"using Plots, Convex, ECOS, DelimitedFiles\nconst MOI = Convex.MOI\n\naux(str) = joinpath(@__DIR__, \"aux_files\", str) # path to auxiliary files\n\ntemps = readdlm(aux(\"melbourne_temps.txt\"), ',')\nn = size(temps, 1)\nplot(\n    1:n,\n    temps[1:n],\n    ylabel = \"Temperature (°C)\",\n    label = \"data\",\n    xlabel = \"Time (days)\",\n    xticks = 0:365:n,\n)","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"We can quickly compute the mean of the time series to be 112. If we were to always guess the mean as the temperature of Melbourne on a given day, the RMS error of our guesswork would be 41. We'll try to lower this RMS error by coming up with better ways to model the temperature than guessing the mean.","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"A simple way to model this time series would be to find a smooth curve that approximates the yearly ups and downs. We can represent this model as a vector s where s_i denotes the temperature on the i-th day. To force this trend to repeat yearly, we simply want","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"s_i = s_i + 365","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"for each applicable i.","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"We also want our model to have two more properties:","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"The first is that the temperature on each day in our model should be relatively close to the actual temperature of that day.\nThe second is that our model needs to be smooth, so the change in temperature from day to day should be relatively small. The following objective would capture both properties:","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"sum_i = 1^n (s_i - x_i)^2 + lambda sum_i = 2^n(s_i - s_i - 1)^2","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"where lambda is the smoothing parameter. The larger lambda is, the smoother our model will be.","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"The following code uses Convex to find and plot the model:","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"yearly = Variable(n)\neq_constraints = [yearly[i] == yearly[i-365] for i in 365+1:n]\n\nsmoothing = 100\nsmooth_objective = sumsquares(yearly[1:n-1] - yearly[2:n])\nproblem = minimize(\n    sumsquares(temps - yearly) + smoothing * smooth_objective,\n    eq_constraints,\n);\nsolve!(\n    problem,\n    MOI.OptimizerWithAttributes(ECOS.Optimizer, \"maxit\" => 200, \"verbose\" => 0),\n)\nresiduals = temps - evaluate(yearly)\n\n# Plot smooth fit\nplot(1:n, temps[1:n], label = \"data\")\nplot!(\n    1:n,\n    evaluate(yearly)[1:n],\n    linewidth = 2,\n    label = \"smooth fit\",\n    ylabel = \"Temperature (°C)\",\n    xticks = 0:365:n,\n    xlabel = \"Time (days)\",\n)","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"We can also plot the residual temperatures, r, defined as r = x - s.","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"# Plot residuals for a few days\nplot(1:100, residuals[1:100], ylabel = \"Residuals\", xlabel = \"Time (days)\")","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"root_mean_square_error = sqrt(sum(x -> x^2, residuals) / length(residuals))","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"Our smooth model has a RMS error of 27, a significant improvement from just guessing the mean, but we can do better.","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"We now make the hypothesis that the residual temperature on a given day is some linear combination of the previous 5 days. Such a model is called autoregressive. We are essentially trying to fit the residuals as a function of other parts of the data itself. We want to find a vector of coefficients a such that","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"textr(i) approx sum_j = 1^5 a_j textr(i - j)","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"This can be done by simply minimizing the following sum of squares objective","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"sum_i = 6^n left(textr(i) - sum_j = 1^5 a_j textr(i - j)right)^2","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"The following Convex code solves this problem and plots our autoregressive model against the actual residual temperatures:","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"# Generate the residuals matrix\nar_len = 5\n\nresiduals_mat = Matrix{Float64}(undef, length(residuals) - ar_len, ar_len)\nfor i in 1:ar_len\n    residuals_mat[:, i] = residuals[ar_len-i+1:n-i]\nend\n\n# Solve autoregressive problem\nar_coef = Variable(ar_len)\nproblem =\n    minimize(sumsquares(residuals_mat * ar_coef - residuals[ar_len+1:end]))\nsolve!(\n    problem,\n    MOI.OptimizerWithAttributes(ECOS.Optimizer, \"maxit\" => 200, \"verbose\" => 0),\n)","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"Plot autoregressive fit of daily fluctuations for a few days:","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"ar_range = 1:145\nday_range = ar_range .+ ar_len\nplot(\n    day_range,\n    residuals[day_range],\n    label = \"fluctuations from smooth fit\",\n    ylabel = \"Temperature difference (°C)\",\n)\nplot!(\n    day_range,\n    residuals_mat[ar_range, :] * evaluate(ar_coef),\n    label = \"autoregressive estimate\",\n    xlabel = \"Time (days)\",\n)","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"Now, we can add our autoregressive model for the residual temperatures to our smooth model to get an better fitting model for the daily temperatures in the city of Melbourne:","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"total_estimate = evaluate(yearly)\ntotal_estimate[ar_len+1:end] += residuals_mat * evaluate(ar_coef)","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"We can plot the final fit of data across the whole time range:","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"plot(1:n, temps, label = \"data\", ylabel = \"Temperature (°C)\")\nplot!(\n    1:n,\n    total_estimate,\n    label = \"estimate\",\n    xticks = 0:365:n,\n    xlabel = \"Time (days)\",\n)","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"The RMS error of this final model is sim 23:","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"root_mean_square_error =\n    sqrt(sum(x -> x^2, total_estimate - temps) / length(temps))","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"","category":"page"},{"location":"examples/time_series/time_series/","page":"Time Series Analysis","title":"Time Series Analysis","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#Convex.jl-Convex-Optimization-in-Julia","page":"Home","title":"Convex.jl - Convex Optimization in Julia","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Convex.jl is a Julia package for Disciplined Convex Programming (DCP).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Convex.jl makes it easy to describe optimization problems in a natural, mathematical syntax, and to solve those problems using a variety of different (commercial and open-source) solvers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Convex.jl can be used to solve:","category":"page"},{"location":"","page":"Home","title":"Home","text":"linear programs\nmixed-integer linear programs and mixed-integer second-order cone programs\nDCP-compliant convex programs including\nsecond-order cone programs (SOCP)\nexponential cone programs\nsemidefinite programs (SDP)","category":"page"},{"location":"#Resources-for-getting-started","page":"Home","title":"Resources for getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"There are a few ways to get started with Convex:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Read the Installation guide\nRead the introductory tutorial Quick Tutorial\nRead the list of Supported Operations\nBrowse some of our examples","category":"page"},{"location":"","page":"Home","title":"Home","text":"tip: Tip\nNeed help? Join the community forum to search for answers to commonly asked questions.Before asking a question, make sure to read the post make it easier to help you, which contains a number of tips on how to ask a good question.","category":"page"},{"location":"#How-the-documentation-is-structured","page":"Home","title":"How the documentation is structured","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Having a high-level overview of how this documentation is structured will help you know where to look for certain things.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Examples contain worked examples of solving problems with Convex. Start here if you are new to Convex, or you have a particular problem class you want to model.\nThe Manual contains short code-snippets that explain how to achieve specific tasks in Convex. Look here if you want to know how to achieve a particular task.\nThe Developer docs section contains information for people contributing to Convex development. Don't worry about this section if you are using Convex to formulate and solve problems as a user.","category":"page"},{"location":"#Extended-formulations-and-the-DCP-ruleset","page":"Home","title":"Extended formulations and the DCP ruleset","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Convex.jl works by transforming the problem (which possibly has nonsmooth, nonlinear constructions like the nuclear norm, the log determinant, and so forth—into) a linear optimization problem subject to conic constraints. This reformulation often involves adding auxiliary variables, and is called an \"extended formulation,\" since the original problem has been extended with additional variables. These formulations rely on the problem being modeled by combining Convex.jl's \"atoms\" or primitives according to certain rules which ensure convexity, called the disciplined convex programming (DCP) ruleset. If these atoms are combined in a way that does not ensure convexity, the extended formulations are often invalid. As a simple example, consider the problem","category":"page"},{"location":"","page":"Home","title":"Home","text":"model = minimize(abs(x), x >= 1, x <= 2)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The optimum occurs at x=1, but let us imagine we want to solve this problem via Convex.jl using a linear programming (LP) solver.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Since abs is a nonlinear function, we need to reformulate the problem to pass it to the LP solver. We do this by introducing an auxiliary variable t and instead solving:","category":"page"},{"location":"","page":"Home","title":"Home","text":"model = minimize(t, x >= 1, x <= 2, t >= x, t >= -x)","category":"page"},{"location":"","page":"Home","title":"Home","text":"That is, we add the constraints t >= x and t >= -x, and replace abs(x) by t. Since we are minimizing over t and the smallest possible t satisfying these constraints is the absolute value of x, we get the right answer. This reformulation worked because we were minimizing abs(x), and that is a valid way to use the primitive abs.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If we were maximizing abs, Convex.jl would error with","category":"page"},{"location":"","page":"Home","title":"Home","text":"Problem not DCP compliant: objective is not DCP","category":"page"},{"location":"","page":"Home","title":"Home","text":"Why? Well, let us consider the same reformulation for a maximization problem. The original problem is:","category":"page"},{"location":"","page":"Home","title":"Home","text":"model = maximize(abs(x), x >= 1, x <= 2)","category":"page"},{"location":"","page":"Home","title":"Home","text":"and the maximum of 2, obtained at x = 2. If we do the same reformulation as above, however, we arrive at the problem:","category":"page"},{"location":"","page":"Home","title":"Home","text":"maximize(t, x >= 1, x <= 2, t >= x, t >= -x)","category":"page"},{"location":"","page":"Home","title":"Home","text":"whose solution is infinity.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In other words, we got the wrong answer by using the reformulation, since the extended formulation was only valid for a minimization problem. Convex.jl always performs these reformulations, but they are only guaranteed to be valid when the DCP ruleset is followed. Therefore, Convex.jl programmatically checks the whether or not these rules were satisfied and errors if they were not.","category":"page"},{"location":"examples/general_examples/chebyshev_center/","page":"Chebyshev center","title":"Chebyshev center","text":"EditURL = \"chebyshev_center.jl\"","category":"page"},{"location":"examples/general_examples/chebyshev_center/#Chebyshev-center","page":"Chebyshev center","title":"Chebyshev center","text":"","category":"section"},{"location":"examples/general_examples/chebyshev_center/","page":"Chebyshev center","title":"Chebyshev center","text":"Boyd & Vandenberghe, \"Convex Optimization\" Joëlle Skaf - 08/16/05","category":"page"},{"location":"examples/general_examples/chebyshev_center/","page":"Chebyshev center","title":"Chebyshev center","text":"Adapted for Convex.jl by Karanveer Mohan and David Zeng - 26/05/14","category":"page"},{"location":"examples/general_examples/chebyshev_center/","page":"Chebyshev center","title":"Chebyshev center","text":"The goal is to find the largest Euclidean ball (that is, its center and radius) that lies in a polyhedron described by affine inequalities in this fashion: P = x  a_i*x leq b_i i=1ldotsm  where x in mathbbR^2.","category":"page"},{"location":"examples/general_examples/chebyshev_center/","page":"Chebyshev center","title":"Chebyshev center","text":"using Convex\nusing LinearAlgebra\nimport SCS","category":"page"},{"location":"examples/general_examples/chebyshev_center/","page":"Chebyshev center","title":"Chebyshev center","text":"Generate the input data","category":"page"},{"location":"examples/general_examples/chebyshev_center/","page":"Chebyshev center","title":"Chebyshev center","text":"a1 = [2; 1];\na2 = [2; -1];\na3 = [-1; 2];\na4 = [-1; -2];\nb = ones(4, 1);\nnothing #hide","category":"page"},{"location":"examples/general_examples/chebyshev_center/","page":"Chebyshev center","title":"Chebyshev center","text":"Create and solve the model","category":"page"},{"location":"examples/general_examples/chebyshev_center/","page":"Chebyshev center","title":"Chebyshev center","text":"r = Variable(1)\nx_c = Variable(2)\nconstraints = [\n    a1' * x_c + r * norm(a1, 2) <= b[1],\n    a2' * x_c + r * norm(a2, 2) <= b[2],\n    a3' * x_c + r * norm(a3, 2) <= b[3],\n    a4' * x_c + r * norm(a4, 2) <= b[4],\n]\np = maximize(r, constraints)\nsolve!(p, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/chebyshev_center/","page":"Chebyshev center","title":"Chebyshev center","text":"Generate the figure","category":"page"},{"location":"examples/general_examples/chebyshev_center/","page":"Chebyshev center","title":"Chebyshev center","text":"x = range(-1.5, stop = 1.5, length = 100);\ntheta = 0:pi/100:2*pi;\nusing Plots\nplot(x, x -> -x * a1[1] / a1[2] + b[1] / a1[2])\nplot!(x, x -> -x * a2[1] / a2[2] + b[2] / a2[2])\nplot!(x, x -> -x * a3[1] / a3[2] + b[3] / a3[2])\nplot!(x, x -> -x * a4[1] / a4[2] + b[4] / a4[2])\nplot!(\n    evaluate(x_c)[1] .+ evaluate(r) * cos.(theta),\n    evaluate(x_c)[2] .+ evaluate(r) * sin.(theta),\n    linewidth = 2,\n)\nplot!(\n    title = \"Largest Euclidean ball lying in a 2D polyhedron\",\n    legend = nothing,\n)","category":"page"},{"location":"examples/general_examples/chebyshev_center/","page":"Chebyshev center","title":"Chebyshev center","text":"","category":"page"},{"location":"examples/general_examples/chebyshev_center/","page":"Chebyshev center","title":"Chebyshev center","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"EditURL = \"povm_simulation.jl\"","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/#POVM-simulation","page":"POVM simulation","title":"POVM simulation","text":"","category":"section"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"This notebook shows how we can check how much depolarizing noise a qubit positive operator-valued measure (POVM) can take before it becomes simulable by projective measurements. The general method is described in arXiv:1609.06139. The question of simulability by projective measurements boils down to an SDP problem. Eq. (8) from the paper defines the noisy POVM that we obtain subjecting a POVM mathbfM to a depolarizing channel Phi_t:","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"leftPhi_tleft(mathbfMright)right_i = t M_i + (1-t)fracmathrmtr(M_i)d mathbb1","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"If this visibility tin01 is one, the POVM mathbfM is simulable.","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"We will use Convex.jl to solve the SDP problem.","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"using Convex, SCS, LinearAlgebra","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"For the qubit case, a four outcome qubit POVM mathbfM inmathcalP(24) is simulable if and only if","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"M_1=N_12^++N_13^++N_14^+","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"M_2=N_12^-+N_23^++N_24^+","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"M_3=N_13^-+N_23^-+N_34^+","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"M_4=N_14^-+N_24^-+N_34^-","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"where Hermitian operators N_ij^pm satisfy N_ij^pmgeq0 and N_ij^++N_ij^-=p_ijmathbb1, where ij , ij=1234 and p_ijgeq0 as well as sum_ijp_ij=1, that is, the p_ij values form a probability vector. This forms an SDP feasibility problem, which we can rephrase as an optimization problem by adding depolarizing noise to the left-hand side of the above equations and maximizing the visibility t:","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"max_tin01 t","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"such that","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"tM_1+(1-t)mathrmtr(M_1)fracmathbb12=N_12^++N_13^++N_14^+","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"tM_2+(1-t)mathrmtr(M_2)fracmathbb12=N_12^-+N_23^++N_24^+","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"tM_3+(1-t)mathrmtr(M_3)fracmathbb12=N_13^-+N_23^-+N_34^+","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"tM_4+(1-t)mathrmtr(M_4)fracmathbb12=N_14^-+N_24^-+N_34^-","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":".","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"We organize these constraints in a function that takes a four-output qubit POVM as its argument:","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"function get_visibility(K)\n    noise = real([tr(K[i]) * I(2) / 2 for i in 1:size(K, 1)])\n    P = [[ComplexVariable(2, 2) for i in 1:2] for j in 1:6]\n    q = Variable(6, Positive())\n    t = Variable(1, Positive())\n    constraints = Constraint[isposdef(P[i][j]) for i in 1:6 for j in 1:2]\n    push!(constraints, sum(q) == 1)\n    push!(constraints, t <= 1)\n    append!(constraints, [P[i][1] + P[i][2] == q[i] * I(2) for i in 1:6])\n    push!(\n        constraints,\n        t * K[1] + (1 - t) * noise[1] == P[1][1] + P[2][1] + P[3][1],\n    )\n    push!(\n        constraints,\n        t * K[2] + (1 - t) * noise[2] == P[1][2] + P[4][1] + P[5][1],\n    )\n    push!(\n        constraints,\n        t * K[3] + (1 - t) * noise[3] == P[2][2] + P[4][2] + P[6][1],\n    )\n    push!(\n        constraints,\n        t * K[4] + (1 - t) * noise[4] == P[3][2] + P[5][2] + P[6][2],\n    )\n    p = maximize(t, constraints)\n    return solve!(p, SCS.Optimizer; silent_solver = true)\nend","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"We check this function using the tetrahedron measurement (see Appendix B in arXiv:quant-ph/0702021). This measurement is non-simulable, so we expect a value below one.","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"function dp(v)\n    return I(2) + v[1] * [0 1; 1 0] + v[2] * [0 -im; im 0] + v[3] * [1 0; 0 -1]\nend\nb = [\n    1 1 1\n    -1 -1 1\n    -1 1 -1\n    1 -1 -1\n] / sqrt(3)\nM = [dp(b[i, :]) for i in 1:size(b, 1)] / 4;\np = get_visibility(M)","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"p.optval","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"This value matches the one we obtained using PICOS.","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"","category":"page"},{"location":"examples/optimization_with_complex_variables/povm_simulation/","page":"POVM simulation","title":"POVM simulation","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/general_examples/trade_off_curves/","page":"Regularized least-squares","title":"Regularized least-squares","text":"EditURL = \"trade_off_curves.jl\"","category":"page"},{"location":"examples/general_examples/trade_off_curves/#Regularized-least-squares","page":"Regularized least-squares","title":"Regularized least-squares","text":"","category":"section"},{"location":"examples/general_examples/trade_off_curves/","page":"Regularized least-squares","title":"Regularized least-squares","text":"Here we solve some constrained least-squares problems with 1-norm regularization, and plot how the solution changes with increasing regularization.","category":"page"},{"location":"examples/general_examples/trade_off_curves/","page":"Regularized least-squares","title":"Regularized least-squares","text":"using Random\nRandom.seed!(1)\nm = 25;\nn = 10;\nA = randn(m, n);\nb = randn(m, 1);\nnothing #hide","category":"page"},{"location":"examples/general_examples/trade_off_curves/","page":"Regularized least-squares","title":"Regularized least-squares","text":"using Convex, SCS, LinearAlgebra\n\ngammas = exp10.(range(-4, stop = 2, length = 100));\n\nx_values = zeros(n, length(gammas));\nx = Variable(n);\nfor i in 1:length(gammas)\n    cost = sumsquares(A * x - b) + gammas[i] * norm(x, 1)\n    problem = minimize(cost, [norm(x, Inf) <= 1])\n    solve!(problem, SCS.Optimizer; silent_solver = true)\n    x_values[:, i] = evaluate(x)\nend","category":"page"},{"location":"examples/general_examples/trade_off_curves/","page":"Regularized least-squares","title":"Regularized least-squares","text":"Plot the regularization path.","category":"page"},{"location":"examples/general_examples/trade_off_curves/","page":"Regularized least-squares","title":"Regularized least-squares","text":"using Plots\nplot(\n    title = \"Entries of x vs lambda\",\n    xaxis = :log,\n    xlabel = \"lambda\",\n    ylabel = \"x\",\n)\nfor i in 1:n\n    plot!(gammas, x_values[i, :], label = \"x$i\")\nend\nplot!()","category":"page"},{"location":"examples/general_examples/trade_off_curves/","page":"Regularized least-squares","title":"Regularized least-squares","text":"","category":"page"},{"location":"examples/general_examples/trade_off_curves/","page":"Regularized least-squares","title":"Regularized least-squares","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/general_examples/svm_l1regularization/","page":"SVM with L^1 regularization","title":"SVM with L^1 regularization","text":"EditURL = \"svm_l1regularization.jl\"","category":"page"},{"location":"examples/general_examples/svm_l1regularization/#SVM-with-L1-regularization","page":"SVM with L^1 regularization","title":"SVM with L^1 regularization","text":"","category":"section"},{"location":"examples/general_examples/svm_l1regularization/","page":"SVM with L^1 regularization","title":"SVM with L^1 regularization","text":"# Generate data for SVM classifier with L1 regularization.\nusing Random\nRandom.seed!(3);\nn = 20;\nm = 1000;\nTEST = m;\nDENSITY = 0.2;\nbeta_true = randn(n, 1);\nidxs = randperm(n)[1:round(Int, (1 - DENSITY) * n)];\nbeta_true[idxs] .= 0\noffset = 0;\nsigma = 45;\nX = 5 * randn(m, n);\nY = sign.(X * beta_true .+ offset .+ sigma * randn(m, 1));\nX_test = 5 * randn(TEST, n);\nnothing #hide","category":"page"},{"location":"examples/general_examples/svm_l1regularization/","page":"SVM with L^1 regularization","title":"SVM with L^1 regularization","text":"# Form SVM with L1 regularization problem.\nusing Convex, SCS, ECOS\n\nbeta = Variable(n);\nv = Variable();\nloss = sum(pos(1 - Y .* (X * beta - v)));\nreg = norm(beta, 1);\n\n# Compute a trade-off curve and record train and test error.\nTRIALS = 100\ntrain_error = zeros(TRIALS);\ntest_error = zeros(TRIALS);\nlambda_vals = exp10.(range(-2, stop = 0, length = TRIALS);)\nbeta_vals = zeros(length(beta), TRIALS);\nfor i in 1:TRIALS\n    lambda = lambda_vals[i]\n    problem = minimize(loss / m + lambda * reg)\n    solve!(problem, SCS.Optimizer; silent_solver = true)\n    train_error[i] =\n        sum(\n            float(\n                sign.(X * beta_true .+ offset) .!=\n                sign.(evaluate(X * beta - v)),\n            ),\n        ) / m\n    test_error[i] =\n        sum(\n            float(\n                sign.(X_test * beta_true .+ offset) .!=\n                sign.(evaluate(X_test * beta - v)),\n            ),\n        ) / TEST\n    beta_vals[:, i] = evaluate(beta)\nend","category":"page"},{"location":"examples/general_examples/svm_l1regularization/","page":"SVM with L^1 regularization","title":"SVM with L^1 regularization","text":"Plot the train and test error over the trade-off curve.","category":"page"},{"location":"examples/general_examples/svm_l1regularization/","page":"SVM with L^1 regularization","title":"SVM with L^1 regularization","text":"using Plots\nplot(lambda_vals, train_error, label = \"Train error\");\nplot!(lambda_vals, test_error, label = \"Test error\");\nplot!(xscale = :log, yscale = :log, ylabel = \"errors\", xlabel = \"lambda\")","category":"page"},{"location":"examples/general_examples/svm_l1regularization/","page":"SVM with L^1 regularization","title":"SVM with L^1 regularization","text":"Plot the regularization path for beta.","category":"page"},{"location":"examples/general_examples/svm_l1regularization/","page":"SVM with L^1 regularization","title":"SVM with L^1 regularization","text":"plot()\nfor i in 1:n\n    plot!(lambda_vals, vec(beta_vals[i, :]), label = \"beta$i\")\nend\nplot!(xscale = :log, ylabel = \"betas\", xlabel = \"lambda\")","category":"page"},{"location":"examples/general_examples/svm_l1regularization/","page":"SVM with L^1 regularization","title":"SVM with L^1 regularization","text":"","category":"page"},{"location":"examples/general_examples/svm_l1regularization/","page":"SVM with L^1 regularization","title":"SVM with L^1 regularization","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"EditURL = \"Fidelity in Quantum Information Theory.jl\"","category":"page"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/#Fidelity-in-quantum-information-theory","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"","category":"section"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"This example is inspired from a lecture of John Watrous in the course on Theory of Quantum Information.","category":"page"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"The Fidelity between two Hermitian semidefinite matrices P and Q is defined as:","category":"page"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"F(P Q) = P^12Q^12_texttr = max_U mathrmtr(P^12U Q^12)","category":"page"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"where the trace norm cdot_texttr is the sum of the singular values, and the maximization goes over the set of all unitary matrices U. This quantity can be expressed as the optimal value of the following complex-valued SDP:","category":"page"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"beginarrayll\n  textmaximize   frac12texttr(Z+Z^dagger) \n  textsubject to \n   leftbeginarrayccPZZ^daggerQendarrayright succeq 0\n   Z in mathbf C^n times n\nendarray","category":"page"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"using Convex, SCS, LinearAlgebra\n\nn = 20\nP = randn(n, n) + im * randn(n, n)\nP = P * P'\nQ = randn(n, n) + im * randn(n, n)\nQ = Q * Q'\nZ = ComplexVariable(n, n)\nobjective = 0.5 * real(tr(Z + Z'))\nconstraint = [P Z; Z' Q] ⪰ 0\nproblem = maximize(objective, constraint)\nsolve!(problem, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"computed_fidelity = evaluate(objective)","category":"page"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"# Verify that computer fidelity is equal to actual fidelity\nP1, P2 = eigen(P)\nsqP = P2 * diagm([p1^0.5 for p1 in P1]) * P2'\nQ1, Q2 = eigen(Q)\nsqQ = Q2 * diagm([q1^0.5 for q1 in Q1]) * Q2'","category":"page"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"actual_fidelity = sum(svdvals(sqP * sqQ))","category":"page"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"We can see that the actual fidelity value is very close the computed fidelity value.","category":"page"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"","category":"page"},{"location":"examples/optimization_with_complex_variables/Fidelity in Quantum Information Theory/","page":"Fidelity in quantum information theory","title":"Fidelity in quantum information theory","text":"This page was generated using Literate.jl.","category":"page"},{"location":"developer/credits/#Credits","page":"Credits","title":"Credits","text":"","category":"section"},{"location":"developer/credits/","page":"Credits","title":"Credits","text":"Convex.jl was created, developed, and maintained by:","category":"page"},{"location":"developer/credits/","page":"Credits","title":"Credits","text":"Jenny Hong\nKaranveer Mohan\nMadeleine Udell\nDavid Zeng","category":"page"},{"location":"developer/credits/","page":"Credits","title":"Credits","text":"Convex.jl is currently developed and maintained by the Julia community; see Contributors for more.","category":"page"},{"location":"developer/credits/","page":"Credits","title":"Credits","text":"The Convex.jl developers also thank:","category":"page"},{"location":"developer/credits/","page":"Credits","title":"Credits","text":"the JuliaOpt team: Iain Dunning, Joey Huchette and Miles Lubin\nStephen Boyd, co-author of the book Convex Optimization\nSteven Diamond, developer of CVXPY and of a DCP tutorial website to teach disciplined convex programming.\nMichael Grant, developer of CVX.\nJohn Duchi and Hongseok Namkoong for  developing the representation of power cones in terms of SOCP constraints  used in this package.","category":"page"},{"location":"examples/mixed_integer/section_allocation/","page":"Section Allocation","title":"Section Allocation","text":"EditURL = \"section_allocation.jl\"","category":"page"},{"location":"examples/mixed_integer/section_allocation/#Section-Allocation","page":"Section Allocation","title":"Section Allocation","text":"","category":"section"},{"location":"examples/mixed_integer/section_allocation/","page":"Section Allocation","title":"Section Allocation","text":"Suppose you have n students in a class who need to be assigned to m discussion sections. Each student needs to be assigned to exactly one section. Each discussion section should have between 6 and 10 students. Suppose an n times m preference matrix P is given, where P_ij gives student i's ranking for section j (1 would mean it is the student's top choice, 10,000 or a large number would mean the student can not attend that section).","category":"page"},{"location":"examples/mixed_integer/section_allocation/","page":"Section Allocation","title":"Section Allocation","text":"The goal will be to get an allocation matrix X, where X_ij = 1 if student i is assigned to section j and 0 otherwise.","category":"page"},{"location":"examples/mixed_integer/section_allocation/","page":"Section Allocation","title":"Section Allocation","text":"using Convex, GLPK\naux(str) = joinpath(@__DIR__, \"aux_files\", str) # path to auxiliary files","category":"page"},{"location":"examples/mixed_integer/section_allocation/","page":"Section Allocation","title":"Section Allocation","text":"Load our preference matrix, P","category":"page"},{"location":"examples/mixed_integer/section_allocation/","page":"Section Allocation","title":"Section Allocation","text":"include(aux(\"data.jl\"))\n\nX = Variable(size(P), BinVar)","category":"page"},{"location":"examples/mixed_integer/section_allocation/","page":"Section Allocation","title":"Section Allocation","text":"We want every student to be assigned to exactly one section. So, every row must have exactly one non-zero entry. In other words, the sum of all the columns for every row is 1. We also want each section to have between 6 and 10 students, so the sum of all the rows for every column should be between these.","category":"page"},{"location":"examples/mixed_integer/section_allocation/","page":"Section Allocation","title":"Section Allocation","text":"constraints =\n    [sum(X, dims = 2) == 1, sum(X, dims = 1) <= 10, sum(X, dims = 1) >= 6];\nnothing #hide","category":"page"},{"location":"examples/mixed_integer/section_allocation/","page":"Section Allocation","title":"Section Allocation","text":"Our objective is simple sum(X .* P), which can be more efficiently represented as vec(X)' * vec(P). Since each entry of X is either 0 or 1, this is basically summing up the rankings of students that were assigned to them. If all students got their first choice, this value will be the number of students since the ranking of the first choice is 1.","category":"page"},{"location":"examples/mixed_integer/section_allocation/","page":"Section Allocation","title":"Section Allocation","text":"p = minimize(vec(X)' * vec(P), constraints)\n\nsolve!(p, GLPK.Optimizer)","category":"page"},{"location":"examples/mixed_integer/section_allocation/","page":"Section Allocation","title":"Section Allocation","text":"","category":"page"},{"location":"examples/mixed_integer/section_allocation/","page":"Section Allocation","title":"Section Allocation","text":"This page was generated using Literate.jl.","category":"page"},{"location":"introduction/installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"introduction/installation/","page":"Installation","title":"Installation","text":"Installing Convex.jl is a one step process. Open up Julia and type:","category":"page"},{"location":"introduction/installation/","page":"Installation","title":"Installation","text":"using Pkg\nPkg.update()\nPkg.add(\"Convex\")","category":"page"},{"location":"introduction/installation/","page":"Installation","title":"Installation","text":"This does not install any solvers. If you don't have a solver installed already, you will want to install a solver such as SCS by running:","category":"page"},{"location":"introduction/installation/","page":"Installation","title":"Installation","text":"Pkg.add(\"SCS\")","category":"page"},{"location":"introduction/installation/","page":"Installation","title":"Installation","text":"To solve certain problems such as mixed integer programming problems you will need to install another solver as well, such as HiGHS.","category":"page"},{"location":"introduction/installation/","page":"Installation","title":"Installation","text":"If you wish to use other solvers, please read the section on Solvers.","category":"page"},{"location":"manual/complex-domain_optimization/#Optimization-with-Complex-Variables","page":"Complex-domain Optimization","title":"Optimization with Complex Variables","text":"","category":"section"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"Convex.jl also supports optimization with complex variables. Below, we present a quick start guide on how to use Convex.jl for optimization with complex variables, and then list the operations supported on complex variables in Convex.jl. In general, any operation available in Convex.jl that is well defined and DCP compliant on complex variables should be available. We list these functions below. organized by the type of cone (linear, second-order, or semidefinite) used to represent that operation.","category":"page"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"Internally, Convex.jl transforms the complex-domain problem to a larger real-domain problem using a bijective mapping. It then solves the real-domain problem and transforms the solution back to the complex domain.","category":"page"},{"location":"manual/complex-domain_optimization/#Complex-Variables","page":"Complex-domain Optimization","title":"Complex Variables","text":"","category":"section"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"Complex Variables in Convex.jl are declared in the same way as the variables are declared but using the different keyword ComplexVariable.","category":"page"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"  # Scalar complex variable\n  z = ComplexVariable()\n\n  # Column vector variable\n  z = ComplexVariable(5)\n\n  # Matrix variable\n  z = ComplexVariable(4, 6)\n\n  # Complex Positive Semidefinite variable\n  z = HermitianSemidefinite(4)","category":"page"},{"location":"manual/complex-domain_optimization/#Linear-Program-Representable-Functions-(complex-variables)","page":"Complex-domain Optimization","title":"Linear Program Representable Functions (complex variables)","text":"","category":"section"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"All of the linear functions that are listed under Linear Program Representable Functions operate on complex variables as well. In addition, several specialized functions for complex variables are available:","category":"page"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"operation description vexity slope notes\nreal(z) real part of complex of variable affine increasing none\nimag(z) imaginary part of complex variable affine increasing none\nconj(x) element-wise complex conjugate affine increasing none\ninnerproduct(x,y) real(trace(x'*y)) affine increasing PR: one argument is constant","category":"page"},{"location":"manual/complex-domain_optimization/#Second-Order-Cone-Representable-Functions-(complex-variables)","page":"Complex-domain Optimization","title":"Second-Order Cone Representable Functions (complex variables)","text":"","category":"section"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"Most of the second order cone function listed under Second-Order Cone Representable Functions operate on complex variables as well. Notable exceptions include:","category":"page"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"inverse\nsquare\nquadoverlin\nsqrt\ngeomean\nhuber","category":"page"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"One new function is available:","category":"page"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"operation description vexity slope notes\nabs2(z) square(abs(z)) convex increasing none","category":"page"},{"location":"manual/complex-domain_optimization/#Semidefinite-Program-Representable-Functions-(complex-variables)","page":"Complex-domain Optimization","title":"Semidefinite Program Representable Functions (complex variables)","text":"","category":"section"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"All SDP-representable functions listed under Semidefinite Program Representable Functions work for complex variables.","category":"page"},{"location":"manual/complex-domain_optimization/#Exponential-SDP-representable-Functions-(complex-variables)","page":"Complex-domain Optimization","title":"Exponential + SDP representable Functions (complex variables)","text":"","category":"section"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"Complex variables also support logdet function.","category":"page"},{"location":"manual/complex-domain_optimization/#Optimizing-over-quantum-states","page":"Complex-domain Optimization","title":"Optimizing over quantum states","text":"","category":"section"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"The complex and Hermitian matrix variables, along with the kron and partialtrace operations, enable the definition of a wide range of problems in quantum information theory. As a simple example, let us consider a state rho over a composite Hilbert space mathcalH_AotimesmathcalH_B, where both component spaces are isomorphic to mathbbC^2. Assume that rho is a product state, with its component in mathcalH_A given as A, a complex-valued matrix. We can optimize over the second component B to meet some requirement. Here we simply fix the second component too, but via the partialtrace operator:","category":"page"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"using Convex, SCS\nA = [ 0.47213595 0.11469794+0.48586827im; 0.11469794-0.48586827im  0.52786405]\nB = ComplexVariable(2, 2)\nρ = kron(A, B)\nconstraints = [\n    partialtrace(ρ, 1, [2; 2]) == [1 0; 0 0],\n    tr(ρ) == 1,\n    isposdef(ρ),\n  ]\np = satisfy(constraints)\nsolve!(p, SCS.Optimizer; silent_solver = true)\np.status","category":"page"},{"location":"manual/complex-domain_optimization/","page":"Complex-domain Optimization","title":"Complex-domain Optimization","text":"Since we fix both components as trace-1 positive semidefinite matrices, the last two constraints are actually redundant in this case.","category":"page"},{"location":"examples/general_examples/logistic_regression/","page":"Logistic regression","title":"Logistic regression","text":"EditURL = \"logistic_regression.jl\"","category":"page"},{"location":"examples/general_examples/logistic_regression/#Logistic-regression","page":"Logistic regression","title":"Logistic regression","text":"","category":"section"},{"location":"examples/general_examples/logistic_regression/","page":"Logistic regression","title":"Logistic regression","text":"using DataFrames\nusing Plots\nusing RDatasets\nusing Convex\nusing SCS","category":"page"},{"location":"examples/general_examples/logistic_regression/","page":"Logistic regression","title":"Logistic regression","text":"This is an example logistic regression using RDatasets's iris data. Our goal is to predict whether the iris species is versicolor using the sepal length and width and petal length and width.","category":"page"},{"location":"examples/general_examples/logistic_regression/","page":"Logistic regression","title":"Logistic regression","text":"iris = dataset(\"datasets\", \"iris\");\niris[1:10, :]","category":"page"},{"location":"examples/general_examples/logistic_regression/","page":"Logistic regression","title":"Logistic regression","text":"We'll define Y as the outcome variable: +1 for versicolor, -1 otherwise.","category":"page"},{"location":"examples/general_examples/logistic_regression/","page":"Logistic regression","title":"Logistic regression","text":"Y = [species == \"versicolor\" ? 1.0 : -1.0 for species in iris.Species]","category":"page"},{"location":"examples/general_examples/logistic_regression/","page":"Logistic regression","title":"Logistic regression","text":"We'll create our data matrix with one column for each feature (first column corresponds to offset).","category":"page"},{"location":"examples/general_examples/logistic_regression/","page":"Logistic regression","title":"Logistic regression","text":"X = hcat(\n    ones(size(iris, 1)),\n    iris.SepalLength,\n    iris.SepalWidth,\n    iris.PetalLength,\n    iris.PetalWidth,\n);\nnothing #hide","category":"page"},{"location":"examples/general_examples/logistic_regression/","page":"Logistic regression","title":"Logistic regression","text":"Now to solve the logistic regression problem.","category":"page"},{"location":"examples/general_examples/logistic_regression/","page":"Logistic regression","title":"Logistic regression","text":"n, p = size(X)\nbeta = Variable(p)\nproblem = minimize(logisticloss(-Y .* (X * beta)))\nsolve!(problem, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/logistic_regression/","page":"Logistic regression","title":"Logistic regression","text":"Let's see how well the model fits.","category":"page"},{"location":"examples/general_examples/logistic_regression/","page":"Logistic regression","title":"Logistic regression","text":"using Plots\nlogistic(x::Real) = inv(exp(-x) + one(x))\nperm = sortperm(vec(X * evaluate(beta)))\nplot(1:n, (Y[perm] .+ 1) / 2, st = :scatter)\nplot!(1:n, logistic.(X * evaluate(beta))[perm])","category":"page"},{"location":"examples/general_examples/logistic_regression/","page":"Logistic regression","title":"Logistic regression","text":"","category":"page"},{"location":"examples/general_examples/logistic_regression/","page":"Logistic regression","title":"Logistic regression","text":"This page was generated using Literate.jl.","category":"page"},{"location":"developer/contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"developer/contributing/","page":"Contributing","title":"Contributing","text":"We'd welcome contributions to the Convex.jl package. Here are some short instructions on how to get started. If you don't know what you'd like to contribute, you could","category":"page"},{"location":"developer/contributing/","page":"Contributing","title":"Contributing","text":"take a look at the current   issues and pick   one. (Feature requests are probably the easiest to tackle.)\nadd a usage   example.","category":"page"},{"location":"developer/contributing/","page":"Contributing","title":"Contributing","text":"Then submit a pull request (PR). (Let us know if it's a work in progress by putting [WIP] in the name of the PR.)","category":"page"},{"location":"developer/contributing/#Adding-examples","page":"Contributing","title":"Adding examples","text":"","category":"section"},{"location":"developer/contributing/","page":"Contributing","title":"Contributing","text":"Take a look at our existing usage   examples   and add another in similar style.\nSubmit a PR. (Let us know if it's a work in progress by putting   [WIP] in the name of the PR.)\nWe'll look it over, fix up anything that doesn't work, and merge   it.","category":"page"},{"location":"developer/contributing/#Adding-atoms","page":"Contributing","title":"Adding atoms","text":"","category":"section"},{"location":"developer/contributing/","page":"Contributing","title":"Contributing","text":"Here are the steps to add a new function or operation (atom) to Convex.jl. Let's say you're adding the new function f.","category":"page"},{"location":"developer/contributing/","page":"Contributing","title":"Contributing","text":"Take a look at the   nuclear norm atom   for an example of how to construct atoms, and see the   norm atom   for an example of an atom that depends on a parameter.\nCopy paste (for example, the nuclear norm file), replace anything saying   nuclear norm with the name of the atom f, fill in monotonicity,   curvature, etc. Save it in the appropriate subdirectory of   src/atoms/.\nEnsure the atom is a mutable struct, so that objectid can be called.\nAdd as a comment a description of what the atom does and its   parameters.\nEnsure evaluate is only called during the definition of evaluate itself, from conic_form!, or   on constants or matrices. Specifically, evaluate must not be called on a potentially fix!'d variable   when the expression tree is being built (for example, when constructing an atom or reformulating),   since then any changes to the variable's value (or it being free!'d) will not be recognized.   See #653 and #585   for previous bugs caused by misuse here.\nThe most mathematically interesting part is the new_conic_form!   function. Following the example in the nuclear norm atom, you'll   see that you can just construct the problem whose optimal value is   f(x), introducing any auxiliary variables you need, exactly as   you would normally in Convex.jl, and then call conic_form!   on that problem.\nAdd a test for the atom so we can verify it works in   src/problem_depot/problem/<cone>, where <cone> matches the subdirectory of   src/atoms. See How to write a ProblemDepot problem for details   on how to write the tests.\nFollowing the other examples, add a test to test/test_atoms.jl.\nSubmit a PR, including a description of what the atom does and its   parameters. (Let us know if it's a work in progress by putting   [WIP] in the name of the PR.)\nWe'll look it over, fix up anything that doesn't work, and merge   it.","category":"page"},{"location":"developer/contributing/#Fixing-the-guts","page":"Contributing","title":"Fixing the guts","text":"","category":"section"},{"location":"developer/contributing/","page":"Contributing","title":"Contributing","text":"If you want to do a more major bug fix, you may need to understand how Convex.jl thinks about conic form.","category":"page"},{"location":"developer/contributing/","page":"Contributing","title":"Contributing","text":"To do this, start by reading the Convex.jl paper.","category":"page"},{"location":"developer/contributing/","page":"Contributing","title":"Contributing","text":"You may find our JuliaCon 2014 talk helpful as well; you can find the Jupyter notebook presented in the talk here.","category":"page"},{"location":"developer/contributing/","page":"Contributing","title":"Contributing","text":"Convex has been updated several times over the years however, so older information may be out of date. Here is a brief summary of how the package works (as of July 2023).","category":"page"},{"location":"developer/contributing/","page":"Contributing","title":"Contributing","text":"A Problem{T} struct is created by putting together an objective function and constraints. The problem is an expression graph, in which variables and constants are the leaf nodes, and atoms form the intermediate nodes. Here T refers to the numeric type of the problem that all data coefficients will be coerced to when we pass the data to a solver.\nWhen we go to solve! a problem, we first load it into a MathOptInterface (MOI) model. To do so, we traverse the Problem and apply our extended formulations. This occurs via conic_form!. We construct a Context{T} associated to the problem, which holds an MOI model, and progressively load it by applying conic_form! to each object's children and then itself. For variables, conic_form! returns SparseTape{T} or ComplexTape{T}, depending on the sign variable. Likewise for constants, conic_form! returns either Vector{T} or ComplexStructOfVec{T}. Here a Tape refers to a lazy sequence of sparse affine operators that will be applied to a vector of variables. The central computational task of Convex is to compose this sequence of operators (and thus enact it's extended formulations). For atoms, conic_form! generally either creates a new object using Convex' primitives (for example, another problem) and calls conic_form! on that, or, when that isn't possible, calls operate to manipulate the tape objects themselves (for example, to add a new operation to the composition). We try to minimize the amount of operate methods and defer to existing primitives when possible. conic_form! can also create new constraints and add them directly to the model. It is easy to create constraints of the form \"vector-affine-function-in-cone\" for any of MOI's many supported cones; these constraints do not need to be exposed at the level of Convex itself as Constraint objects, although they can be.\nOnce we have filled our Context{T}, we go to solve it with MOI. Then we recover the solution status and values of primal and dual variables, and populate them using dictionaries stored in the Context.","category":"page"},{"location":"developer/contributing/","page":"Contributing","title":"Contributing","text":"You're now armed and dangerous. Go ahead and open an issue (or comment on a previous one) if you can't figure something out, or submit a PR if you can figure it out. (Let us know if it's a work in progress by putting [WIP] in the name of the PR.)","category":"page"},{"location":"developer/contributing/","page":"Contributing","title":"Contributing","text":"PRs that comment the code more thoroughly will also be welcomed.","category":"page"},{"location":"developer/contributing/#Developer-notes","page":"Contributing","title":"Developer notes","text":"","category":"section"},{"location":"developer/contributing/","page":"Contributing","title":"Contributing","text":"conic_form! is allowed to mutate the context, but should never mutate the atoms or problems\nWe currently construct a fresh context on every solve. It may be possible to set things up to reuse contexts for efficiency.\nData flow: we take in user data that may be of any type.\nAt the level of problem formulation (when we construct atoms), we convert everything to an AbstractExpr (or Constraint); in particular, constants become Constant or ComplexConstant. At this time we don't know the numeric type that will be used to solve the problem.\nOnce we begin to solve! the problem, we recursively call conic_form!. The output is of type SparseTape{T}, ComplexTape{T}, Vector{T}, or ComplexStructOfVec{T}. We can call operate to manipulate these outputs.\nWe convert these to MOI.VectorAffineFunction before passing them to MOI.","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"EditURL = \"basic_usage.jl\"","category":"page"},{"location":"examples/general_examples/basic_usage/#Basic-Usage","page":"Basic Usage","title":"Basic Usage","text":"","category":"section"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"First we load Convex itself, LinearAlgebra to access the identity matrix I, and two solvers: SCS and GLPK.","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"using Convex\nusing LinearAlgebra\nusing SCS, GLPK","category":"page"},{"location":"examples/general_examples/basic_usage/#Linear-program","page":"Basic Usage","title":"Linear program","text":"","category":"section"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"beginarrayll\n  textmaximize  c^T x \n  textsubject to  A x leq b\n   x geq 1 \n   x leq 10 \n   x_2 leq 5 \n   x_1 + x_4 - x_2 leq 10 \nendarray","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"x = Variable(4)\nc = [1; 2; 3; 4]\nA = I(4)\nb = [10; 10; 10; 10]\nconstraints = [A * x <= b, x >= 1, x <= 10, x[2] <= 5, x[1] + x[4] - x[2] <= 10]\np = minimize(dot(c, x), constraints) # or c' * x\nsolve!(p, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"We can also inspect the objective value and the values of the variables at the solution:","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"println(round(p.optval, digits = 2))\nprintln(round.(evaluate(x), digits = 2))\nprintln(evaluate(x[1] + x[4] - x[2]))","category":"page"},{"location":"examples/general_examples/basic_usage/#Matrix-Variables-and-promotions","page":"Basic Usage","title":"Matrix Variables and promotions","text":"","category":"section"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"beginarrayll\n  textminimize   X _F + y \n  textsubject to  2 X leq 1\n   X + y geq 1 \n   X geq 0 \n   y geq 0 \nendarray","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"X = Variable(2, 2)\ny = Variable()\n# X is a 2 x 2 variable, and y is scalar. X' + y promotes y to a 2 x 2 variable before adding them\np = minimize(norm(X) + y, 2 * X <= 1, X' + y >= 1, X >= 0, y >= 0)\nsolve!(p, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"We can also inspect the values of the variables at the solution:","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"println(round.(evaluate(X), digits = 2))\nprintln(evaluate(y))\np.optval","category":"page"},{"location":"examples/general_examples/basic_usage/#Norm,-exponential-and-geometric-mean","page":"Basic Usage","title":"Norm, exponential and geometric mean","text":"","category":"section"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"beginarrayll\n  textsatisfy   x _2 leq 100 \n   e^x_1 leq 5 \n   x_2 geq 7 \n   sqrtx_3 x_4 geq x_2\nendarray","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"x = Variable(4)\np = satisfy(\n    norm(x) <= 100,\n    exp(x[1]) <= 5,\n    x[2] >= 7,\n    geomean(x[3], x[4]) >= x[2],\n)\nsolve!(p, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/basic_usage/#PSD-cone-and-Eigenvalues","page":"Basic Usage","title":"PSD cone and Eigenvalues","text":"","category":"section"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"y = Semidefinite(2)\np = maximize(eigmin(y), tr(y) <= 6)\nsolve!(p, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"x = Variable()\ny = Variable((2, 2))\n\n# PSD constraints\np = minimize(x + y[1, 1], y ⪰ 0, x >= 1, y[2, 1] == 1)\nsolve!(p, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/basic_usage/#Mixed-integer-program","page":"Basic Usage","title":"Mixed integer program","text":"","category":"section"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"beginarrayll\n  textminimize  sum_i=1^n x_i \n    textsubject to  x in mathbbZ^n \n   x geq 05 \nendarray","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"x = Variable(4, IntVar)\np = minimize(sum(x), x >= 0.5)\nsolve!(p, GLPK.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"And the value of x at the solution:","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"evaluate(x)","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"","category":"page"},{"location":"examples/general_examples/basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"This page was generated using Literate.jl.","category":"page"},{"location":"manual/operations/#Supported-Operations","page":"Supported Operations","title":"Supported Operations","text":"","category":"section"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"Convex.jl currently supports the following functions. These functions may be composed according to the DCP composition rules to form new convex, concave, or affine expressions.","category":"page"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"Convex.jl transforms each problem into an equivalent conic program in order to pass the problem to a specialized solver. Depending on the types of functions used in the problem, the conic constraints may include linear, second-order, exponential, or semidefinite constraints, as well as any binary or integer constraints placed on the variables.","category":"page"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"Below, we list each function available in Convex.jl organized by the (most complex) type of cone used to represent that function, and indicate which solvers may be used to solve problems with those cones. Problems mixing many different conic constraints can be solved by any solver that supports every kind of cone present in the problem.","category":"page"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"In the notes column in the tables below, we denote implicit constraints imposed on the arguments to the function by IC, and parameter restrictions that the arguments must obey by PR. Convex.jl will automatically impose ICs; the user must make sure to satisfy PRs.","category":"page"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"Elementwise means that the function operates elementwise on vector arguments, returning a vector of the same size.","category":"page"},{"location":"manual/operations/#Linear-Program-Representable-Functions","page":"Supported Operations","title":"Linear Program Representable Functions","text":"","category":"section"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"An optimization problem using only these functions can be solved by any LP solver.","category":"page"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"operation description vexity slope notes\nx+y or x.+y addition affine increasing \nx-y or x.-y subtraction affine increasing in x decreasing in y \nx*y multiplication affine increasing if constant term ge 0 decreasing if constant term le 0 not monotonic otherwise PR: one argument is constant\nx/y division affine increasing PR: y is scalar constant\nx .* y elementwise multiplication affine increasing PR: one argument is constant\nx ./ y elementwise division affine increasing PR: one argument is constant\nx[1:4, 2:3] indexing and slicing affine increasing \ndiag(x, k) k-th diagonal of a matrix affine increasing \ndiagm(x) construct diagonal matrix affine increasing PR: x is a vector\nx' transpose affine increasing \nvec(x) vector representation affine increasing \ndot(x,y) sum_i x_i y_i affine increasing PR: one argument is constant\nkron(x,y) Kronecker product affine increasing PR: one argument is constant\nsum(x) sum_ij x_ij affine increasing \nsum(x, k) sum elements across dimension k affine increasing \nsumlargest(x, k) sum of k largest elements of x convex increasing \nsumsmallest(x, k) sum of k smallest elements of x concave increasing \ndotsort(a, b) dot(sort(a),sort(b)) convex increasing PR: one argument is constant\nreshape(x, m, n) reshape into m times n affine increasing \nminimum(x) min(x) concave increasing \nmaximum(x) max(x) convex increasing \n[x y] or [x; y] hcat(x, y) or vcat(x, y) stacking affine increasing \ntr(x) computes the trace mathrmtr left(X right) affine increasing \npartialtrace(x,sys,dims) Partial trace affine increasing \npartialtranspose(x,sys,dims) Partial transpose affine increasing \nconv(h,x) h in mathbbR^m, x in mathbbR^n, hstar x in mathbbR^m+n-1; entry i is given by sum_j=1^m h_jx_i-j+1 with x_k=0 for k out of bounds affine increasing if hge 0 decreasing if hle 0 not monotonic otherwise PR: h is constant\nmin(x,y) min(xy) concave increasing \nmax(x,y) max(xy) convex increasing \npos(x) max(x0) convex increasing \nneg(x) max(-x0) convex decreasing \ninvpos(x) 1x convex decreasing IC: x0\nabs(x) leftxright convex increasing on x ge 0 decreasing on x le 0 \nopnorm(x, 1) maximum absolute column sum: max_1  j  n sum_i=1^m leftx_ijright convex increasing on x ge 0 decreasing on x le 0 \nopnorm(x, Inf) maximum absolute row sum: max_1  i  m sum_j=1^n leftx_ijright convex increasing on x ge 0 decreasing on x le 0 ","category":"page"},{"location":"manual/operations/#Second-Order-Cone-Representable-Functions","page":"Supported Operations","title":"Second-Order Cone Representable Functions","text":"","category":"section"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"An optimization problem using these functions can be solved by any SOCP solver (including ECOS, SCS, Mosek, Gurobi, and CPLEX). Of course, if an optimization problem has both LP and SOCP representable functions, then any solver that can solve both LPs and SOCPs can solve the problem.","category":"page"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"operation description vexity slope notes\nnorm(x, p) (sum x_i^p)^1p convex increasing on x ge 0 decreasing on x le 0 PR: p >= 1\nquadform(x, P; assume_psd=false) x^T P x convex in x affine in P increasing on x ge 0 decreasing on x le 0 increasing in P PR: either x or P must be constant; if x is not constant, then P must be symmetric and positive semidefinite. Pass assume_psd=true to skip checking if P is positive semidefinite.\nquadoverlin(x, y) x^T xy convex increasing on x ge 0 decreasing on x le 0 decreasing in y IC: y  0\nsumsquares(x) sum x_i^2 convex increasing on x ge 0 decreasing on x le 0 \nsqrt(x) sqrtx concave decreasing IC: x0\nsquare(x), x^2 x^2 convex increasing on x ge 0 decreasing on x le 0 PR : x is scalar\nx .^ 2 x^2 convex increasing on x ge 0 decreasing on x le 0 elementwise\ngeomean(x, y) sqrtxy concave increasing IC: xge0, yge0\nhuber(x, M=1) begincases x^2 x leq M  2Mx - M^2 x  M endcases convex increasing on x ge 0 decreasing on x le 0 PR: M=1","category":"page"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"Note that for p=1 and p=Inf, the function norm(x,p) is a linear-program representable, and does not need a SOCP solver, and for a matrix x, norm(x,p) is defined as norm(vec(x), p).","category":"page"},{"location":"manual/operations/#Exponential-Cone-Representable-Functions","page":"Supported Operations","title":"Exponential Cone Representable Functions","text":"","category":"section"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"An optimization problem using these functions can be solved by any exponential cone solver (SCS).","category":"page"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"operation description vexity slope notes\nlogsumexp(x) log(sum_i exp(x_i)) convex increasing \nexp(x) exp(x) convex increasing \nlog(x) log(x) concave increasing IC: x0\nentropy(x) sum_ij -x_ij log(x_ij) concave not monotonic IC: x0\nentropy_elementwise(x) -x log(x) concave not monotonic IC: x0\nlogisticloss(x) log(1 + exp(x_i)) convex increasing \nrelative_entropy(x, y) sum_i x_i log(x_i  y_i) convex not monotonic IC: x0 y0","category":"page"},{"location":"manual/operations/#Semidefinite-Program-Representable-Functions","page":"Supported Operations","title":"Semidefinite Program Representable Functions","text":"","category":"section"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"An optimization problem using these functions can be solved by any SDP solver (including SCS and Mosek).","category":"page"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"operation description vexity slope notes\nnuclearnorm(x) sum of singular values of x convex not monotonic \nopnorm(x, 2) (operatornorm(x)) max of singular values of x convex not monotonic \neigmax(x) max eigenvalue of x convex not monotonic \neigmin(x) min eigenvalue of x concave not monotonic \nrootdet(X) n-th root-determinant of the n-by-n matrix X, that is det(X)^1n concave not monotonic \nmatrixfrac(x, P) x^TP^-1x convex not monotonic IC: P is positive semidefinite\nsumlargesteigs(x, k) sum of top k eigenvalues of x convex not monotonic IC: P symmetric\nConvex.Constraint((T, A, B), GeometricMeanHypoConeSquare(t, size(T, 1))) T preceq A _t B = A^12 (A^-12 B A^-12)^t A^12 concave increasing IC: A succeq 0, B succeq 0, t in 01\nConvex.Constraint((T, A, B), GeometricMeanEpiConeSquare(t, size(T, 1))) T succeq A _t B = A^12 (A^-12 B A^-12)^t A^12 convex not monotonic IC: A succeq 0, B succeq 0, t in -1 0 cup 1 2\nquantum_entropy(X) -textrmTr(X log X) concave not monotonic IC: X succeq 0; uses natural log\nquantum_relative_entropy(A, B) textrmTr(A log A - A log B) convex not monotonic IC: A succeq 0, B succeq 0; uses natural log\ntrace_logm(X, C) textrmTr(C log X) concave in X not monotonic IC: X succeq 0, C succeq 0, C constant; uses natural log\ntrace_mpower(A, t, C) textrmTr(C A^t) concave in A for t in 01, convex for t in -10 cup 12 not monotonic IC: X succeq 0, C succeq 0, C constant, t in -1 2\nlieb_ando(A, B, K, t) textrmTr(K A^1-t K B^t) concave in A,B for t in 01, convex for t in -10 cup 12 not monotonic IC: A succeq 0, B succeq 0, K constant, t in -1 2\nConvex.Constraint((T, X, Y), RelativeEntropyEpiConeSquare(size(X, 1), m, k, e)) T succeq e X^12 log(X^12 Y^-1 X^12) X^12 e convex not monotonic IC: e constant; uses natural log","category":"page"},{"location":"manual/operations/#Exponential-SDP-representable-Functions","page":"Supported Operations","title":"Exponential + SDP representable Functions","text":"","category":"section"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"An optimization problem using these functions can be solved by any solver that supports exponential constraints and semidefinite constraints simultaneously (SCS).","category":"page"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"operation description vexity slope notes\nlogdet(x) log of determinant of x concave increasing IC: x is positive semidefinite","category":"page"},{"location":"manual/operations/#Promotions","page":"Supported Operations","title":"Promotions","text":"","category":"section"},{"location":"manual/operations/","page":"Supported Operations","title":"Supported Operations","text":"When an atom or constraint is applied to a scalar and a higher dimensional variable, the scalars are promoted. For example, we can do max(x, 0) gives an expression with the shape of x whose elements are the maximum of the corresponding element of x and 0.","category":"page"},{"location":"manual/types/#Basic-Types","page":"Basic Types","title":"Basic Types","text":"","category":"section"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"The basic building block of Convex.jl is called an expression, which can represent a variable, a constant, or a function of another expression. We discuss each kind of expression in turn.","category":"page"},{"location":"manual/types/#Variables","page":"Basic Types","title":"Variables","text":"","category":"section"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"The simplest kind of expression in Convex.jl is a variable. Variables in Convex.jl are declared using the Variable keyword, along with the dimensions of the variable.","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"# Scalar variable\nx = Variable()\n\n# Column vector variable\nx = Variable(5)\n\n# Matrix variable\nx = Variable(4, 6)","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"Variables may also be declared as having special properties, such as being","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"(elementwise) positive: x = Variable(4, Positive())\n(elementwise) negative: x = Variable(4, Negative())\nintegral: x = Variable(4, IntVar)\nbinary: x = Variable(4, BinVar)\n(for a matrix) being symmetric, with nonnegative eigenvalues (that is,    positive semidefinite): z = Semidefinite(4)","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"The order of the arguments is the size, the sign, and then the Convex.VarType (that is, integer, binary, or continuous), and any may be omitted to use the default. The current value of a variable x can be accessed with evaluate(x). After solve!ing a problem, the value of each variable used in the problem is set to its optimal value.","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"See also Custom Variable Types for how to implement your own variable types.","category":"page"},{"location":"manual/types/#Constants","page":"Basic Types","title":"Constants","text":"","category":"section"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"Numbers, vectors, and matrices present in the Julia environment are wrapped automatically into a Constant expression when used in a Convex.jl expression.","category":"page"},{"location":"manual/types/#Expressions","page":"Basic Types","title":"Expressions","text":"","category":"section"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"Expressions in Convex.jl are formed by applying any atom (mathematical function defined in Convex.jl) to variables, constants, and other expressions. For a list of these functions, see Supported Operations. Atoms are applied to expressions using operator overloading. For example, 2+2 calls Julia's built-in addition operator, while 2+x calls the Convex.jl addition method and returns a Convex.jl expression. Many of the useful language features in Julia, such as arithmetic, array indexing, and matrix transpose are overloaded in Convex.jl so they may be used with variables and expressions just as they are used with native Julia types.","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"Expressions that are created must be DCP-compliant. More information on DCP can be found here. :","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"x = Variable(5)\n# The following are all expressions\ny = sum(x)\nz = 4 * x + y\nz_1 = z[1]","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"Convex.jl allows the values of the expressions to be evaluated directly.","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"x = Variable()\ny = Variable()\nz = Variable()\nexpr = x + y + z\nproblem = minimize(expr, x >= 1, y >= x, 4 * z >= y)\nsolve!(problem, SCS.Optimizer)\n\n# Once the problem is solved, we can call evaluate() on expr:\nevaluate(expr)","category":"page"},{"location":"manual/types/#Constraints","page":"Basic Types","title":"Constraints","text":"","category":"section"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"Constraints in Convex.jl are declared using the standard comparison operators <=, >=, and ==. They specify relations that must hold between two expressions. Convex.jl does not distinguish between strict and non-strict inequality constraints.","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"x = Variable(5, 5)\n# Equality constraint\nconstraint = x == 0\n# Inequality constraint\nconstraint = x >= 1","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"Note that constraints apply elementwise automatically; that is, x >= 1 means that x[i, j] >= 1 for i in 1:5 and j in 1:5. Consequently, broadcasting should not be used to constrain arrays, that is, use x >= y instead of x .>= y.","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"Matrices can also be constrained to be positive semidefinite.","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"x = Variable(3, 3)\ny = Variable(3, 1)\nz = Variable()\n# constrain [x y; y' z] to be positive semidefinite\nconstraint = isposdef([x y; y' z])\n# or equivalently,\nconstraint = ([x y; y' z] ⪰ 0)","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"Constraints can also be added to variables after their construction, to automatically apply constraints to any problem which uses the variable. For example,","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"x = Variable(3)\nadd_constraint!(x, sum(x) == 1)","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"Now, in any problem in which x is used, the constraint sum(x) == 1 will be added.","category":"page"},{"location":"manual/types/#Objective","page":"Basic Types","title":"Objective","text":"","category":"section"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"The objective of the problem is a scalar expression to be maximized or minimized by using maximize or minimize respectively. Feasibility problems can be expressed by either giving a constant as the objective, or using problem = satisfy(constraints).","category":"page"},{"location":"manual/types/#Problem","page":"Basic Types","title":"Problem","text":"","category":"section"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"A problem in Convex.jl consists of a sense (minimize, maximize, or satisfy), an objective (an expression to which the sense verb is to be applied), and zero or more constraints that must be satisfied at the solution. Problems may be constructed as","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"problem = minimize(objective, constraints)\n# or\nproblem = maximize(objective, constraints)\n# or\nproblem = satisfy(constraints)","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"Constraints can be added at any time before the problem is solved.","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"# No constraints given\nproblem = minimize(objective)\n# Add some constraint\nproblem.constraints += constraint\n# Add many more constraints\nproblem.constraints += [constraint1, constraint2, ...]","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"A problem can be solved by calling solve!","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"solve!(problem, solver)","category":"page"},{"location":"manual/types/","page":"Basic Types","title":"Basic Types","text":"passing a solver such as SCS.Optimizer() from the package SCS as the second argument. After the problem is solved, problem.status records the status returned by the optimization solver, and can be :Optimal, :Infeasible, :Unbounded, :Indeterminate or :Error. If the status is :Optimal, problem.optval will record the optimum value of the problem. The optimal value for each variable x participating in the problem can be found in evaluate(x). The optimal value of an expression can be found by calling the evaluate() function on the expression as follows: evaluate(expr).","category":"page"},{"location":"examples/mixed_integer/n_queens/","page":"N queens","title":"N queens","text":"EditURL = \"n_queens.jl\"","category":"page"},{"location":"examples/mixed_integer/n_queens/#N-queens","page":"N queens","title":"N queens","text":"","category":"section"},{"location":"examples/mixed_integer/n_queens/","page":"N queens","title":"N queens","text":"using Convex, GLPK, LinearAlgebra, SparseArrays, Test\naux(str) = joinpath(@__DIR__, \"aux_files\", str) # path to auxiliary files\ninclude(aux(\"antidiag.jl\"))\n\nn = 8","category":"page"},{"location":"examples/mixed_integer/n_queens/","page":"N queens","title":"N queens","text":"We encode the locations of the queens with a matrix of binary random variables.","category":"page"},{"location":"examples/mixed_integer/n_queens/","page":"N queens","title":"N queens","text":"x = Variable((n, n), BinVar)","category":"page"},{"location":"examples/mixed_integer/n_queens/","page":"N queens","title":"N queens","text":"Now we impose the constraints: at most one queen on any anti-diagonal, at most one queen on any diagonal, and we must have exactly one queen per row and per column.","category":"page"},{"location":"examples/mixed_integer/n_queens/","page":"N queens","title":"N queens","text":"# At most one queen on any anti-diagonal\nconstraints = Constraint[sum(antidiag(x, k)) <= 1 for k in -n+2:n-2]\n# At most one queen on any diagonal\nappend!(constraints, [sum(diag(x, k)) <= 1 for k in -n+2:n-2])\n# Exactly one queen per row and one queen per column\nappend!(constraints, [sum(x, dims = 1) == 1, sum(x, dims = 2) == 1])\np = satisfy(constraints)\nsolve!(p, GLPK.Optimizer)","category":"page"},{"location":"examples/mixed_integer/n_queens/","page":"N queens","title":"N queens","text":"Let us test the results:","category":"page"},{"location":"examples/mixed_integer/n_queens/","page":"N queens","title":"N queens","text":"for k in -n+2:n-2\n    @test evaluate(sum(antidiag(x, k))) <= 1\n    @test evaluate(sum(diag(x, k))) <= 1\nend\n@test all(evaluate(sum(x, dims = 1)) .≈ 1)\n@test all(evaluate(sum(x, dims = 2)) .≈ 1)","category":"page"},{"location":"examples/mixed_integer/n_queens/","page":"N queens","title":"N queens","text":"","category":"page"},{"location":"examples/mixed_integer/n_queens/","page":"N queens","title":"N queens","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"EditURL = \"tomography.jl\"","category":"page"},{"location":"examples/tomography/tomography/#Tomography","page":"Tomography","title":"Tomography","text":"","category":"section"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"Tomography is the process of reconstructing a density distribution from given integrals over sections of the distribution. In our example, we will work with tomography on black and white images. Suppose x be the vector of n pixel densities, with x_j denoting how white pixel j is. Let y be the vector of m line integrals over the image, with y_i denoting the integral for line i. We can define a matrix A to describe the geometry of the lines. Entry A_ij describes how much of pixel j is intersected by line i. Assuming our measurements of the line integrals are perfect, we have the relationship that","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"  y = Ax","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"However, anytime we have measurements, there are usually small errors that occur. Therefore it makes sense to try to minimize","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":" y - Ax_2^2","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"This is simply an unconstrained least squares problem; something we can readily solve.","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"using Convex, ECOS, DelimitedFiles, SparseArrays\naux(str) = joinpath(@__DIR__, \"aux_files\", str) # path to auxiliary files\nline_mat_x = readdlm(aux(\"tux_sparse_x.txt\"))\nsummary(line_mat_x)\n\nline_mat_y = readdlm(aux(\"tux_sparse_y.txt\"))\nsummary(line_mat_y)\n\nline_mat_val = readdlm(aux(\"tux_sparse_val.txt\"))\nsummary(line_mat_val)\n\nline_vals = readdlm(aux(\"tux_sparse_lines.txt\"))\nsummary(line_vals)","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"Form the sparse matrix from the data Image is 50 x 50","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"img_size = 50","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"The number of pixels in the image","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"num_pixels = img_size * img_size\n\nline_mat = spzeros(length(line_vals), num_pixels)\n\nnum_vals = length(line_mat_val)\n\nfor i in 1:num_vals\n    x = Int(line_mat_x[i])\n    y = Int(line_mat_y[i])\n    line_mat[x+1, y+1] = line_mat_val[i]\nend\n\npixel_colors = Variable(num_pixels)\n# line_mat * pixel_colors should be close to the line_integral_values\n# to reflect that, we minimize a norm\nobjective = sumsquares(line_mat * pixel_colors - line_vals)\nproblem = minimize(objective)\nsolve!(problem, ECOS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"rows = zeros(img_size * img_size)\ncols = zeros(img_size * img_size)\nfor i in 1:img_size\n    for j in 1:img_size\n        rows[(i-1)*img_size+j] = i\n        cols[(i-1)*img_size+j] = img_size + 1 - j\n    end\nend","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"Plot the image using the pixel values obtained:","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"using Plots\nimage = reshape(evaluate(pixel_colors), img_size, img_size)\nheatmap(\n    image,\n    yflip = true,\n    aspect_ratio = 1,\n    colorbar = nothing,\n    color = :grays,\n)","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"","category":"page"},{"location":"examples/tomography/tomography/","page":"Tomography","title":"Tomography","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/mixed_integer/binary_knapsack/","page":"Binary (or 0-1) knapsack problem","title":"Binary (or 0-1) knapsack problem","text":"EditURL = \"binary_knapsack.jl\"","category":"page"},{"location":"examples/mixed_integer/binary_knapsack/#Binary-(or-0-1)-knapsack-problem","page":"Binary (or 0-1) knapsack problem","title":"Binary (or 0-1) knapsack problem","text":"","category":"section"},{"location":"examples/mixed_integer/binary_knapsack/","page":"Binary (or 0-1) knapsack problem","title":"Binary (or 0-1) knapsack problem","text":"Given a knapsack of some capacity C and n objects with object i having weight w_i and profit p_i, the goal is to choose some subset of the objects that can fit in the knapsack (that is, the sum of their weights is no more than C) while maximizing profit.","category":"page"},{"location":"examples/mixed_integer/binary_knapsack/","page":"Binary (or 0-1) knapsack problem","title":"Binary (or 0-1) knapsack problem","text":"This can be formulated as a mixed-integer program as:","category":"page"},{"location":"examples/mixed_integer/binary_knapsack/","page":"Binary (or 0-1) knapsack problem","title":"Binary (or 0-1) knapsack problem","text":"beginarrayll\n  textmaximize  x p \n    textsubject to  x in 0 1 \n   w x leq C \nendarray","category":"page"},{"location":"examples/mixed_integer/binary_knapsack/","page":"Binary (or 0-1) knapsack problem","title":"Binary (or 0-1) knapsack problem","text":"where x is a vector is size n where x_i is one if we chose to keep the object in the knapsack, 0 otherwise.","category":"page"},{"location":"examples/mixed_integer/binary_knapsack/","page":"Binary (or 0-1) knapsack problem","title":"Binary (or 0-1) knapsack problem","text":"# Data taken from http://people.sc.fsu.edu/~jburkardt/datasets/knapsack_01/knapsack_01.html\nw = [23; 31; 29; 44; 53; 38; 63; 85; 89; 82]\nC = 165\np = [92; 57; 49; 68; 60; 43; 67; 84; 87; 72];\nn = length(w)","category":"page"},{"location":"examples/mixed_integer/binary_knapsack/","page":"Binary (or 0-1) knapsack problem","title":"Binary (or 0-1) knapsack problem","text":"using Convex, GLPK\nx = Variable(n, BinVar)\nproblem = maximize(dot(p, x), dot(w, x) <= C)\nsolve!(problem, GLPK.Optimizer)","category":"page"},{"location":"examples/mixed_integer/binary_knapsack/","page":"Binary (or 0-1) knapsack problem","title":"Binary (or 0-1) knapsack problem","text":"evaluate(x)","category":"page"},{"location":"examples/mixed_integer/binary_knapsack/","page":"Binary (or 0-1) knapsack problem","title":"Binary (or 0-1) knapsack problem","text":"","category":"page"},{"location":"examples/mixed_integer/binary_knapsack/","page":"Binary (or 0-1) knapsack problem","title":"Binary (or 0-1) knapsack problem","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/supplemental_material/paper_examples/","page":"Paper examples","title":"Paper examples","text":"EditURL = \"paper_examples.jl\"","category":"page"},{"location":"examples/supplemental_material/paper_examples/#Paper-examples","page":"Paper examples","title":"Paper examples","text":"","category":"section"},{"location":"examples/supplemental_material/paper_examples/","page":"Paper examples","title":"Paper examples","text":"using Convex, ECOS","category":"page"},{"location":"examples/supplemental_material/paper_examples/","page":"Paper examples","title":"Paper examples","text":"Summation.","category":"page"},{"location":"examples/supplemental_material/paper_examples/","page":"Paper examples","title":"Paper examples","text":"println(\"Summation example\")\nx = Variable();\ne = 0;\n@time begin\n    for i in 1:1000\n        global e\n        e += x\n    end\n    p = minimize(e, x >= 1)\nend\n@time solve!(p, ECOS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/supplemental_material/paper_examples/","page":"Paper examples","title":"Paper examples","text":"Indexing.","category":"page"},{"location":"examples/supplemental_material/paper_examples/","page":"Paper examples","title":"Paper examples","text":"println(\"Indexing example\")\nx = Variable(1000, 1);\ne = 0;\n@time begin\n    for i in 1:1000\n        global e\n        e += x[i]\n    end\n    p = minimize(e, x >= ones(1000, 1))\nend\n@time solve!(p, ECOS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/supplemental_material/paper_examples/","page":"Paper examples","title":"Paper examples","text":"Matrix constraints.","category":"page"},{"location":"examples/supplemental_material/paper_examples/","page":"Paper examples","title":"Paper examples","text":"println(\"Matrix constraint example\")\nn, m, p = 100, 100, 100\nX = Variable(m, n);\nA = randn(p, m);\nb = randn(p, n);\n@time begin\n    p = minimize(norm(X), A * X == b)\nend\n@time solve!(p, ECOS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/supplemental_material/paper_examples/","page":"Paper examples","title":"Paper examples","text":"Transpose.","category":"page"},{"location":"examples/supplemental_material/paper_examples/","page":"Paper examples","title":"Paper examples","text":"println(\"Transpose example\")\nX = Variable(5, 5);\nA = randn(5, 5);\n@time begin\n    p = minimize(norm2(X - A), X' == X)\nend\n@time solve!(p, ECOS.Optimizer; silent_solver = true)\n\nn = 3\nA = randn(n, n);\n#@time begin\nX = Variable(n, n);\np = minimize(norm(X' - A), X[1, 1] == 1);\nsolve!(p, ECOS.Optimizer; silent_solver = true)\n#end","category":"page"},{"location":"examples/supplemental_material/paper_examples/","page":"Paper examples","title":"Paper examples","text":"","category":"page"},{"location":"examples/supplemental_material/paper_examples/","page":"Paper examples","title":"Paper examples","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"EditURL = \"phase_recovery_using_MaxCut.jl\"","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/#Phase-recovery-using-MaxCut","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"","category":"section"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"In this example, we relax the phase retrieval problem similar to the classical MaxCut semidefinite program and recover the phase of the signal given the magnitude of the linear measurements.","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"Phase recovery has wide applications such as  in X-ray and crystallography imaging, diffraction imaging or microscopy and audio signal processing. In all these applications, the detectors cannot measure the phase of the incoming wave and only record its amplitude i.e complex measurements of a signal x in mathbbC^p are obtained from a linear injective operator A, but we can only measure the magnitude vector Ax, not the phase of Ax.","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"Recovering the phase of Ax from Ax is a nonconvex optimization problem. Using results from this paper, the problem can be relaxed to a (complex) semidefinite program (complex SDP).","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"The original representation of the problem is as follows:","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"beginarrayll\n  textfind  x in mathbbC^p \n    textsubject to  Ax = b\nendarray","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"where A in mathbbC^n times p and b in mathbbR^n.","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"In this example, the problem is to find the phase of Ax given the value Ax. Given a linear operator A and a vector b= Ax of measured amplitudes, in the noiseless case, we can write Ax = textdiag(b)u where u in mathbbC^n  is a phase vector, satisfying mathbbu_i = 1 for i = 1ldots n.","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"We relax this problem as Complex Semidefinite Programming.","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/#Relaxed-Problem-similar-to-[MaxCut](http://www-math.mit.edu/goemans/PAPERS/maxcut-jacm.pdf)","page":"Phase recovery using MaxCut","title":"Relaxed Problem similar to MaxCut","text":"","category":"section"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"Define the positive semidefinite hermitian matrix M = textdiag(b) (I - A A^*) textdiag(b). The problem is:","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"beginarrayll\n  textminimize  langle U M rangle \n    textsubject to  textdiag(U) = 1\n     U succeq 0\nendarray","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"Here the variable U must be hermitian (U in mathbbH_n ) and we have a solution to the phase recovery problem if U = u u^* has rank one. Otherwise, the leading singular vector of U can be used to approximate the solution.","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"using Convex, SCS, LinearAlgebra\n\nn = 20\np = 2\nA = rand(n, p) + im * randn(n, p)\nx = rand(p) + im * randn(p)\nb = abs.(A * x) + rand(n)\n\nM = diagm(b) * (I(n) - A * A') * diagm(b)\nU = ComplexVariable(n, n)\nobjective = inner_product(U, M)\nc1 = diag(U) == 1\nc2 = isposdef(U)\np = minimize(objective, c1, c2)\nsolve!(p, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"evaluate(U)","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"Verify if the rank of U is 1:","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"B, C = eigen(evaluate(U));\nlength([e for e in B if (abs(real(e)) > 1e-4)])","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"Decompose U = uu^* where u is the phase of Ax","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"u = C[:, 1];\nfor i in 1:n\n    u[i] = u[i] / abs(u[i])\nend\nu","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"","category":"page"},{"location":"examples/optimization_with_complex_variables/phase_recovery_using_MaxCut/","page":"Phase recovery using MaxCut","title":"Phase recovery using MaxCut","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"EditURL = \"robust_approx_fitting.jl\"","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/#Robust-approximate-fitting","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"","category":"section"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"Section 6.4.2 Boyd & Vandenberghe \"Convex Optimization\" Original by Lieven Vandenberghe Adapted for Convex by Joelle Skaf - 10/03/05","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"Adapted for Convex.jl by Karanveer Mohan and David Zeng - 26/05/14 Original CVX code and plots here: http://web.cvxr.com/cvx/examples/cvxbook/Ch06_approx_fitting/html/fig6_15.html","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"Consider the least-squares problem:       minimize (A + tB)x - b_2 where t is an uncertain parameter in [-1,1] Three approximate solutions are found:","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"nominal optimal (that is, letting t=0)\nstochastic robust approximation:      minimize mathbbE(A+tB)x - b_2 assuming u is uniformly distributed on [-1,1]. (reduces to minimizing mathbbE (A+tB)x-b^2 = A*x-b^2  + x^TPx   where P = mathbbE(t^2) B^TB = (13) B^TB )\nworst-case robust approximation:      minimize mathrmsup_-1leq uleq 1 (A+tB)x - b_2 (reduces to minimizing max(A-B)x - b_2 (A+B)x - b_2 ).","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"using Convex, LinearAlgebra, SCS","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"Input Data","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"m = 20;\nn = 10;\nA = randn(m, n);\n(U, S, V) = svd(A);\nS = diagm(exp10.(range(-1, stop = 1, length = n)));\nA = U[:, 1:n] * S * V';\n\nB = randn(m, n);\nB = B / norm(B);\n\nb = randn(m, 1);\nx = Variable(n)","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/#Case-1:-nominal-optimal-solution","page":"Robust approximate fitting","title":"Case 1: nominal optimal solution","text":"","category":"section"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"p = minimize(norm(A * x - b, 2))\nsolve!(p, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"x_nom = evaluate(x)","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/#Case-2:-stochastic-robust-approximation","page":"Robust approximate fitting","title":"Case 2: stochastic robust approximation","text":"","category":"section"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"P = 1 / 3 * B' * B;\np = minimize(square(pos(norm(A * x - b))) + quadform(x, Symmetric(P)))\nsolve!(p, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"x_stoch = evaluate(x)","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/#Case-3:-worst-case-robust-approximation","page":"Robust approximate fitting","title":"Case 3: worst-case robust approximation","text":"","category":"section"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"p = minimize(max(norm((A - B) * x - b), norm((A + B) * x - b)))\nsolve!(p, SCS.Optimizer; silent_solver = true)","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"x_wc = evaluate(x)","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/#Plots","page":"Robust approximate fitting","title":"Plots","text":"","category":"section"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"Here we plot the residuals.","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"parvals = range(-2, stop = 2, length = 100);\n\nerrvals(x) = [norm((A + parvals[k] * B) * x - b) for k in eachindex(parvals)]\nerrvals_ls = errvals(x_nom)\nerrvals_stoch = errvals(x_stoch)\nerrvals_wc = errvals(x_wc)\n\nusing Plots\nplot(parvals, errvals_ls, label = \"Nominal problem\")\nplot!(parvals, errvals_stoch, label = \"Stochastic Robust Approximation\")\nplot!(parvals, errvals_wc, label = \"Worst-Case Robust Approximation\")\nplot!(\n    title = \"Residual r(u) vs a parameter u for three approximate solutions\",\n    xlabel = \"u\",\n    ylabel = \"r(u) = ||A(u)x-b||_2\",\n)","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"","category":"page"},{"location":"examples/general_examples/robust_approx_fitting/","page":"Robust approximate fitting","title":"Robust approximate fitting","text":"This page was generated using Literate.jl.","category":"page"}]
}
